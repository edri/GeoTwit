\documentclass[a4paper,11pt]{report}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{listings}
\usepackage[usenames, dvipsnames]{color}
\usepackage{longtable}
\usepackage{fontawesome}
\usepackage[table,xcdraw]{xcolor}
\usepackage[export]{adjustbox}
\usepackage{titlesec}
\usepackage{psvectorian}

\setlength{\textwidth}{16cm}
\setlength{\textheight}{24cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\voffset}{-2.2cm}
\setlength{\headheight}{15pt}
\setlength{\parindent}{0pt}
% Disables red links
\hypersetup{pdfborder={0 0 0}}

\definecolor{DarkBlue}{HTML}{2e6c97}

\pagestyle{fancy}
\lhead{\small\textsc{GeoTwit - Thesis}}
\rhead{\small\textsc{Miguel Santamaria -- \today}}

\title{GeoTwit - Thesis}
\author{Miguel Santamaria}
\date{\today}

\begin{document}

\thispagestyle{empty}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.5\textwidth, left]{figures/logo-heig.png}
\end{center}
\end{figure}

\vspace{30pt}
\begin{center}
\Huge \textbf{Bachelor Thesis}
\end{center}
\vspace{50pt}

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.75\textwidth]{figures/logo.png}
\end{center}
\end{figure}

\vspace{90pt}

\begin{figure}[h]
\begin{center}
\psvectorian[color=DarkBlue, opacity=0.5, width=10cm]{88}
\end{center}
\end{figure}

\vspace{50pt}

\renewcommand{\arraystretch}{2.0}
\begin{tabular}{p{5cm}l}
\noindent\Large Author: & \noindent\Large Miguel \textsc{Santamaria}\\
\noindent\Large Supervisor: & \noindent\Large Ph.D. Nastaran \textsc{Fatemi}\\
\noindent\Large Date: & \noindent\Large\today
\end{tabular}
\renewcommand{\arraystretch}{1.0}

\newpage
\thispagestyle{empty}
~
\newpage
	
% Hide header and footer for this page
\thispagestyle{empty}
\vspace*{8cm}
\begin{center}
    \textbf{Acknowledgement}\\
    \bigskip
    I would first like to thank my thesis supervisor, Ph.D. Nastaran Fatemi, who steered me in the right the direction whenever I needed it and for providing me with all the necessary facilities for the research.\\
    \bigskip
    
    I would also like to acknowledge Jaspreet Sandhu as the second reader of this thesis, and I am gratefully indebted to her for her very valuable comments on this paper.\\
    \bigskip
    
    I also place on record, my sense of gratitude to one and all, who directly or indirectly, have lent their hand in this venture.
\end{center}

\setcounter{page}{0}

\chapter*{Abstract}
The purpose of this project is to enable analysis of varied subjects using the social platform of \emph{Twitter}, static or in real-time, using an application that provides an expanded geographic filtering of tweets and generates maps and charts on the analysis, \emph{GeoTwit}. This paper thus contains the analysis, development and implementation of this web tool, as well as numerous and varied subjects that have already been analyzed over the duration of a semester, at the level of individual topics as well as current events.\\
In preparation for the development of this tool, market studies were realized in addition to a thorough analysis of different Twitter's APIs, in order to glean the most optimal approach for the project. Furthermore, several prototype applications were developed and tested in advance, to understand and resolve the potential roadblocks and issues that could and did arise in the course of development. Finally, various schemas and diagrams like the mock-up and UML schemas were prepared, in order to visualize and discretize the steps involved. The final implementation integrates and streamlines all these tools to allow the end user to analyze a maximum of two simultaneous subjects in real-time. Static analyses are also possible, but are not as expanded as the real-time functionality. Requiring the user to connect to his Twitter account, the application allows him/her to import and export their analyses (maps, charts, speed reception and various other useful information) as external files.

\tableofcontents
\newpage
\thispagestyle{empty}
~
\newpage
\thispagestyle{empty}
~
\newpage

\setcounter{page}{4}
\chapter{Introduction}
\section{Summary of the Problem}
Within the span of a few years, social networks have rapidly mushroomed across the globe to become all-encompassing and omnipresent in our lives. Across such networks, a massive quantity of data is shared at every moment in time, and each time a larger volume from the preceding day. In addition to a widespread use by lambda people, the scientific community has been finding novel ways to analyze and make sense of this cascade of available social information: \emph{data mining}. Indeed, what better source there is than a social network, continuously and increasingly reporting collective human opinion and behavior in real-time, to analyze human-oriented subjects?\\

The goal of this project is thus to set up a web application that allows users to enter one or two subjects of their choice, to visualize a real-time activity graph/map for these subjects both on Twitter and on a geographic map and finally to import and export the results as external files. Static analyses are also possible, but are not as expanded as the real-time functionality.\\

In case the reader would not well know Twitter\footnote{https://en.wikipedia.org/wiki/Twitter}, or as a reminder, this social networking service enables to send and read short 140-character messages called "tweets". Tweets can be posted in a public way or in a private one (in this case, only the users that follow the poster can access and read these data). If a user decided to enable the geolocation features in the parameters of his account, he can choose to either add or not his current location as a metadata for each posted tweet, identified by a geolocation tag. A word, phrase or topic that is mentioned at a greater rate than others is said to be a "trending topic".
\newpage	

\section{Generalities}
This project - \emph{GeoTwit} - is carried out as the final Bachelor thesis within the TIC department of the Computer Engineering sector of the Yverdon-les-Bains's HEIG-VD school, for the IICT institute. The student - \emph{Miguel Santamaria} - supported by his supervisor - \emph{Ph.D. Nastaran Fatemi}, professor and teacher - worked alone on his project, at the level of both development and documentation. Note that this project is not confidential.\\

The project - selected in \emph{December 2015} and started on \emph{February 22, 2016} - covers several different aspects of web development as well as intelligent analysis (data mining) and data visualization. A progress report was asked for the \emph{June 17, 2016}, the final thesis was delivered on \emph{July 29, 2016}, and the oral defense is planned for the \emph{September 5, 2016}. The basic duration of the project is \emph{450 hours}, even if the work was done in less time than this value.
\bigskip

\section{Specifications}
\subsubsection{Functionalities}
The application provides the following features:
\begin{itemize}
	\item The reading of keywords and the selection of geographic areas on the map by the user.
	\item The retrieval of tweets, using the Twitter's APIs.
		Note that only a certain percentage of these tweets include geographic information, necessary for the future operations; a first filtering is thus operated here.
	\item The analysis and the filtering of tweets via calculation of the number of tweets by areas and by subjects.
	\item The visualization of the results on maps.
	\item The interaction (zoom-in, zoom-out, etc.) with the maps. The development of this feature involved the use of appropriate algorithms (like tweet grouping) and libraries.
	\item The generation of data charts and the possibility to import/export one's analysis.
\end{itemize}
\bigskip

\subsubsection{Used Technologies}
The suggested tools for this project were the \emph{Scala} language coupled with \emph{Play Framework}, while noting other technological choices were also possible. Due to both the personal interests of the student and a certain curiosity for unknown technologies in general, it was decided that these choices will be retained.
Lacking proficiency in either of these technologies, the student was initially required to learn the Scala language and the use of Play Framework before starting the development on the application.
Note that the \emph{JavaScript} web language is used to complement the client side of the application.

\chapter{Analyses}
This chapter contains all the analyses done before the implementation of the application itself.

\section{Existing Applications / Market Studies}
There are several existing applications that allow a user to visualize tweets on a map, the most visible being:
\begin{itemize}
	\item \textbf{One Million tweet Map}\footnote{http://onemilliontweetmap.com/}: the One Million tweet Map application lays out, to some extent, what has been implemented in this project, i.e. displaying tweets in real time (filtered or not) on a geographical map with grouping methods.
	
	However, unlike GeoTwit, the application does not offer a static search and view of tweets nor the possibility to export and import the analyzed data and to access and analyze charts. In addition, it also does not offer the possibility to analyze and compare two subjects at the same time or to filter tweets by language or location of the tweet. Nevertheless, this application does not require users to connect with their Twitter's accounts, for using the application.
	\item \textbf{TrendsMap}\footnote{http://trendsmap.com/ - found via mashable.com/\footnote{http://mashable.com/2010/08/06/twitter-mapping-tools/\#yrksT7cG8Sqg}}: this application shows a real-time mapping of Twitter trends across the world. If you click on any word you will see a real-time stream of relevant tweets. One must register and/or pay for further access to features/content.
	
	The interesting feature of this application is the display of current trends across the world, which is not currently present in GeoTwit. Users are also not required to connect to their Twitter accounts. However, users have to pay to access more detailed content, and there is no possibility to access static content nor to filter tweets with more complex filters or to access data analysis and visualization tools like charts. There is also not the possibility of importing and exporting one's analysis report.
	\newpage
	
	\item \textbf{tweetping}\footnote{https://tweetping.net/}: shows real-time reception of tweets all around the world on a map.
	
	Since the application is still in a beta phase, it is not possible to access most features, but the application intends to offer data analysis tools like charts as well as a complex filtering process, nearly matching the GeoTwit's features. However as with the previous two apps, users do not seem to be able to access static features or import and export analysis. Users also need to pay in order to access more interesting functionalities, but don't have to connect with their Twitter accounts. A feature of note was that the GUI of this application is well designed and user friendly.
	\item \textbf{tweetMap}\footnote{https://worldmap.harvard.edu/tweetmap/}: This tool allows one to visualize tweets written in a certain period, and create interesting charts and visualizations.
	
	This application provides interesting tools to analyze static content with a map (complex filters and charts), perhaps more so than GeoTwit. Users also do not need to connect. In this case as well, there was not the possibility to analyze real-time data or export analysis.
	\item \textbf{Stweet}\footnote{http://tweet.we-love-the.net/Stweet/ found via mashable.com/}: his service combines Twitter and Google Street View data to provide us a location-based breakdown of trending topics in a specific neighborhood.
	
	This application, despite its simplicity, is innovative given the functionality of allowing the user to display tweets directly onto 3D street view.
	\item \textbf{GeoChirp}\footnote{http://tweet.geochirp.com/}: this app allows the user to search for tweets in a given location, but only in a static way (no real time updating).
	
This application is a basic one, only returning tweets in a given location, with the upper limit specified by the user. Like GeoTwit, users must connect to their Twitter's accounts to use the application. However, there is no way to analyze real-time data or visualize the data.
	\item \textbf{tweet To Map}\footnote{http://tweettomap.com/}: this web plug-in allows one to put markers on a map, based on tweeted topics.
	
	The idea of making a plug-in for use in other application is an innovative idea, which make this application more interesting than the others. However, the application only supports static content and solely displays markers on the geographical map. Subsequently, it has not been used by GeoTwit.
\end{itemize}
\bigskip
\newpage

GeoTwit is thus not a standalone innovation, but can attest to some new and interesting features, markedly the possibility to collect tweets in both dynamic and static ways and the access to data analysis, visualization tools and import/export features. Here is a brief summary of GeoTwit's \color{ForestGreen}strengths \color{black}and \color{BrickRed}drawbacks\color{black}:\color{ForestGreen}
\renewcommand{\labelitemi}{\faicon{check}}
\begin{itemize}
	\item \textbf{Data export}: None of the analyzed applications seem to allow users to export the real-time analyzed data. GeoTwit allows the user to fully export the queried data and its analysis.
	\item \textbf{Charts}: Charts are a good way to easily analyze data, but is a functionality not often provided by applications. In fact, very few of the analyzed applications provided a charts tool.
	\item \textbf{More complex filters}: In most of the analyzed applications, users can only type a word or a phrase to filter tweets. In GeoTwit, users can filter tweets by complex keywords sets (a maximum of 2) with the possibility to add Boolean operators like "AND" and "OR" and compare results in real-time. It is further possible to filter tweets by location, language and dates (only for the static view).
	\item \textbf{Static and streaming search}: Other similar applications seem to offer either static or streaming analysis, but rarely both of them at the same time. This is one of the strong points of GeoTwit.
	\item \textbf{Free}: Lastly, GeoTwit is entirely free, unlike several other applications, including access to all of its features. In return, users have to connect to their Twitter's accounts and are limited to a certain number of simultaneous searches and static searches per period.\color{BrickRed}\renewcommand{\labelitemi}{\faicon{times}}
	\item \textbf{Mandatory connection to Twitter account}: Users need to connect with their Twitter accounts in order to use GeoTwit. As the application is free, this requirement arises out of Twitter imposed limitations.
	\item \textbf{Limitations}: Users cannot not follow more than two simultaneous streaming processes per account, and have an upper limit to possible static search per period of time. This disadvantage arises from the fact that GeoTwit has free and thereby a limited access to Twitter API.
	\item \textbf{Restricted Map display}: Unlike other applications that provide displays like Google Street View or other original views, GeoTwit only offers Roadmap view.
	\item \textbf{No functionality of current trends}: This was an often seen and useful functionality in the previously discussed applications, and can be useful to guide the users in their choice of filters. Unfortunately, there is currently no way to access current trends on GeoTwit.
	\item \textbf{GUI}: Despite the functional and reasonably user friendly GUI of GeoTwit, it can clearly be improved especially in regard to the previously analyzed applications, some of which clearly had enviable interfaces.
\end{itemize}
\color{black}
\renewcommand{\labelitemi}{\textbullet}
\bigskip
\newpage

\section{Technologies}
This chapter contains a description of the main technologies used throughout this project. Note that many links have been provided for further reference, but are not necessary to get a reasonable understanding of the technology and its relevance to the project.
\bigskip

\subsection{Scala}
Scala is a multi-paradigm (object-oriented, imperative, concurrent and functional) programming language designed at the EPFL in 2003 by Martin Odersky. It has a strong and static typing discipline and runs on a JVM (Java Virtual Machine). As Scala heavily draws from Java, Java libraries may be used directly in Scala code and vice-versa.

\subsubsection{Motivations}
Given the generally high regard for Scala as a language and the student's penchant for functional (like Haskell or Java8) and object-oriented languages during his formation, Scala has been used in this project.
\bigskip

\subsection{Play Framework}
Play Framework is an open-source web framework created in 2007 by Guillaume Bort that allows the developer to write web applications with Java or Scala. With the decision to use Scala in this project, this choice follows logically, as the framework is compatible with the language, is well-documented and well regarded by the community.\\

In the following paragraphs, one can find a brief overview of the framework, in order to understand and follow what was done during the project.

\subsubsection{Installation}
Since the installation of the framework is quite simple, it will not be rewritten in this documentation, and refers the reader to its official documentation\footnote{https://tweet.playframework.com/documentation/2.5.x/Installing.}.

\subsubsection{Creation of a new Project}
To create a new Play's Scala project, the developer has to type the following command, where [APPLICATION\_NAME] is the name of the application to create:
\begin{lstlisting}
	activator new [APPLICATION\_NAME] play-scala
\end{lstlisting}
Once done, one simply enters the new created folder and types "\emph{activator}" to enter the Play console, where it is now possible to compile the code and start the server with a "\emph{run}" command. Once the server has been started, the code will compile automatically on the loading of the webpage.
\bigskip

\subsection{Twitter's APIs}
Before starting, it must be noted that all content of this chapter comes from the documentation of the official Twitter development web site\cite{twitter2016}. Subsequently, many diagrams contained in this document have been taken from this documentation.\\

Please also note that Twitter offers a "Best Practices" page, which have been helpful during the conception of the application, both for the security and optimization parts\footnote{https://dev.twitter.com/overview/general}.

\subsubsection{Twitter's Technologies}
Twitter offers many tools to developers to facilitate the use of their platform within applications, which include:
\begin{itemize}
	\item Fabric: mobile development tools to enable reliable interfacing with the social features of Twitter (sharing, connection, etc.).
	\item Twitter for Websites: widgets collection, used to streamline integration of Twitter widgets ("Post tweet" and "Follow" buttons, display of tweets, etc.) into a web site.
	\item \textbf{OAuth}: an authentication protocol used by Twitter APIs for securing and authorizing different executed requests. It should be noted that this is not a platform developed by Twitter but rather a free protocol. You can find more details about this protocol in the following chapter.
	\item \textbf{REST API}: These provide read and write access to Twitter data in JSON format or via OAuth in order to enable their use in development. They work like most of the APIs provided by services of this kind and are listed below.
	\item \textbf{Streaming API}: These APIs serve the purpose of sending a response to the requests of the REST APIs through a sustained HTTP connection. They can be useful to continually display tweets linked to a given subject and thus to automatically fetch new tweets matching with the filter. This technology is further analyzed below.
\end{itemize}

The various technologies listed above are the ones that can potentially be interesting within the project. After a quick analysis, it turns out that the tools that will be most useful and relevant are the \underline{REST API} and the \underline{Streaming API}, coupled with the \underline{OAuth} protocol for security. For the first version, it has been planned to not use mobile technologies or the widgets option, which for the moment has been found not sufficiently wide and complete.
\newpage

\subsubsection{OAuth}
The current version (v1.1) of Twitter APIs uses version 1.0A of the OAuth protocol. The developer can choose from among two ways of authentication within their application:
\begin{wrapfigure}{r}{0.4\textwidth}
\vspace{2pt}
\begin{center}
\fbox{\includegraphics[width=0.35\textwidth]{figures/twitter-application-authorization.png}}
\caption{example of Twitter application's authorization}
\end{center}
\vspace{-32pt}
\end{wrapfigure}

\begin{enumerate}
	\item \textbf{Application-User authentication}: this is the most common form of resource authentication for APIs of this kind (Instagram, Facebook, Twitter, etc.). Signed requests both identify the identity of the application and the end-user it makes API calls on behalf of.
Specifically, the application user must have a Twitter account, be able to connect and then authorize the application to access data via his account (for actions requested by the developer: reading, posting tweets among other things) in order to get an access token. Later, all executed requests will be signed by this token linked to the current application and user.
In order to use this method, the user must therefore have a Twitter account in addition to trusting the given application so as to grant it reading and/or writing access on Twitter's data.
\end{enumerate}

\begin{enumerate}
	\setcounter{enumi}{1}
	\item \textbf{Application authentication only}: the application executes the requests of the APIs on its own behalf irrespective of the user. This method could at the first sight seem promising but is unfortunately limited in its list and number of possible calls to the API. While it enables the user to search for tweets and get user information, one does not have access to geographic data or the possibility of streaming.
\end{enumerate}

In spite of the ergonomic issues that the first method (\underline{application-user authentication}) could give rise to, it is the better choice for our application. This is especially evident given that the second method does not allow the developer to use either geolocation or streaming, which are essential to the real-time visualization of Twitter's activities and are employed heavily in our application.
\newpage

\subsubsection{Introduction to REST and Streaming APIs}
The REST API identifies the Twitter applications and its users by using OAuth and returns JSON-formatted responses. It allows one - among other things and according to the specific user's rights - to read and post tweets, access to Twitter user profiles, etc. The last released version (namely v1.1) works in the following manner:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.7\textwidth]{figures/twitter-rest-api-use-case.png}}
\vspace{-5pt}
\caption{use-case of the REST API within a web application, taken from the documentation of the Twitter's APIs}
\end{center}
\end{figure}

In a nutshell, the user makes requests that are executed by the server, which then receive response from Twitter and forwards them to the client.\\

Streaming API's which dynamically update results of the search in real-time works a bit differently, but also uses OAuth and JSON:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.75\textwidth]{figures/twitter-streaming-api-use-case.png}}
\vspace{-5pt}
\caption{use-case of the Streaming API within a web application, also taken from the documentation of the Twitter's APIs}
\end{center}
\end{figure}

The operation is here a little more complex: the part of the code that maintains the streaming is executed in a separate process from the one that processes the HTTP requests.\\

There are 3 types of endpoints:
\begin{enumerate}
	\item \textbf{Public streams}: streaming of public data flowing through Twitter, which can be filtered by keywords, users, etc.
	\item \textbf{User streams}: single-user stream, roughly containing all of the data corresponding to a single user's twitter feed.
	\item \textbf{Site streams}: a multi-user version of user streams.
\end{enumerate}

For obvious reasons the \underline{public stream} is the endpoint used within GeoTwit.\\

The Streaming API is the core functionality used in GeoTwit, owing to our desired end-goal, which is to be able to visualize the activities of the Twitter's subjects in real-time. The REST API is not set aside, though, as it will be useful for the comprehension of the streaming which bases its search algorithms on it. It will also enable users of GeoTwit to statically search tweets.

\subsubsection{Streaming API}
To initialize a stream based on a given filter, the developer must execute a specific HTTP request (as detailed above), indicating the Twitter's servers he wants to connect on. It is also necessary that the connection remain open such that the servers will send him data at regular intervals. To ensure a sustained connection, the Twitter's servers send a message to the application every 30 seconds, indicating that the connection is still active (in case there would be no new actualities). If no confirmation is received for a duration of 90 seconds, the developer needs to initiate a new connection to the stream.\\

Since there is no other way to test the Streaming API than to develop another application, there is no example in this chapter. It is advised however to refer to the "\nameref{prototypeApplications}" chapter, in which several prototype applications made to test both Twitter APIs and maps libraries have been introduced and explained. Search examples have also been presented below the "REST API" section of this document.\\

In the aforementioned prototype applications, it was noticed during testing that: Twitter seems to restrict the number of tweets transiting through streaming, which means that \underline{the Streaming} \underline{API does not send all posted tweets}. Indeed, when trying to post some relevant tweets when listening to a streaming (in particular during the Euro2016 event), the prototype application did not systematically receive them through the streaming (even if they had geolocation tags, etc.). Thus, the developer only receives limited samples (which still partially represent a view of reality) when listening to a streaming.\\

\textbf{\emph{Limitations}}\\
In order to prevent the Twitter's servers to be overloaded by "churns" (a large number of requests of connections' opening and closing), Twitter restricts the possible number of connection requests\footnote{https://dev.twitter.com/streaming/overview/connecting} in a certain time frame. Once an application exceeds this number, it will receive an HTTP response containing the 420 error code; for unclear reasons, Twitter privately keeps the maximum number of possible connections as well as the time frame value. Note that if the limit is regularly exceeded, Twitter will block the IP address of the client for a while, making this an issue that must be taken into account.\\

Please also note that the streaming is discouraged with mobiles using cellular network, because of the "$disconnection\to$ \emph{connection}" loop's high risk to create "churns".\\

Finally, note that it is only possible to initiate two instances of the same application following a Twitter streaming with the same account at the same time.\\

\textbf{\emph{Subscription to Twitter's subjects}}\\
As stated above, it is necessary to make an HTTP request to indicate to the servers of Twitter that the developer wants to subscribe to a particular stream. This request must/can have several parameters\footnote{https://dev.twitter.com/streaming/overview/request-parameters.}, which notably allow one to give various parameters to the server, like the desired language of the tweets, the filters (keywords, hashtags, etc.), the location, or other minor parameters.\\

For location, the Streaming API uses bounding boxes. In other words, it is a rectangular area defined by a pair of geographic coordinates (a longitude and latitude pair; be aware of the fact that we are more used to see coordinates in the "latitude, longitude" format), with the southwest corner of the bounding box given first, followed by the northeast coordinate. It is possible to have many bounding boxes in one request, separated by commas.\\

For example, if one wants to search tweets in Switzerland, one has to give the following coordinates: \emph{5.956085,45.817851} and \emph{10.489254,47.808454}.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/coordinates-pair-bounding-switzerland.png}}
\vspace{-20pt}
\caption{graphical representation of the coordinates pair bounding Switzerland, taken from Google Maps}
\end{center}
\end{figure}

Beware however: the location does not act as a filter for the other filters, which means the request "\emph{track=twitter\&locations=5.956085,45.817851, 10.489254,47.808454}" will search tweets containing the "twitter" keyword OR being located in Switzerland.\\

To better understand the operation of the location "filter", several tests were conducted. It is important to note that those tests have been realized with the "twitter4jDesktop" prototype application (detailed on the "\nameref{prototypeApplications}" chapter below) and with the Streaming API. The used keyword was "michel" and the location was a rectangle bounding the U.S.A. (\emph{[-124.411668, 24.957884], [-66.888435, 49.001895]}). This tuple of keyword/location was used to avoid retrieving too many results and to (unsuccessfully) isolate information.
\begin{enumerate}
	\item \textbf{Search with keyword only} $\to$ the obtained results are correct and correspond to the keyword:
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/search-keyword-only.png}}
	\vspace{-5pt}
	\caption{search of tweets with keyword only}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	
	However, the tweets returned were from all around the world, which restrict the use cases within the application...
	
	\item \textbf{Search with the location only} $\to$ anything and everything was obtained here: indeed, most of the returned tweets don't have geographic coordinates. This can be explained because of this "OR" operator applied on the search: the application is going to display all tweets coming from the location area OR the tweets corresponding to the keyword (namely an empty keyword in this case, so any tweet...). In summary, absolutely all tweets are displayed...
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/search-location-only.png}}
	\vspace{-5pt}
	\caption{search of tweets with location only}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	
	\item \textbf{Search with keyword AND location} $\to$ at first sight one can expect this method to provide good results, but no... It is indeed a simple application of the operator OR: the application is going to display all tweets having the keyword "michel" OR written in the U.S.A. (without taking in account the keyword...), which will give incongruous results again.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/search-keyword-and-location.png}}
	\vspace{-5pt}
	\caption{search of tweets with both keyword and location}
	\end{center}
	\end{figure}
\end{enumerate}

The tweets don't necessary have a geolocation tag (a rate of about 20-30\% of tweets having geographic data was observed during tests), so we decided filter the returned results on-the-fly to only accept those which have one.\\

None of these search methods gave appropriate results... This is a huge problem, because we need to search tweets by a keyword AND a location in the application. Therefore, we have no other option but to code our own location filter, which is applied to the results of a search by keywords.\\

Two different approaches are possible here:
\begin{enumerate}
	\item Search tweets by keywords and (effectively OR) geolocation, and thereafter apply a manual filtering by keywords AND by geolocation for countering the logic filter OR. Here are the results obtained in two periods of 5 minutes with the "job" keyword and in the U.S.A. (by around 15:30, Swiss time zone):
	
	\vspace{5pt}
	\textit{"I received 8061 tweets in 300 seconds, including 314 ones WITH geolocation tags and 284 ones with the wanted geolocation.\\
	\textbf{This means 3.9\% of the received tweets with the "job" tag(s) owned a geolocation tag and 3.52\% contained the desired location.}}
		
	\textit{I received 6980 tweets in 300 seconds, including 225 ones WITH geolocation tags and 215 ones with the wanted geolocation.\\
	\textbf{This means 3.22\% of the received tweets with the "job" tag(s) owned a geolocation tag and 3.08\% contained the desired location.}"}
	\vspace{5pt}
	
	Results here were disappointing with only about 3-4\% of tweets having a geolocation tag.
	
	\item Search tweets by keywords, and then apply a manual filtering of the geolocation.
	
	\vspace{5pt}
	\textit{"I received 2290 tweets in 300 seconds, including 548 ones WITH geolocation tags and 516 ones with the wanted geolocation.\\
	\textbf{This means 23.93\% of the received tweets with the "job" tag(s) owned a geolocation tag and 22.53\% contained the desired location.}}
	
	\textit{I received 2774 tweets in 300 seconds, including 571 ones WITH geolocation tags and 535 ones with the wanted geolocation.\\
	\textbf{This means 20.58\% of the received tweets with the "job" tag(s) owned a geolocation tag and 19.29\% contained the desired location."}}
	\vspace{5pt}
	
	Results are way more convincing here, with about 20\% of tweets having the desired geolocation tag. Even if about 3 to 4 times less tweets were received than before, there was still more relevant tweets. Thus, this methodology was chosen for the project.	
\end{enumerate}

Within GeoTwit, it could also be possible to propose default locations (for example by countries). A prototype application ("Leaflet Countries' Borders" - see the "\nameref{prototypeApplications}" chapter) was made. In this application, you can double-click on countries to draw borders all around them. Many libraries/APIs exist and allow one to easily get coordinates by a pointed location for example. The most well-known of them undoubtedly is Google Maps, but other alternatives like OpenStreetMap exists and has been discussed below.\\

In addition to receiving JSON messages containing tweets' data\footnote{formatted in the following way:  https://dev.twitter.com/overview/api/tweets}, it is also possible to receive other kind of messages\footnote{all messages' types are available right here: https://dev.twitter.com/streaming/overview/messages-types} from the servers of Twitter, for example to indicate a deletion of tweets or of geolocation tags, a disconnection, a limit reached, etc.\\
\newpage

\textbf{\emph{Optimizations}}\\
In order to reduce the bandwidth, it is possible to request a compressed Gzip data format to the servers with the following HTTP header: "\emph{Accept-Encoding: deflate, gzip}". Twitter also warns the developer about the fact that it is possible to receive a huge amount of data during certain sports or important cultural events. It is thus advisable to run initial tests in order to test if one's application is able to handle this massive flow and take precautionary measures.\\

Also, for optimization purposes, the servers use the "Chunked transfer encoding" data transfer mechanism, which essentially means sending data in a series of "chunks" without knowing the length of the content before starting transmission of a response to the receiver. The counterpart application thus must be compatible with this type of transfer.\\

It also must be noted that the received messages are not necessarily ordered and that a message can be received several times.

\subsubsection{REST API}
We layout some analysis on REST APIs below, which will still be useful in the future of the project, especially for the purposes of the static search functionality.\\

\textbf{\emph{Limitations}}\\
First of all, it should be noted that this API has some limitations to ensure the security of the platform, and to avoid Twitter's servers to be overloaded by requests. Limitations are largely applied on the number of requests that can be made in periods of 15 minutes and by windows. Note that if one's application exceeds limits once too often it may be put on black list, which will consequently forbid the access to the API and thereby the possibility to make further requests. For this reason, the developer needs to carefully optimize the volume of sent requests.\\

When an application exceeds the rate limit of authorized requests, the Twitter's API send back a "429: Too Many Requests" HTTP response code, which allows one to indicate to the user a limit to the number of requests allowed by Twitter, has been reached, following his request.\\

The HTTP headers contain, among other things, information concerning the current rates of made and remaining requests:
\begin{itemize}
	\item X-Rate-Limit-Limit: indicates the rate limit of requests for the type of the given request\footnote{https://dev.twitter.com/rest/public/rate-limits}.
	\item X-Rate-Limit-Remaining: indicates the number of remaining requests for the type of the given request and for the current period of 15 minutes.
	\item X-Rate-Limit-	Reset: the remaining time before the next reset of the 15 minutes' period.
\end{itemize}
\newpage

\textbf{\emph{Tweet search}}\\
The search API belongs to the Twitter's REST API. It allows one to search tweets by various criteria\footnote{Many search filters exist https://dev.twitter.com/rest/public/search} (latest or popular tweets containing or not keywords or hashtags, filtered by language and by geolocation, etc.). Given the massive quantity of data to process, for performance purposes, search only takes into account tweets published during the 7 last days preceding the request. After some tests, it was observed that some 9-days old tweets might be received in some cases, but no older.\\

Most of these parameters are also compatible with the Streaming API. Among them, the most interesting ones are the ones regarding the search of keywords and hashtags. In addition to these filters, there are several other parameters that allow further fine tuning of the returned query, such as the type of results (the latest and/or most popular tweets), the geolocation, the language of the tweet and even iterative or cumulative parameters (\textit{count}, \textit{until}, \textit{since\_id}, \textit{max\_id}).\\

Regarding the location, this API directly uses the geolocation. Specifically, it is strictly speaking not possible to search near a specific location: one has to use a specific geocode in the "latitude,longitude,radius" format, where \emph{radius} is the search radius in miles (\emph{mi}) or kilometers (\emph{km}), having for center point the given latitude and longitude.\\

The API will initially attempt to search for tweets that match the requested latitude, longitude and radius constraints, and then in case of no results, will search for tweets whose users' profiles satisfy the desired geolocation parameters.\\

For example, if one wants to search for tweets in a radius of 50 kilometers around the main station of Yverdon-les-Bains, the parameters will have the following values: "\emph{46.783177,6.640630,50km}".
Ideally for this project, the user should be able to point to a location on a map and indicate the desired limiting radius, while the application automatically acquires the longitude and latitude values associated with the location.\\

Note that it is also possible to search for tweets by the ID of locations\footnote{More information can be found here: https://dev.twitter.com/rest/public/search-by-place}. For example, the tweets emitted from the Twitter's headquarters can be found with the "place\%3A247f43d441defc03" parameter. This parameter will certainly be less useful than the search with geolocation, but still deserves to be mentioned in this documentation.\\
\newpage

\textbf{\emph{Examples}}\\
\begin{wrapfigure}{l}{0.30\textwidth}
\vspace{-30pt}
\begin{center}
\fbox{\includegraphics[width=0.275\textwidth]{figures/twitter-api-console-authorization.png}}
\caption{authorization of the API console through a Twitter account}
\end{center}
\vspace{-15pt}
\end{wrapfigure}
\\
To enable facilitated and speedy search requests, the advanced search tool\footnote{https://twitter.com/search-advanced} was used for the coming couple of examples. Whenever a search is made, the developer is redirected to a web page containing a URL, built using a particular format. For example, with a search about the @twitterapi user, one will get the following URL of redirection: \emph{https://twitter.com/search?q=\%40twitterapi}. Once done, the developer can get the valid URL to use in the API by replacing "\emph{https://twitter.com/search}" by "\emph{https://api.twitter.com/1.1/search/tweets.json}" (in this current example $\to$ \\\emph{https://api.twitter.com/1.1/search/tweets.json?q=\%40twitterapi}).\\\\\\\\\\
 
In addition, various tests have been conducted with the API console tool provided by Twitter\footnote{https://dev.twitter.com/rest/tools/console}. Beforehand and in order to be able to make requests, one has to authorize the console to access one's Twitter account. This access is automatically requested as soon as one attempts the first request.\\
\newpage 

Here is a list containing a few examples, which will turn out to be subsequently useful:
\begin{enumerate}
	\item Search of the latest French-language tweets having "yverdon" as keyword and (in reality OR, as mentioned previously) located in a radius of 10 kilometers from the Yverdon-les-Bains' main station.
	\begin{itemize}
		\item Request: https://api.twitter.com/1.1/search/tweets.json?\\q=yverdon\&geocode=46.783177,6.640630,10km\&lang=fr\&result\_type=recent
		\item Results:
		\begin{figure}[H]
		\vspace{-5pt}
		\begin{center}
		\fbox{\includegraphics[width=0.75\textwidth]{figures/french-tweets-yverdon.png}}
		\vspace{-5pt}
		\caption{results of the latest French tweets containing the "yverdon" keyword and (OR) being located near Yverdon-Les-Bains in the API console}
		\end{center}
		\end{figure}
		\vspace{-20pt}
		
		\item Results on Twitter:
		\begin{figure}[H]
		\vspace{-5pt}
		\begin{center}
		\fbox{\includegraphics[width=0.6\textwidth]{figures/french-tweets-yverdon-twitter.png}}
		\vspace{-5pt}
		\caption{results on Twitter of the latest French tweets containing the "yverdon" keyword and (OR) being located near Yverdon-Les-Bains}
		\end{center}
		\end{figure}
		\vspace{-20pt}
	\end{itemize}
	
	\item Search of popular and positive (containing a happy ":)" smiley) tweets, having the "\#pizza" hashtag and (OR) being located in a radius of 100 kilometers from the center of Rome (without constraints on tweet language).
	\begin{itemize}
		\item Request: https://api.twitter.com/1.1/search/tweets.json?\\q=\%23pizza\%20\%3A\%29\&geocode=41.894143,12.495479,100km\&src=popular
		\item Results on Twitter:
		\begin{figure}[H]
		\vspace{-5pt}
		\begin{center}
		\fbox{\includegraphics[width=0.75\textwidth]{figures/popular-happy-pizza-tweets.png}}
		\vspace{-5pt}
		\caption{results on Twitter of the popular and positive tweets, having the "\#pizza" hashtag and (OR) being located near to Rome}
		\end{center}
		\end{figure}
		\vspace{-20pt}
	\end{itemize}
\end{enumerate}
\newpage

\subsection{Interesting Twitter's Subjects}
\label{interestingTwitterSubjects}
This section contains various Twitter's subjects, which were deemed interesting for the project. These tests have been conducted with the "twitter4jDesktop" application on 25 April 2016, between 13:00 and 16:30 (CET) for (tweets made in) Europe and the following day from 09:00 to 10:00 (CET) for U.S.A.

\begin{figure}[H]
\begin{center}
\includegraphics[width=\textwidth]{figures/subjects-list.png}
\end{center}
\end{figure}

\newpage

As for recurring themes for tweets made from Switzerland, the subjects can be separated into two main categories: the Swiss media and Industries/Companies based in Switzerland. Some of the unsuccessful tests included the following keywords: "coop", "glencore", "logitech", "migros", "nestle", "novartis" and "swatch". These words not included in the table above, because of the insufficient refreshment rate (zero or one tweet per 5 minutes).
Concerning the media, we tested some potential keywords that were deemed relevant for use across different parts of Switzerland (French, German and Italian parts) like "job", "sport" or "people". These keywords were unfortunately not successful either. A further search for tweets containing the 'e' letter (which is pretty common) in a last ditch attempt to find tweets with geolocation in Switzerland, resulted in zero results.\\

As for other major Europeans countries like UK, France or Italy, though most of the tested keywords were still unsuccessful, some of them gave better results, especially from major cities in each country.\\

We shifted our analysis to the U.S.A., given their affinity for social media usage, especially compared to Europe. During the tests, a holy grail was found: "job". More than 600 results in a 5 minutes' period were received. More typical and currently relevant American subjects - like "vote", "trump", "clinton", "mcdonald" - and computer subjects like "java" or "scala" were tried, but did not yield interesting results.\\

In conclusion, results are worse than expected. On an average, better results were observed in the U.S.A., especially for the "job" keyword, which is the only keyword with great tweet frequency. Switzerland was quickly eliminated as a potential source of relevant tweets along with most other European countries, due to the zero rate of tweets having geolocation tags.\\

After some consideration and discussion with the supervisor, it has been decided that it would be better to try these keywords as well as other ones during hours/days instead of minutes. Those tests have been done once the application was developed, because it was more convenient to do long search and we could have access to graphs as well. You can find these tests in the "\nameref{tests}" part of this document.\\

Another interesting way to find subjects to analyze is to analyze the current trends in the world. Various web sites allow us to watch them, among which "trend24"\footnote{http://trends24.in/}, was found especially useful to find an hourly listing of worldwide trends.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/trend24.png}}
\vspace{-20pt}
\caption{example of hot trends in the world within trend24}
\end{center}
\end{figure}
\newpage

In addition to these examples, other interesting cases linked to ongoing events are possible: for example, a keyword corresponding to a subject of Swiss votes was analyzed, the name of a candidate in the U.S.A.'s elections during the results day (be careful about the massive flow of data!); the name of a show during its transmission on Television; the name of a sporting event's winner; etc.\\

Finally, here is a subset of hot and interesting topics that occurred during the semester:
\vspace{-10pt}
\begin{center}
\def\arraystretch{1.5}
\begin{longtable}{|l|l|}\hline
	\textbf{Date} & \textbf{Subject}\\\hline \endhead
	17.03.2016 & Hockey: LHC - HC Ambri-Piotta\\\hline
	17.03.2016 & St. Patrick\\\hline
	27.03.2016 & Easter\\\hline
	21.04.2016 & 90 years of the Queen Elizabeth\\\hline
	23.04.2016 & 400 years of Shakespeare\\\hline
	23.04.2016 to 24.04.2006 & 20KM of Lausanne\\\hline
	08.05.2016 & Football: FC Sion - BSC Young Boys\\\hline
	09.05.2016 & Transit of Mercury in front of the Sun\\\hline
	11.05.2016 to 22.05.2016 & 69th Cannes Festival\\\hline
	14.05.2016 & Eurovision in Stockholm\\\hline
	16.05.2016 to 05.06.2016 & Tennis: Roland-Garros\\\hline
	29.05.2016 & Football: Final of the "Coupe de Suisse"\\\hline
	05.06.2016 & Swiss votes\\\hline
	10.06.2016 to 10.07.2016 & UEFA Euro 2016\\\hline
	01.07.2016 to 16.07.2016 & 50th Montreux Jazz Festival\\\hline
	02.07.2016 to 24.07.2016 & "Tour de France" 2016\\\hline
	19.07.2016 to 24.07.2016 & Paléo Festival Nyon\\\hline
	20.07.2016 & "Independence Day Resurgence" film release\\\hline
	22.07.2016 & "Star Trek Beyond" film release\\\hline
	01.08.2016 & National day of Switzerland\\\hline
	04.08.2016 to 14.08.2016 & "Fêtes de Genève"\\\hline
	05.08.2016 to 21.08.2016 & Olympic Games of Rio de Janeiro\\\hline
	07.09.2016 to 18.09.2016 & Paralympic Games of Rio de Janeiro\\\hline
	25.09.2016 & Swiss votes\\\hline
	29.10.2016 to 30.10.2016 & Lausanne's marathon\\\hline
	08.11.2016 & Elections in the U.S.A.\\\hline
\end{longtable}
\end{center}
\vspace{-20pt}

\subsubsection{Euro 2016}
It was decided to test the prototype application on Twitter during the Euro 2016's games. In order to achieve this, the "leafletAndTwitter4J" prototype application (see the well-named "\nameref{prototypeApplications}" chapter for more information) was used and markers that did not belong to the country being analyzed were manually removed (this could occur given the rectangular shape of the limiting coordinates). For all analyses, the "euro2016" keyword was tested:
\begin{itemize}
	\item On \textbf{11.06.2016}, from \textbf{22:00} to \textbf{23:00}, during the 2nd half of the \textbf{England-Russia} match
	\vspace{50pt}
	\parbox[t]{\dimexpr\textwidth-\leftmargin}{%
		\begin{wrapfigure}{l}{0.30\textwidth}
		\vspace{-50pt}
		\begin{center}
		\fbox{\includegraphics[width=0.275\textwidth]{figures/en-ru-match-results.png}}
		\vspace{-8pt}
		\caption{tweets results for the UK-RU match}
		\end{center}
		\end{wrapfigure}
		\vspace{-15pt}
		
		\vspace{40pt}
		Score: \textbf{1-1}
		
		\vspace{5pt}
		\textbf{47} tweets with geolocation tags
		
		\vspace{5pt}
		Average of \textbf{12'000-14'000} tweets per 10 minutes until the first goal (73'), then \textbf{20'000-24'000} after it and until the end
		
		\vspace{5pt}
		A massive flow of \textbf{16'000} tweets was received in a few minutes after the goal of the 92'.
		
		\vspace{5pt}
		Between \textbf{0.01\%} and \textbf{0.07\%} of tweets had a geolocation tag (between 2 and 20 tweets per 10 minutes).
	}
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/en-ru-match-progression.png}}
	\vspace{-5pt}
	\caption{progression of the EN-RU match, taken from the RTS web site on the euro2016 section.}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	\newpage
	
	\item On \textbf{15.06.2016}, from \textbf{17:55} to \textbf{19:55}, during the \textbf{Romania-Switzerland} match
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.56\textwidth]{figures/ro-ch-match-results.png}}
	\vspace{-5pt}
	\caption{tweets results for the RO-CH match}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	Score: \textbf{1-1}\\
	\textbf{5} tweets with geolocation tags\\
	Average of \textbf{6'500-8'000} tweets per 10 minutes and \textbf{10'000-11'000} at the end of the match\\
	Between \textbf{0.01\%} and \textbf{0.05\%} of tweets had a geolocation tag (between 1 and 4 tweets per 10 minutes).
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/ro-ch-match-progression.png}}
	\vspace{-5pt}
	\caption{progression of the RO-CH match, taken from the RTS web site}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	
	\item On \textbf{16.06.2016}, from \textbf{14:55} to \textbf{17:05}, during the \textbf{England-Wales} match\\
	\vspace{70pt}
	\parbox[t]{\dimexpr\textwidth-\leftmargin}{%
		\begin{wrapfigure}{l}{0.30\textwidth}
		\vspace{-50pt}
		\begin{center}
		\fbox{\includegraphics[width=0.275\textwidth]{figures/en-wls-match-results.png}}
		\vspace{-8pt}
		\caption{tweets results for the EN-WLS match}
		\end{center}
		\end{wrapfigure}
		\vspace{-15pt}
		
		\vspace{40pt}
		Score: \textbf{2-1} for England
		
		\vspace{5pt}
		\textbf{135} tweets with geolocation tags
		
		\vspace{5pt}
		Average of \textbf{18'000} tweets / 10 minutes at the beginning of the match, then \textbf{10'000-14'000} until the end and finally \textbf{22'000-24'000} each time England scored (56' and 91').
		
		\vspace{5pt}
		Between \textbf{0.02\%} and \textbf{0.14\%} of tweets had a geolocation tag (between 3 and 20 tweets per 10 minutes).
		
		\vspace{5pt}
		There were several tweets from London, Norwich and Manchester, but just a few in Wales.
	}
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/en-wls-match-progression.png}}
	\vspace{-5pt}
	\caption{progression of the EN-WLS match, taken from the RTS web site}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	\newpage
	
	\item On \textbf{15.06.2016}, from \textbf{20:55} to \textbf{23:10}, during the \textbf{Albania-France} match
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.55\textwidth]{figures/al-fr-match-results.png}}
	\vspace{-5pt}
	\caption{tweets results for the AL-FR match}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	Score: \textbf{2-0} for France\\
	\textbf{126} tweets with geolocation tags\\
	Average of \textbf{15'000} / 10 minutes at the beginning of the match, \textbf{9'000-12'000} during it, and \textbf{24'000-28'000} at the end when the France scored twice.
	Between \textbf{0.01\%} and \textbf{0.09\%} of tweets had a geolocation tag (between 5 and 15 tweets per 10 minutes).
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.7\textwidth]{figures/al-fr-match-progression.png}}
	\vspace{-5pt}
	\caption{progression of the AL-FR match, taken from the RTS web site}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	There were plenty of tweets made from Lille, Paris and Marseille. Since the match took place in the "Stade Vélodrome" of Marseille, several tweets were posted there.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.4\textwidth]{figures/al-fr-match-stadium.png}}
	\vspace{-5pt}
	\caption{tweets results for the AL-FR match in the "Stade Vélodrome" of Marseille}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	\newpage
	
	\item On \textbf{16.06.2016}, from \textbf{20:55} to \textbf{22:55}, during the \textbf{Germany-Poland} match
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/de-pl-match-results.png}}
	\vspace{-5pt}
	\caption{tweets results for the DE-PL match}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	Score: \textbf{0-0}
	
	\textbf{40} tweets with geolocation tags
	
	Average of \textbf{18'000} tweets / 10 minutes at the beginning of the match, \textbf{11'000-14'000} until the end.
	
	Between \textbf{0.01\%} and \textbf{0.08\%} of tweets had a geolocation tag (between 1 and 6 tweets per 10 minutes).\\
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/de-pl-match-progression.png}}
	\vspace{-5pt}
	\caption{progression of the DE-PL match, taken from the RTS web site}
	\end{center}
	\end{figure}
	\vspace{-10pt}
	\newpage
	
	\item On \textbf{19.06.2016}, from \textbf{20:55} to \textbf{22:55}, during the \textbf{France-Switzerland} and \textbf{Albania-Romania} matches
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/fr-ch-match-results.png}}
	\vspace{-5pt}
	\caption{tweets results for the FR-CH match}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	Score (FR-CH): \textbf{0-0}\\
	Score (AL-RO): \textbf{1-0} for Albania
	
	\textbf{90} tweets with geolocation tags (85 in France, 5 in Switzerland)
	
	Average of \textbf{10'000-13'000} tweets per 10 minutes, and \textbf{20'800} at the end of the matches.
	
	Between \textbf{0.03\%} and \textbf{0.18\%} of tweets had a geolocation tag (between 5 and 24 tweets per 10 minutes).
	
	There were several tweets in Lyon (the city in where the AL-RO match took place), Paris and Lille (the city in where the FR-CH match took place).
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/fr-ch-match-progression.png}}
	\vspace{-5pt}
	\caption{progression of the FR-CH match, taken from the RTS web site}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/al-ro-match-progression.png}}
	\vspace{-5pt}
	\caption{progression of the AL-RO match, taken from the RTS web site}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	\newpage
	
	\item On \textbf{27.06.2016}, from \textbf{17:55} to \textbf{20:00}, during the \textbf{Italy-Spain} match
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/it-es-match-results.png}}
	\vspace{-5pt}
	\caption{tweets results for the IT-ES match}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	Score: \textbf{2-0} for Italy
	
	\textbf{99} tweets with geolocation tags
	
	Average of \textbf{15'000-18'000} tweets / 10 minutes at the beginning of the match, \textbf{24'000} during the half-time break, and \textbf{26'000} at the end.
	
	Between \textbf{0.02\%} and \textbf{0.19\%} of tweets had a geolocation tag (between 4 and 26 tweets per 10 minutes). There were a lot of tweets posted in the North of Italy.\\
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/it-es-match-progression.png}}
	\vspace{-5pt}
	\caption{progression of the IT-ES match, taken from the RTS web site}
	\end{center}
	\end{figure}
	\vspace{-10pt}
	 \newpage
	
	\item On \textbf{27.06.2016}, from \textbf{20:55} to \textbf{23:05}, during the \textbf{England-Iceland} match
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/en-is-match-results.png}}
	\vspace{-5pt}
	\caption{tweets results for the EN-IS match}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	Score: \textbf{2-1} for Iceland
	
	\textbf{3} tweets with geolocation tags
	
	Average of \textbf{22'000-26'000} tweets / 10 minutes during the whole match.
	
	Between \textbf{0.02\%} and \textbf{0.19\%} of tweets had a geolocation tag (between 4 and 26 tweets per 10 minutes).
	
	One tweet was received right after the first goal of Iceland, then two tweets in the same 10 minutes' period after the second one.\\
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/en-is-match-progression.png}}
	\vspace{-5pt}
	\caption{progression of the EN-IS match, taken from the RTS web site}
	\end{center}
	\end{figure}
	\vspace{-10pt}
	\newpage
	
	\item On \textbf{08.07.2016}, from \textbf{21:00} to \textbf{22:55}, during the \textbf{France-Germany} match\\
	\vspace{50pt}
	\parbox[t]{\dimexpr\textwidth-\leftmargin}{%
		\begin{wrapfigure}{l}{0.5\textwidth}
		\vspace{-50pt}
		\begin{center}
		\fbox{\includegraphics[width=0.475\textwidth]{figures/fr-de-match-results.png}}
		\vspace{-8pt}
		\caption{tweets results for the FR-DE match}
		\end{center}
		\end{wrapfigure}
		\vspace{-15pt}
		
		\vspace{40pt}
		Score: \textbf{2-0} for France
		
		\vspace{5pt}
		\textbf{98} tweets with geolocation tags
		
		\vspace{5pt}
		Average of \textbf{18'000-21'000} tweets / 10 minutes during the match, and \textbf{26'000-27'000} for each France's goal
		
		\vspace{5pt}
		Between \textbf{0\%} and \textbf{0.08\%} of tweets had a geolocation tag (between 0 and 16 tweets per 10 minutes). There were a lot of tweets posted from the stadium of Marseilles (the city in where the match took place).
	}
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/fr-de-match-progression.png}}
	\vspace{-5pt}
	\caption{progression of the FR-DE match, taken from the RTS web site}
	\end{center}
	\end{figure}
	\vspace{-20pt}	
	In addition, the GeoTwit application was developed enough to realize the first analyses with it. Here are the results:
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/fr-de-match-geotwit-results.png}}
	\vspace{-5pt}
	\caption{tweets results for the FR-DE match with the GeoTwit application}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	There were \textit{about} \textbf{90} tweets with geolocation posted from France ("about", because of certain groupings that grouped tweets from several countries; logically, if we manually count them by zooming on the map, the number must be 98, like the prototype application), about \textbf{41} from Germany and a total of \textbf{178} in the selected area in about \textbf{01:54:24}, which gives an average speed of \textbf{1.56 tweets per minute}.
	\newpage	
	
	\item On \textbf{10.07.2016}, from \textbf{21:00} to \textbf{23:35}, during the \textbf{France-Portugal} \underline{final} match
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.55\textwidth]{figures/fr-pt-match-results.png}}
	\vspace{-5pt}
	\caption{tweets results for the FR-PT match}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	Score: \textbf{1-0} for Portugal
	
	\textbf{135} tweets with geolocation tags
	
	Average of \textbf{27'000-28'000} tweets / 10 minutes during the whole match.
	
	Between \textbf{0.01\%} and \textbf{0.08\%} of tweets had a geolocation tag (between 3 and 22 tweets per 10 minutes).\\
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/fr-pt-match-progression.png}}
	\vspace{-5pt}
	\caption{progression of the FR-PT match, taken from the RTS web site}
	\end{center}
	\end{figure}
	\vspace{-10pt}
	This match was also totally analyzed with the GeoTwit application (see the "\nameref{euro2016final}" chapter).
\end{itemize}
\bigskip

\textbf{\emph{Discussions}}\\
After these results, we can conclude that England and France are the countries whose people posted the most tweets during the Euro2016 event, from among the analyzed countries. The France case can easily be explained: as this was the hosting country, several people were tweeting from the live matches. The English just seem to like Twitter and thus generally use it in high volumes; also, it is the country with the highest percentage level of tweets having geolocation tags. Concerning Switzerland, there were still too few tweets for it to be interesting: the application indeed received only 5 tweets from Switzerland in a 2 hours' period.
Tweets are generally clustered around big cities and in stadiums during matches. We can obviously see a certain correlation between the goals moments and the peak of tweets. These analyses were much more rewarding compared to the previous analyses realized in Europe, because of the considerably higher tweet rate.
\newpage

\subsection{Twitter4J}
\label{twitter4j}
Twitter4J\footnote{http://twitter4j.org/en/index.html} is a non-official library, which - as its name implies - allows one to easily use the features of Twitter's APIs with Java and which was proposed by the supervisor, Mrs. Fatemi. Given that Java classes are fully compatibles with Scala classes and vice-versa, it turns out that this library is easily usable with the Scala programming language.\\

It works both with the OAuth technology and the Gzip compression and is 100\% compatible with the version 1.1 of the Twitter's API. The advantages of this library lies in the fact that it significantly simplifies the APIs uses (for both REST and Streaming APIs), at the level of both data reading and writing. The documentation of this library has examples for each use-case, as well as many useful explanations.\\

You can find brief relevant instructions about the use of this library in the following paragraphs, allowing you to better understanding what I did in order to develop the prototype applications, and finally the GeoTwit application.\\

\subsubsection{Installation}
If the developer is using a Java or Scala desktop application, he can download the latest stable version of Twitter4J, then add "lib/twitter4j-core-4.0.4.jar" to his application class path. If he wants to use other features (like streaming or asynchronicity), he has to add other libraries (like "lib/twitter4j-stream-4.0.4.jar" for the streaming), too.
When using Play Framework, the developer just has to add \textbf{"org.twitter4j" \% "twitter4j-core" \% "4.0.4"} in the library dependencies in the \emph{/build.sbt} configuration file of his application.

\subsubsection{Creation of a New Application}
\label{creatingNewTwitterApplication}
\begin{wrapfigure}{r}{0.4\textwidth}
\vspace{-50pt}
\begin{center}
\fbox{\includegraphics[width=0.35\textwidth]{figures/twitter-application-parameters.png}}
\caption{Twitter application's parameters}
\end{center}
\vspace{-32pt}
\end{wrapfigure}

\vspace{30pt}
In order to use the Twitter's APIs on a new application the developer firstly has to create and register a new application on Twitter\footnote{https://apps.twitter.com/app/new}.\\

After this, he will be given access to a web page, which contains all the parameters of his application.\\

Once he has ensured that the access level of the application is the one he wants, he can easily get all keys and access tokens in the well-named tab, in order to use them in his application.
\newpage

\subsubsection{Desktop Applications}
Desktop applications are a little bit trickier to use with the Twitter's APIs, because the developer has to manually get access and must indicate to the API that he does not want it to redirect the user on a web page after the connection phase. You can find an example in the \emph{GeoTwit/prototypes/twitter4jDesktop} folder.\\

For afore-mentioned reasons, the developer has to use the PIN-based OAuth\footnote{https://dev.twitter.com/oauth/pin-based}\footnote{https://dev.twitter.com/web/sign-in/implementing} (the user has to give a PIN number generated by Twitter in order to use the API); here is the procedure used in the prototype applications:
\begin{enumerate}
	\item Create a new app on the "Twitter for developers" web site (see "\nameref{creatingNewTwitterApplication}" chapter above).
	\item Use the Twitter4J library to make a token request and get an access token\footnote{http://twitter4j.org/en/code-examples.html, on the "OAuth support" section}.\\
	
	If done correctly, the user will then be asked to grant access to the application through a web page's link. Through clicking on this page, he will be redirected to the authorization page:
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.6\textwidth]{figures/twitter-application-authorization-page.png}}
	\vspace{-5pt}
	\caption{Twitter application's authorization page}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	
	\item Once the user has authorized the app, he will get the PIN to input into the application:
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.75\textwidth]{figures/twitter-generated-code.png}}
	\vspace{-5pt}
	\caption{PIN code generated from Twitter}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	
	Here is an example of a right output:
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.7\textwidth]{figures/twitter-desktop-application-output-example.png}}
	\vspace{-5pt}
	\caption{output example of a Twitter's desktop application}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	
	\item On having received the token, you can simply use it for your next requests.
\end{enumerate}
\bigskip

\subsection{Geolocation Features}
In order to make the web site as ergonomic as possible and thus to be able to display and work with geographic maps, a library is used. Two technologies were analyzed in this section: the all-pervading Google Maps, and one of its main open-sourced rival, OpenStreetMap.

\subsubsection{Google Maps}
The Google Maps' API\footnote{https://developers.google.com/maps/?hl=fr} is very intuitive and user friendly (having already used it in the past), but can be problematic in its limitations: if one desires to use the site in a proper way, one needs to pay for the service in one way or another, in order to not be limited at the requests rate level, which is out of scope for a Bachelor thesis. For this reason, further analyses will not be drawn on this library.

\subsubsection{OpenStreetMap}
OpenStreetMap (currently in version 0.6) - proposed as an alternative by Mrs. Fatemi - is a direct and open-sourced competitor of Google Maps, and also boasts of an API\footnote{http://wiki.openstreetmap.org/wiki/API\_v0.6}.
In fact, there is two different APIs: the live-API\footnote{http://api.openstreetmap.org/}, and another used for tests\footnote{http://api06.dev.openstreetmap.org/}.\\

Although this technology is open source, this does not mean that it is lacking or slower: for example, it also uses the OAuth protocol and is used in a lot of applications, like Leaflet and MapQuest\footnote{a short list of these applications: http://wiki.openstreetmap.org/wiki/List\_of\_OSM-based\_services}.
This technology gets especially relevant to this project when coupled to a JavaScript library - also open-sourced and very light - allowing to easily manipulate maps and add effects on them: Leaflet\footnote{http://leafletjs.com/index.html}. In our case, the interest in this library - in addition to the main features offered by the map - especially lies in the fact that it allows one to easily communicate with the OpenStreetMap's API. One can also easily draw circles (by a latitude, a longitude and a given radius), rectangles or custom polygons on the map (usable for geolocation features)\footnote{You can find an example here: http://leafletjs.com/examples/quick-start.html} and add annotations as panels. Also, it is fully compatible with the mobile world, which is interesting for scalability issues.\\

Subsequently, we decided to use this platform within the project for all its geographic requirements.
\newpage

\subsection{Data Grouping}
In course of the project, two major data issues were encountered:
\begin{enumerate}
	\item A proper method had to be found when sending data from the GeoTwit's server to the client, because of the massive volume of data involved, which can potentially crash the web browser. After some analyses, it turned out that the web sockets technology is one of the best technologies to use in this case, because of the bidirectional connection between the client and the server, and especially given the fact that one can send floods of data through this route without problems other that some latency. This technology is detailed in the chapters related to the GeoTwit application (see the "\nameref{geotwitApplication}" chapter) further in the document.
	\item When displaying data on the map: since there can perhaps be thousands of tweets to display on the map, we have to group them with a grouping algorithm.
Some existing solutions were analyzed, and one of the most interesting one was without a doubt the marker-cluster plug-in proposed by Leaflet again\footnote{https://github.com/Leaflet/Leaflet.markercluster}, which allows one to group data by circles on the map. This library provides a lot of interesting functions, is fully compatible with the Leaflet library (because Leaflet wouldn't develop libraries incompatible with their main library), and can handle up to 50'000 markers in one map\footnote{http://leaflet.github.io/Leaflet.markercluster/example/marker-clustering-realworld.10000.html} \footnote{http://leaflet.github.io/Leaflet.markercluster/example/marker-clustering-realworld.50000.html}.
\end{enumerate}
\newpage

\section{Prototype Applications}
\label{prototypeApplications}
In order to test various libraries that were used within the project and as discussed before in this documentation, some prototype applications containing interesting tests were developed (in the \emph{GeoTwit/prototypes} folder). Please note that there are no inputs controls and the GUIs are very poor, because these are only test applications.

\subsection{Leaflet}
This application contains a simple map drawn with \textbf{Mapbox}'s imagery and \textbf{OpenStreetMap}'s data all together through the \textbf{Leaflet} JavaScript library. In this basic application, you can pin markers and draw rectangles and circles on a map. The \emph{js/map.js} file contains all the map's code.\\

Just open \emph{index.html} in your web browser to initiate the application.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/leaflet-prototype-application.png}}
\vspace{-20pt}
\caption{"Leaflet" prototype application}
\end{center}
\end{figure}

In this application, the Leaflet library uses both OpenStreetMap for the map's data and Mapbox for the map's imagery. The Leaflet library acts like a wrapper on the OpenStreetMap's API. Mapbox\footnote{https://tweet.mapbox.com/} is a platform, which provides map's design as blocks to easily integrate location into web applications. Note that this platform has a limited requests rate, which was noticed too late. In GeoTwit, the OpenStreetMap imagery is directly used.\\

Before interacting with all the Leaflet's components, the developer is required to create an account on the Mapbox platform\footnote{https://tweet.mapbox.com/studio/signup/} and then he has to:
\begin{enumerate}
	\item Get the default API's public token\footnote{https://tweet.mapbox.com/studio/account/tokens/} and write it somewhere.
	\item Create a MapBox project\footnote{https://tweet.mapbox.com/studio/classic/projects}, which is used by the Leaflet library to provides imagery. Once done, the developer has to get the project's ID and write it somewhere.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/mapbox-project.png}}
	\vspace{-5pt}
	\caption{a MapBox project}
	\end{center}
	\end{figure}
	\vspace{-20pt}
\end{enumerate}
	
Finally, this simple web-application was created and the downloaded Leaflet library was extracted into it, followed by well detailed steps from the quick start tutorial. In order to call the map's layer with MapBox; the code looks like this:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/mapbox-integration-into-leaflet.png}}
\vspace{-20pt}
\caption{MapBox integration into Leaflet}
\end{center}
\end{figure}

\subsection{Leaflet - Countries' Borders}
\label{leafletCountriesBorders}
This application provides the same functionalities as \textbf{Leaflet}, with the possibility to double-click on a country to draw a polygon all around it. In order to make the application work, please do the following:
\begin{enumerate}
	\item Ensure the JavaScript's packages manager "NPM"\footnote{https://tweet.npmjs.com/} in installed on your computer.
	\item Run "npm install -g browserify" to install \emph{browserify}, which is a library used to use npm's packages within an application without having a Node.js server\footnote{https://tweet.npmjs.com/package/browserify}.
	\item "npm install" to install npm's packages.
	\item Open the \emph{index.html} file in your web browser.
	\newpage
	\item Double-Click on any country on the map in order to draw a polygon all around it. Notice that some small countries are not present in the data set so you won't be able to draw a polygon around them.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/leafletcountriesborders-prototype-application.png}}
	\vspace{-5pt}
	\caption{"LeafletCountriesBorders" prototype application}
	\end{center}
	\end{figure}
	\vspace{-20pt}
\end{enumerate}

All implementation's details are well commented in the code, but here are the main things to know:
\begin{itemize}
	\item The data of all the countries' borders are contained in the \emph{/data} folder in a binary format (Shapefile or ".shp"\footnote{https://en.wikipedia.org/wiki/Shapefile}).
	\item Since the content of this file is in binary, JavaScript cannot read it directly and thus has to parse it and convert it to JSON (specifically to GeoJson\footnote{http://geojson.org/}, which is an agreed format used for encoding a variety of geographic data structures in JSON).\\
	
	In order to do this, the \emph{calvinmetcalf}'s shapefile-js library\footnote{https://github.com/calvinmetcalf/shapefile-js} was used, which was randomly found on Google and which perfectly works by writing "shp('data/TM\_WORLD\_BORDERS-0.3').then(function(geojson) {...}".
	\item Two npm's packages were also used to respectively get a country's ISO (e.g. CH or FR) by a longitude and latitude pair and to get a country's name by its ISO (e.g. CH => Switzerland): which-country\footnote{https://tweet.npmjs.com/package/which-country} and world-countries\footnote{https://tweet.npmjs.com/package/world-countries}.
	
	Since these packages are npm packages (which must normally be used in a Node.js server), the amazing "browserify" npm package is used. This one allows the developer to use npm's packages within a simple JavaScript application.
	
	As soon as you updated the \emph{/js/map.js} file (which manages the double-click event on the map), you have to type "browserify map.js -o bundle.js" in a console to generate the \emph{/js/bundle.js} file that contains a working version of the map.js with all the code of the npm's packages included.
	\item When the user double-clicks on a map, the application first gets the clicked point's coordinates, then search for the clicked country's ISO and name with the coordinates, and finally get the borders' coordinates in the GeoJson data by the country's name in order to draw the polygon with Leaflet.
\end{itemize}
\bigskip

\subsection{Twitter4JDesktop}
This is a simple Java application in which you can search for tweets with the REST API or subscribe to the Twitter's Streaming API (by default).
In order to use this application please do the following: uncomment the country in which you want to search tweets in the "main" of the \emph{Twitter4j.java}, then compile the application, launch \emph{Twitter4j.java} and follow the output instructions.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/twitter4jdesktop-prototype-application.png}}
\vspace{-20pt}
\caption{"Twitter4JDesktop" prototype application}
\end{center}
\end{figure}

Note that you may have to provide a PIN for the application if this is the first time you opened it on your computer (see the above "\nameref{twitter4j}" chapter).\\

All the Twitter4J documentation was used in order to develop this application and everything is well-commented so no other explanations were put here.
\bigskip

\subsection{Leaflet and Twitter4J}
This application receives streams of tweets and displays them on a map.\\

It contains two applications: \textbf{twitter4JWeb}, which is a Java application containing the \textbf{twitter4jDesktop} code and a web sockets server used to communicate with the second application \textbf{leafletAndTwitter}. This application is a JavaScript application displaying the received tweets from the web socket server in a map. The well-named "js/websocket.js" file contains the web socket client part.\\

Is order to correctly use them, please do the following:
\begin{enumerate}
	\item Uncomment the country in which you want to search tweets in the beginning of the "readStreaming" method in the \emph{Streaming.java} file of the "twitter4jWeb" application. You can also change the duration of the analysis below.
	\item Include the keyword you want to search at the end of the "initializeConfiguration" method in the \emph{Streaming.java} file. Try with the "job" keyword in the U.S.A. to get good results.
	\item Check that NetBeans and the GlassFish server are properly configured on your computer.
	\item Run the Java server "twitter4jWeb" in order to deploy and run the GlassFish server.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.75\textwidth]{figures/deployment-glassfish-server-twitter4jweb.png}}
	\vspace{-5pt}
	\caption{deployment of the GlassFish server in the "twitter4JWeb" prototype application}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	\item Open the \emph{leafletAndTwitter/index.html} file in your web browser when the server is correctly started.
	\item Then click on the "Start Streaming" button in the web application to start the streaming.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.75\textwidth]{figures/leafletandtwitter-prototype-application.png}}
	\vspace{-5pt}
	\caption{"LeafletAndTwitter" prototype application, using the "job" keyword in the U.S.A.}
	\end{center}
	\end{figure}
	\vspace{-20pt}
\end{enumerate}

Like before, the Twitter4J documentation was used in order to develop this application and everything is well-commented so no further explanations deemed necessary.
\bigskip

\subsection{Discussions}
These applications were used to properly test all libraries used within the project, especially the Leaflet and the Twitter4J libraries. By developing them, a substantial part of the project's work was accomplished in addition to allowing us to better understand and document these libraries.\\

Note that the \textbf{"Leaflet - Countries' Borders"} application will perhaps be put on a project apart on GitHub, because some people could need to use it.

\chapter{GeoTwit - Technical Documentation}
\label{geotwitApplication}
This chapter contains all documentation related to the \emph{GeoTwit} application's code.\\
See the "\textsl{\nameref{instruction}}" appendix in order to properly install the application.
\section{Mock-Up}
The following mock-up was produced with the open-sourced \emph{Evolus Pencil} application. In order to make the mock-up more comprehensible, it was separated into distinct sections; in order to have a global view, you can find the whole mock-up schema in the \emph{doc/analysis/GUI} file.
When the user first accesses the web site, he is redirected to the home page, where he is prompted to log into his Twitter account.
\begin{figure}[H]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/mock-up-connection.png}}
\vspace{-20pt}
\caption{mock-up - connection process}
\end{center}
\end{figure}
\newpage
\vspace*{\fill}
Once the connection is established, the user is redirected to the search page where he can search for tweet keywords in dynamic or static mode. First have a look at the \textbf{dynamic mode} (sometimes also called "streaming mode" in the application), which streams from Twitter's API.
\bigskip
\begin{figure}[H]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/mock-up-dynamic-mode-search.png}}
\caption{mock-up - main search page (dynamic mode)}
\end{center}
\end{figure}
\vspace*{\fill}
\newpage
\vspace*{\fill}
The user fills in the fields, and clicks on "Start Streaming!" to begin the streaming process.
\bigskip
\begin{figure}[H]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/mock-up-streaming-process.png}}
\caption{mock-up - streaming process}
\end{center}
\end{figure}
\vspace*{\fill}
\newpage
% Vertically center the text
\vspace*{\fill}
When observing the results, the user has access the "Charts" tab, which can provide interesting charts on the current streaming data.
\bigskip
\begin{figure}[H]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/mock-up-charts.png}}
\caption{mock-up - charts of a streaming process}
\end{center}
\end{figure}
\vspace*{\fill}
\newpage
\vspace*{\fill}
And now let's have a look at the \textbf{static mode}, which uses Twitter's REST API.
\begin{figure}[H]
\begin{center}
\fbox{\includegraphics[width=0.9\textwidth]{figures/mock-up-static-mode-search.png}}
\caption{mock-up - main search page (static mode)}
\end{center}
\end{figure}
\vspace*{\fill}
\newpage
\vspace*{\fill}
Here again the user must fill in the fields and click on the "View results!" button in order to access the results.
\begin{figure}[H]
\begin{center}
\fbox{\includegraphics[width=0.8\textwidth]{figures/mock-up-static-results.png}}
\caption{mock-up - view of the results in the static mode}
\end{center}
\end{figure}
There is no chart functionality in Static mode.
\vspace*{\fill}
\newpage

\section{Anatomy of the Application}
\begin{wrapfigure}{l}{0.35\textwidth}
\vspace{-25pt}
\begin{center}
\includegraphics[width=0.26\textwidth]{figures/anatomy.png}
\caption{anatomy of GeoTwit}
\end{center}
\vspace{-100pt}
\end{wrapfigure}
Here is the anatomy\footnote{More information here: https://www.playframework.com/documentation/2.5.x/Anatomy} of the application:
\begin{itemize}
\item \textbf{app}: contains all components related to the server, like controllers, views and assets.
	\begin{itemize}
	\item \textbf{assets}: contains Less\footnote{http://lesscss.org/} files, which are converted to CSS when the server compiles the code.
	\item \textbf{controllers}: contains all the application's controllers and actions, which act like standard API endpoints. It also contains action composition (see the "\nameref{actionsCompositions}" chapter below for more information).
	\item \textbf{views}: contains all views and sub-views of the application.
	\end{itemize}
\item \textbf{conf}: contains all the application's configurations, in particular the routes' configuration file.
\item \textbf{logs}: contains the application's log files.
\item \textbf{node\_modules}: contains all the client's NPM\footnote{https://www.npmjs.com/} packages; you can find more information about them in the "\nameref{usedJavascriptLibraries}" chapter below.
\item \textbf{public}: contains files related to the build properties of the project.
	\begin{itemize}
	\item \textbf{data}: contains json and shapefile data files, that respectively contain all the possible search languages and territorial data of nearly all of the world's countries.
	\item \textbf{javascript}: contains the Search page's JavaScript file as well as various JavaScript libraries (see the "\nameref{usedJavascriptLibraries}" chapter below for more details).
	\item \textbf{stylesheets}: contains the various JavaScript libraries' CSS files.
	\end{itemize}
\item \textbf{build.sbt}: this file is used to build the application and contains the application's description, library dependencies, resolvers and a filter used to automatically compile the Less code.
\end{itemize}
Other files and folders are not important and are largely used by Activator and SBT to start and compile the application.
\newpage

\section{Implementation Details - Server Side}
\subsection{UML Diagram of the Server}
Here is the UML diagram of the application's server, made with an old Bachelor project of HEIG-VD, \emph{Slyum}\footnote{https://github.com/HEIG-GAPS/slyum}:
\begin{figure}[H]
\begin{center}
\fbox{\includegraphics[width=0.8\textwidth]{figures/uml.jpg}}
\caption{UML diagram of the application's server, made with Slyum}
\end{center}
\end{figure}
The controllers' actions are laid out in this schema; the blocks attached at the bottom of the controllers are subclasses: "NotAuthenticatedAction" are action composition classes used to check if the user is correctly connected before accessing an action (see the "\nameref{actionsCompositions}" chapter below); "StreamingSocketActor" is the class that represents a WebSocket actor / thread (also see the "\nameref{webSockets}" chapter below).
Certain parameters of the "callback" method of the \textbf{HomeController} as well as the "writetweetInFile", "streaming" and "getStatictweets" methods of the \textbf{SearchController} were explicitly removed from the schema due to too much volume and space taken up by them. Here are the three methods' complete signatures:
\begin{itemize}
\item \emph{callback(denied: Option[String], oauthToken: Option[String], oauthVerifier: Option[String]): Result}
\item \emph{writetweetInFile(out: ActorRef, sessionId: String, keywordsSetIdentifier: String, internalId: Int, creationDate: String, longitude: Double, latitude: Double, user: String, content: String): Boolean}
\item \emph{streaming(out: ActorRef, twitterStream: TwitterStream, id: String, isAreaRectangle: Boolean, keywordsSetIdentifier: String, query: String, southwestCoordinates: Array[Double], northeastCoordinates: Array[Double], language: String): Unit}
\item \emph{getStatictweets(twitter: Twitter, query: Query, keywordsSetIdentifier: String, maximumNumberOfRequests: Int): JsArray}
\end{itemize}
The \textbf{SearchController} entity does not contain all attributes, as it would have significantly increased the project's complexity; the missing ones are the constant variables that contain the backup file's string values (\textit{\textsc{metadata\_string}}, containing "METATADA:", etc.).\\
As you can see, there are two controllers: \textbf{HomeController} and \textbf{SearchController}.\\

\textbf{HomeController} is related to all actions linked to the Home page and miscellaneous pages, and the connection process:
\begin{enumerate}
\item $\textbf{\underline{index}}\to$ displays the Home page and an error is the flash scope "error" is set (see the "\nameref{sessionsAndFlashScopes}" chapter).
\item $\textbf{\underline{auth}}\to$ occurs when the user clicks on the "Get Started" or "Connect" buttons of the Home page; redirecting the user to Twitter's connection page, giving it the URL of the callback function, which corresponds to the "callback" action below.
\item $\textbf{\underline{callback}}\to$ redirects the user either on the Search page if the connection was successful or on the Home page if there was an error or if the user denied the connection process. The status of the connection process depends on the three optional parameters: if "denied" is set, this means that the user denied the connection process and that the two following parameters are null. If "denied" is not set, this means that the two following parameters must have a value, respectively the token's string value (which will be used for the next requests to the APIs) and the "verifier" of the Twitter's OAuth process as a string (which will be asked by Twitter to verify the requests).\\
This action is called anyway by the Twitter's API when the user leaves the Twitter's connection page.
\item $\textbf{\underline{logout}}\to$ logs the user out and redirects him to the Home page.
\item $\textbf{\underline{about}}\to$ displays the About page; the user does not have to be authenticated to access this page.
\item $\textbf{\underline{help}}\to$ displays the Help page; the user does not have to be authenticated to access this page.
\end{enumerate}

\textbf{SearchController} is related to the Search page (search and display of the results); all parameters are explained directly in the code so they are not detailed here; all actions require the user to be connected:
\begin{enumerate}
\item $\textbf{isUserAuthenticated}\to$ checks if the user is correctly authenticated and returns a Boolean value; this method is used by Ajax actions in the "AuthenticatedAction" actions composition.
\item $\textbf{writeInFile}\to$ writes the given string value in the given file's name, and it used to write the data in the file-to-export.
\item $\textbf{validateAndParseFile}\to$ Validates and parses the given file-to-import in order to export its data within the application; the file must be a well-formatted GeoTwit file (".gt" extension, containing metadata, tweets and results).
\item $\textbf{writetweetInFile}\to$ writes the tweets whose information are given in parameters in the file to export.
\item $\textbf{streaming}\to$ starts a new Twitter's streaming process, according to the given parameters.
\item $\textbf{getStatictweets}\to$ collects the tweets associated to the given query with the given Twitter object, while not exceeding the specified upper limit of requests.
\item $\textbf{\underline{index}}\to$ displays the Search page, allowing the user to search for tweets with the Twitter APIs.
\item $\textbf{\underline{fileAction}}\to$ gets the file containing the previous streaming's results and either downloads or deletes it, depending on the given "action" parameter; returning a BadRequest result if the file or the given action does not exist.
\item $\textbf{\underline{uploadAndParseFile}}\to$ parses and validates the uploaded file to import.
\item $\textbf{\underline{streamingSocket}}\to$ this special action opens a WebSocket's connection (through the internal "StreamingSocketActor" class) between a new server's actor (thread) and the client when this one accesses this entity; this connection receives and sends json values.
\item $\textbf{\underline{staticResults}}\to$ gets and returns the results of the static mode search, when the user clicks the "View Results" of the "Static Mode" tab in the Search page.
\end{enumerate}
More information about these actions and methods is further detailed in the following chapters.
\bigskip

\subsection{Behavior of the Views}
All action's views inherit from one of these two interface templates:
\begin{itemize}
	\item \emph{mainIndex.scala.html} if the current page is the home page. It contains only the header and footer partials and calls the page's content.
	 \item \emph{main.scala.html} for all other pages; in addition to containing the page's header and footer, it also contains a page content format, which allows the views to have the same page format (page's header, content, page's footer, etc.) on all pages.
\end{itemize}
As mentioned above, there is a header (\emph{header.scala.html}) and a footer (\emph{footer.scala.html}) partial templates, which are loaded in all templates. They respectively contain all HTML headers (page title, CSS and libraries loading, etc.) and footers (closing tags).\\

The \emph{search.scala.html} search page's view is separated in three partial templates: \emph{searchContent.scala.html}, which contains the search fields of both dynamic and static modes, and \emph{searchStreamingResults.scala.html} and \emph{searchStaticResults.scala.html}, which respectively contain the components of the dynamic and static results' pages.
\bigskip

\subsection{Configuration}
Configuration files are located in the "/conf" folder. You can find the routes' configuration file in here, as well as the application's and Twitter's configuration files.\\

The Twitter file contains the consumer's key and secret code used to make requests to the APIs. They are in a separate file in order to avoid security issues: the developer can only push the application's configuration file (which contains non-confidential data) in the GitHub repository and keep the Twitter file local.
\newpage
In order to be able to read the configuration in the code, the developer has to inject a Configuration object into the controller of his choice:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/configuration-injection.png}}
\vspace{-20pt}
\caption{injection of a Configuration object in a controller}
\end{center}
\end{figure}

\subsection{Routes}
\subsubsection{Play Router}
Routes\footnote{More information here: https://www.playframework.com/documentation/2.5.x/ScalaRouting} are set in the "/conf/routes" file, and are declared with three distinct parameters: the HTTP method (GET, POST, etc.), the URL of access ("/auth", etc.) and the controller and action in which the client will be redirected ("controller.HomeController.auth", etc.) and which can have parameters. They will be used by the server's routing process in order to determine the action required with the given URL. Note that there is also a route configured for the asset files (images, CSS, JavaScript, etc.), so the client can access all these files.
\bigskip

\subsubsection{JavaScript Routes}
In addition of the Play's main Scala routes and according to the documentation\footnote{https://www.playframework.com/documentation/2.5.x/ScalaJavascriptRouting}, the Play router is able to generate JavaScript code to handle routing from JavaScript running client side back to the application. The JavaScript router aids in refactoring the application; for example, when a URL or a parameter's name is changed, the JavaScript code will automatically get the new structure.
In GeoTwit, JavaScript routing is used in the Search page of the application. Specifically, JavaScript routes are declared in the "search.scala.html" view and are used in the "search.js" file:
\begin{itemize}
	\item \textbf{search.scala.html}: JavaScript routes are declared with a helper that generate the JavaScript code of a router named \emph{jsRoutes}. This router contains five routes: the route of the Logout page (in order to redirect the user from the JavaScript code if he is not connected anymore), the one of the "streamingSocket" of the SearchController (in order to initialize a web socket's connection with the server, see the "\nameref{webSockets}" chapter below), the "fileAction" action (in order to either delete of download the file-to-export from the JavaScript code), the "staticResults" action (in order to ask the server for the static mode's results), and finally a router for the asset files (in order to access images, fonts and other public assets).
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.55\textwidth]{figures/javascript-routes-declaration.png}}
	\vspace{-5pt}
	\caption{declaration of JavaScript routes in the Search page's view}
	\end{center}
	\end{figure}
	In order to make the helper work, the developer has to include the request's header as a parameter of the view:
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.86\textwidth]{figures/implicit-request-inclusion.png}}
	\vspace{-5pt}
	\caption{declaration of JavaScript routes in the Search page's view}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	\item \textbf{search.js}: JavaScript routes are used with the \emph{jsRoutes.controllers.[CONTROLLER\_NAME]. [ACTION\_NAME]()} code. If one wants to access an asset entity (like an image or a font), it must replace the controller and action's names by the \emph{Assets.versioned('ASSET\_NAME')} code. It is then possible to get the URL of the current entity with the "url" method (which will for example generate the following link: "http://localhost:9000/logout") or also get a web socket's URL with the "webSocketURL" method (which could generate the following link: "ws://localhost:9000/streamingSocket").
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.86\textwidth]{figures/javascript-route-redirection.jpg}}
	\vspace{-5pt}
	\caption{example of redirection to the Logout page with a JavaScript route}
	\end{center}
	\end{figure}
\end{itemize}


\subsection{Sessions and Flash Scopes}
\label{sessionsAndFlashScopes}
In Play Framework, sessions do not really act like anyone might guess, because of the stateless status of the framework: according to the documentation\footnote{https://www.playframework.com/documentation/2.5.x/ScalaSessionFlash}, data is in fact not stored by the server, but is added to each subsequent HTTP request, using the cookie mechanism. This means that we can only store string values, and not objects. Since this limitation is very annoying, the developer has to use the cache mechanism to store objects (like the Twitter's signed object used to make requests in GeoTwit), by adding a unique ID related to the current user's session for each cached data instance (see the "\nameref{cache}" chapter below).\\

Each value stored in the session is identified by a string key and has a string value (like a "$String\to$ String" map). In order to add a value to the session, the developer has to use the "withSession" method right after the action's result (\emph{Ok}, \emph{Redirect}, etc.). Since the sessions' data is added in each HTTP request, he has to get the current request's session and put it first in the new request's session in order to keep the older session's data.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.86\textwidth]{figures/session-add-string-example.jpg}}
\vspace{-5pt}
\caption{example of addition of a string value to the session}
\end{center}
\end{figure}
\vspace{-10pt}

If one wants to set a new session, the app will just ignore the current request's session by removing the "request.session" call. In order to discard the whole session (during a disconnection for example), use the "withNewSession" method right after the action's result.\\

The session's time-out value is set in the "conf/application.conf" configuration file, specifically in the "play.http.session.maxAge" section. The value currently is 7 days.
\newpage
In GeoTwit, there can be two types of information contained in the session:
\begin{itemize}
	\item \textbf{id}: a unique ID used to identify the current user's session and its cache objects. This ID is used to name and identify these cache objects (since the cache is not related to one user initially and is available across users). This ID is generated with the Java's UUID generator and is set in the "auth" action of the \textbf{HomeController}.
	\item \textbf{username}: the username of the connected user, which is displayed in the header of the Search pages and is used to know if the user is connected or not (coupled with the cache - see the "\nameref{actionsCompositions}" chapter below). This value is set right after the connection with Twitter, in the "callback" action of the \textbf{HomeController}.
\end{itemize}

In addition of sessions, the framework also offers another tool: the flash scope. This mechanism works like sessions, with the difference that data is kept for only one request. It is used in GeoTwit to forward success/errors codes when an action redirects the user to the Home page.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/flash-scope-use.jpg}}
\vspace{-20pt}
\caption{example of use-case of a flash scope in GeoTwit}
\end{center}
\end{figure}
\vspace{-10pt}

The index action of the \textbf{HomeController} gets the flash message and sends it to the view, which displays (or not) the right message, according to the status. If there is no error, the controller just sends a "success" message to the view.

\subsection{Cache}
\label{cache}
As mentioned before, the cache\footnote{https://www.playframework.com/documentation/2.5.x/ScalaCache} is used to store objects that cannot be stored in sessions. Though, there is one main problem with this method: each time the server compiles a new code, the cache is cleared so the currently connected user will be disconnected. When the application will be put in production environment, this problem will occur much less frequently.\\

In order to use the cache, add the "cache" library dependency in the \emph{build.sbt} file (normally already done by default), then inject a \emph{CacheApi} object in the controller in which you want to use the cache functionalities:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/cacheapi-injection.jpg}}
\vspace{-20pt}
\caption{injection of a CacheApi object in a controller}
\end{center}
\end{figure}
\vspace{-10pt}

It is then possible to use this "cache" object to read and write the cache.\\
When an object is added in the cache, the developer can set a time value, which will determine the period of validity of this object; once the limit is reached, the object will be removed from the cache.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.8\textwidth]{figures/cache-timer.jpg}}
\vspace{-5pt}
\caption{example of addition of objects in the cache, for a 2 minutes' period}
\end{center}
\end{figure}
\vspace{-10pt}

\newpage
In order to read a cache object, this one must be converted to the right type (a RequestToken object in the example below):
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.8\textwidth]{figures/cache-reading.jpg}}
\vspace{-5pt}
\caption{example of reading a cache object}
\end{center}
\end{figure}
\vspace{-10pt}

The developer has to think about testing the collected object's existence, in order to ensure it is still available.\\

Here are the different objects that can be stored in the cache in GeoTwit ("[id]" represents the unique ID generated for the current session, in order to differentiate the cache objects stored on the server side between each connected client):
\begin{itemize}
	\item \textbf{[id]-tmpTwitter} (2 minutes' validity)\\
	Contains the \textbf{Twitter} object used to get the \underline{request} token and the Twitter's authentication URL. This object is stored in the "auth" action of the \textbf{HomeController} and is collected in the "callback" action right after the Twitter's connection. It has to be stored, because it has already been signed with the \underline{request} token, and still has to be signed with the \underline{access} token in the callback, in order to be able to make requests to the APIs. This object is thus reused to build the final \textbf{Twitter} object, and since it is not possible to build a new object for security issues, it has to be kept.\\
	If this object times out before the user reached the callback, he will be redirected to the Home page with an explicit error.
	\item \textbf{[id]-requestToken} (2 minutes' validity)\\
	Also stored in the "auth" action and used in the "callback" action, this object contains the current user's request token, which will be used to generate the access token. If this object times out before the user has reached the callback, he will be redirected to the Home page with an explicit error.
	\item \textbf{[id]-twitter} (currently has a 7-day validity, which is the same value as the session's time-out and is stored in the application's configuration file)\\
	Contains the \textbf{Twitter} final and signed object, which will be used to make requests to the Twitter's APIs in the \textbf{SearchController}. It is set in the "callback" action of the \textbf{HomeController}, right after the two cached objects above have been removed. If this object expired for whichever reason, the user will be considered as disconnected and thus will be redirected to the Home page.
\end{itemize}

\subsection{Twitter4J}
Twitter4J is used in the \textbf{HomeController} and is automatically loaded by SBT with the \emph{build.sbt} file. As mentioned before, this file contains the Twitter4J's library dependency:
\begin{lstlisting}
	org.twitter4j" % "twitter4j-core" % "4.0.4
\end{lstlisting}
The first string is the library's location, the second one is the library's name and the last one is the library's version. In order to use the streaming API, the file also has to contain the Twitter4J's stream library:
\begin{lstlisting}
	org.twitter4j" % "twitter4j-stream" % "4.0.4
\end{lstlisting}

\subsection{Connection Process}
In order to properly implement the connection process, the code is based on the \emph{yusuke}'s implementation example of a connection process to Twitter with the Twitter4J library in Java\footnote{Available on GitHub here: https://github.com/yusuke/sign-in-with-twitter} as well as a \emph{Qiita}'s tutorial\footnote{http://qiita.com/omiend/items/90163d29b465fb7ab8f0}.\\

When the user clicks on the "Get Started" button located in the Home page, the following steps are operated:
\begin{enumerate}
	\item The click action calls the "auth" method of the \textbf{HomeController}.
	\item This action uses the Twitter4J library to make an authentication's request, in order to get the OAuth token and the authentication URL that points to the Twitter's authorization page. When making the request, the application also gives the API a callback URL (\emph{/callback} => action "callback" of the \textbf{HomeController}), which will be called at the end of the authentication. This action also stores the temporary Twitter's and request token's objects in the cache for 2 minutes, in order to properly use them in the callback action.
	\item Redirection of the user to the Twitter's authorization page.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/twitter-authorization-page-in-geotwit.jpg}}
	\vspace{-5pt}
	\caption{Twitter's authorization page in GeoTwit application}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	\item Whatever the user does, he will be redirected on the callback action:
	\begin{enumerate}
		\item If the user successfully authorized the application, the callback receives the OAuth token and verifier from Twitter. It will then get and memorize the OAuth \underline{access} token (according to the temporary Twitter and \underline{request} token's objects received from the "auth" action, as well as the received verifier from Twitter), add the new Twitter object in the cache for 7 days (same time as the session's time-out) in order to use it in the \textbf{SearchController} to make requests to the API, and then finally redirect the user to the "search" action, passing the username in the session (in order to display it on the pages' header).
		\item If something went wrong, the user is redirected to the Home page with an error.
	\end{enumerate}
\end{enumerate}
\newpage

\subsection{Actions Compositions}
\label{actionsCompositions}
Actions compositions\footnote{More information here: https://www.playframework.com/documentation/2.5.x/ScalaActionsComposition} can be interpreted as generic action functionalities, allowing the developer to execute generic code in several actions without writing it again. In summary, this code is executed before the execution of the action itself; compositions thus act like filters (which contain code executed before any action when a request is made to the Play server), but for specific actions, and can be chained by forwarding the HTTP request through a pipeline-like process.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.9\textwidth]{figures/filters-actions-compositions.png}}
\vspace{-5pt}
\caption{pipeline-like schema of filters and actions compositions, made with Evolus Pencil}
\end{center}
\end{figure}
\vspace{-10pt}
In the above schema, the client first makes a request to the Play's application server, which sends it to the first filter. This filter executes its code and can perhaps modify the request's content; once done, he forwards the updated request to the next filter, and so on. When the request is finally forwarded to the right controller's action (through the routing process), this one calls the actions compositions related to itself, which alternately execute their code and can also update the request. Once the last action composition finishes its process, the request is finally forwarded to the action, whose body's code is executed and in which a result (for example the call of the view with the \emph{Ok} keyword) will be provided.
\newpage
In GeoTwit, actions compositions are used to check if the user is connected or not before accessing pages: for all the \textbf{HomeController} actions (except the "logout", "about" and "help" ones), the user is redirected to the Search page if he is already connected; on the opposite, if the user is not connected and tries to access a \textbf{SearchController}'s action, he will be redirected to the Home page with an error message. In order to determine if a user is disconnected or not, the actions compositions check if the "username" value of the session is set and if a \emph{[id]-twitter} object exists in the cache for the current session (where "[id]" represents the unique ID of the current session). If one of these two values does not exist (or if the unique ID is not set), the user is considered disconnected.\\

These actions compositions are objects extending the \textbf{\emph{ActionBuilder}} trait (the \textbf{\emph{NotAuthenticatedAction}} object in the \textbf{HomeController}, and the \textbf{\emph{AuthenticatedAction}} object in the \textbf{SearchController}), and implementing a "invokeBlock" method, which is called for every action built by the \textbf{\emph{ActionBuilder}}.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/actions-composition-example.jpg}}
\vspace{-20pt}
\caption{an example of an actions composition}
\end{center}
\end{figure}
\vspace{-10pt}

The "request" parameter of the "invokeBlock" method corresponds to the current HTTP request object. The "block" parameter is a function used to wrap the request, by giving a future result to the HTTP request and thus allowing the request to be chained until the last actor (the action itself).\\

Actions are then defined with these objects, in order to execute the "invokeBlock" method code before the action itself.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.45\textwidth]{figures/action-using-actions-composition.jpg}}
\vspace{-6pt}
\caption{an action using an actions composition}
\end{center}
\end{figure}
\vspace{-10pt}

\subsection{Web Sockets}
\label{webSockets}
According to documentation\footnote{https://www.playframework.com/documentation/2.5.x/ScalaWebSockets}, web sockets are sockets that can be used from a web browser based on a protocol that allow a two-way full duplex communication. Both the client and the server can send and receive messages at any time, as long as there is an active web socket connection between them. In summary, one can use this technology to forward real-time messages from a client to a server and vice-versa; in GeoTwit, web sockets are used to forward data during the streaming process.
\newpage
In order to make the web socket's communication work, the application thus must have a web socket's client and server entities. In GeoTwit, the client is a JavaScript part located in the "search.js" file (modern HTML5 compliant web browsers natively support web sockets via JavaScript), and the server is located in the \textbf{SearchController}. Play provides two different mechanisms for handling web sockets: by using Akka Streams and by using iteratees. Since this second method is deprecated, it will not be explained below and Akka is the one which is used in the application.\\

Since it would be annoying to split the server and the client parts' explanations, they are both explained here.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/web-sockets.png}}
\vspace{-20pt}
\caption{diagram of the web sockets' process is Play Framework, made with Evolus Pencil}
\end{center}
\end{figure}
\vspace{-15pt}

In order to be able to send web sockets between a client and the server, first a connection has to be established. In GeoTwit, the following process is executed:
\begin{enumerate}
	\item The client has to open a new web socket connection to the server, with the web socket protocol (\emph{ws://[URL\_OF\_SERVER]}). A route is declared in the server's router in order to redirect the client's request to the right entity (which is not exactly an action in this case, but a \emph{WebSocket} object).
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.86\textwidth]{figures/web-socket-connection-openning.jpg}}
	\vspace{-5pt}
	\caption{opening of a new web socket connection from the client to the server}
	\end{center}
	\end{figure}
	\vspace{-20pt}

	\item The \emph{WebSocket} object of the server acted like an action until now, but no longer. Once it received the client's connection request, it checks that the current user is still authenticated (for security reasons) and if so, it creates a new \emph{Actor} object through an \emph{ActorFlow} object that allows the developer to access a "out" \emph{ActorRef} object that will itself be used to send messages to the client. This \emph{Actor} object acts like a Java Thread, which means it will keep a persistent connection with the client until request to close the connection (web browser's closing, reload of the current page, expressed web socket message indicating the server that the connection has to end, etc.), and which also means that the server can run other processes at the same time. In summary, the \emph{WebSocket} object acts as a thread manager while the \emph{Actor} objects act like threads; there can indeed be several current \emph{Actor} instances running at the same time, but only one connection's \emph{WebSocket} object, which ends as soon as the \emph{Actor} is created.\\
	In GeoTwit, the \emph{WebSocket} object also gives the \emph{Actor} the session's unique ID, so it will be able to access the current user's cache object. Note that the \emph{WebSocket} object (and thus the \emph{Actor} one) is configured to only accept and send Json data (in Play: \emph{JsObject/JsValue} objects\footnote{https://www.playframework.com/documentation/2.5.x/ScalaJson}), so it will be easier to identify and deal with web sockets' types. If the user is not connected anymore when the server receives a connection request, this will send back a Forbidden result that will make the client disconnect the user.
	\item Once the \emph{Actor} threaded object is created, it immediately sends a "successfulInit" message to the client, in order to notify it that the connection was successfully opened. As said before, only Json objects can be sent and received in GeoTwit.\\
	
	In order to send sockets to the client, the \emph{Actor} object has to use the given \emph{ActorRef} ("out") object:
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.86\textwidth]{figures/web-socket-sending-server-to-client.jpg}}
	\vspace{-5pt}
	\caption{example of a web socket's path from the server to the client}
	\end{center}
	\end{figure}
	\vspace{-20pt}

	When a message is sent from the client, the \emph{Actor} receives it with the "receive" method. This method gets the received json message's type and process actions depending on it.\\
	Finally, the "postStop" method is triggered when the web socket's connection ends, and is used to properly close the ongoing streaming process. In order to manually ends the \emph{Actor} threaded object, the developer has to send a \emph{PoisonPill} object to the \emph{ActorRef} ("out") object with the following command:
	\begin{lstlisting}
		out ! PoisonPill
	\end{lstlisting}

	\item On the client side, everything is managed with the \emph{socketConnection} object, initialized when the client first sent a connection request to the server. This object contains several methods, among which the "onmessage" one that is triggered when the client receives a message from the socket. Like in the server side, when a message is received, the client first gets the received json message's type and process actions depending on it.
\end{enumerate}
\bigskip
Here are the messages that can be sent \underline{from the server to the client} in GeoTwit (the first Json object's attribute is the message type, sometimes followed by various parameters):
\begin{itemize}
	\item \textbf{\{"successfulInit"\}}: sent when the client's connection request has been successfully received by the server; when the client receives it, it shows the results page, draws the selected area (rectangle or complex polygon) on the results map and sends back a "readyToStream" message to the server.
	\newpage
	
	\item \textbf{\{"newTweet", \textit{keywordsSet}, \textit{internalId}, \textit{creationDate}, \textit{longitude}, \textit{latitude}, \textit{user}, \textit{content}, \textit{nbReceivedtweets}\}}: sent when a new tweet has been received through the Twitter's streaming process; the client adds the tweet to the map, displays it in the results panel, and sends a confirmation to the server, indicating that the tweet belongs to the country's territories (only if the user selected the country in the drop-down menu ; see the client's "readyToStream" and "tweetLocationConfirmation" web sockets for more information).\\The "keywordsSet" parameter indicates the keywords set identifier (either "first" or "second") to which the tweet belongs (so the client can display tweets with different colors). "internalId" contains the internal ID of the tweet, used to identify tweets in the file-to-export. "creationDate" contains the creation date of the tweet, "longitude" and "latitude" are the tweet's geographic coordinates, "user" contains the name of the user who posted the tweet and "content" contains the tweet's content. Finally, "nbReceivedtweets" contains the current number of received tweets (with or without geolocation tags), used by the graphs.
	\item \textbf{\{"errorFile"\}}: sent when an error occurred during the creation/writing of the file-to-export by the server. An alert is displayed in order to inform the user, but the streaming process continues.
	\item \textbf{\{"stopStreaming", \textit{reason}\}}: sent when the server stops the streaming process (mainly in cases of exception); triggers the client to indicate to the user that the streaming has ended. The "reason" is an optional parameter that indicates the reason of this interruption, which can be:
	\begin{itemize}
		\item \emph{sessionExpired}: when the server cannot access the Twitter's cached object anymore, which means that the current user's session has expired; the client redirects the user to the Logout page in order to properly disconnect him. This error can only happen at the beginning of the streaming process, when the server tries to initialize the streaming connection with the Twitter's API; once done, the cached object is not used anymore and the streaming can continue (in order to avoid unwanted interruptions during the process).
	\item \emph{tooManyStreamingProcesses}: when the user ran too many (more than two) copies of the same application that are authenticated with the same account.
	\item \emph{queryTooLong}: when the user typed too many characters in one of the keywords sets (more than 60 in one \underline{OR} phrase - see the "\nameref{buildingTwitterQueryString}" chapter of the client side below); the search page is reloaded in order to make a new search.
	\item \emph{exception}: when a non-handled exception occurred; displays an alert window.
	\end{itemize}
\end{itemize}
\bigskip
\newpage

Here are now the messages than can be sent \underline{from the client to the server}:
\begin{itemize}
	\item \textbf{\{"readyToStream", \textit{isAreaRectangle}, \textit{firstKeywords}, \textit{secondKeywords}, \textit{coordinates}, \textit{language}\}}: sent as soon as the client received a "successfulInit" message from the server; when the server receives this message, he starts the Twitter's streaming process(es) with the given user's input parameters. "isAreaRectangle" indicates whether the user manually drew a rectangle on the map (\emph{true}) or selected a country in the drop-down menu (\emph{false}); if \emph{false}, this value indicates that the server must wait for the client to send a confirmation that indicates that the received tweet is in the geographical area that represents the country's territories (since the client only gave the bounding box to the server, in order to avoid to overload the connection - see the figure below), in order to only write tweets belonging to the area in the file to export; if the value is \emph{true}, the server will automatically write tweets in the file, without asking the client for a confirmation. "secondKeywords" can be empty, since it is an optional input. "language" contains a BCP 47 language identifier\footnote{https://en.wikipedia.org/wiki/IETF\_language\_tag} ("en", "fr", etc.) that corresponds to the value of the language the user selected to filter tweets (if empty, the system will simply not filter tweets by language).	The "coordinates" parameter is an array that contains each coordinate of the rectangle's corners; this rectangle can either be the rectangle the user manually drew, or the rectangle bounding all the selected country's territories (since a country can have many of them) if the user selected a country in the drop-down menu. Here is an example of a bounding rectangle for French territories (aka France and Corsica):
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/france-bounding-rectangle-example.jpg}}
	\vspace{-5pt}
	\caption{example of a rectangle bounding French territories}
	\end{center}
	\end{figure}
	\vspace{-20pt}

	Since it is not possible to send huge amounts of data like the coordinates of all the country's borders through the web sockets system (and even if potentially possible, it is not optimal), the client only sends this bounding rectangle. Once received, the server will search for tweets contained within this rectangle, and will send them to the client, which will finally check that the tweet belongs to one of the country's territories (see the client side's "\nameref{receptionOftweetsFromTheServer}" chapter below for more information).

	\item \textbf{\{"currentResults", \textit{elapsedTime}, \textit{gtrt}, \textit{grt}, \textit{gprt}, \textit{atrt}, \textit{art}, \textit{agvw}\}}: sent each second by the client; gives the server the current results (the elapsed time, and the data related to each of the 6 charts: $\textbf{gtrt}\to$ Total of received geolocated tweets; $\textbf{grt}\to$ Reception of geolocated tweets; $\textbf{gprt}\to$ Parts of the received geolocated tweets; $\textbf{atrt}\to$ Total of all (with and without geolocation) received tweets; $\textbf{art}\to$ Reception of all tweets; $\textbf{agvw}\to$ tweets with geolocation tags vs. tweets without; more information about charts in the "\nameref{charts}" chapter of the client side's section). The server internally saves and overwrites these results, in order to write them at the end of the backup file once the streaming process has ended.
	
	\item \textbf{\{"tweetLocationConfirmation", \textit{keywordsSet}, \textit{internalId}, \textit{creationDate}, \textit{longitude}, \textit{latitude}, \textit{user}, \textit{content}\}}: sent by the client in order to confirm that the received tweet (received through the "newtweet" web socket) belongs to the selected country's territories, in order for the server to write it in the file to export. This socket is only sent if the user selected a country in the drop-down menu; indeed, only the bounding box is sent to the server at the beginning of the streaming process, in order to avoid overloading it (see the "readyToStream" web socket). If the tweet does not belong to the territories, the client simply ignores it. The "keywordsSet" parameters indicates which subject the tweet-to-confirm belongs to (either "first" or "second"), "internalId" is used to internally differentiate the tweets to write and results from an internal server's counter, and "creationDate" is the tweet's creation date, as a \emph{YYYY-MM-DD'T'hh:mm:ss} format. Other parameters are the basic tweet's parameters.
	\item \textbf{\{"stopStreaming"\}}: sent when the user clicks on the "Stop Streaming" button; indicates to the server that he can kill the current \emph{Actor} and stop the ongoing streaming process.
\end{itemize}
\newpage

Here is a summary of the communication that occurs between the server and the client:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.53\textwidth]{figures/web-sockets-in-geotwit.png}}
\vspace{-5pt}
\caption{web sockets' operation diagram in GeoTwit, made with Evolus Pencil}
\end{center}
\end{figure}

\subsection{Streaming Processes}
Once the client sends all the data related to the streaming process(es) through the "readyToStream" web socket, the current web socket's \emph{Actor} of the server validates the user parameters and checks that the user is still connected and only then, it starts the first streaming process through the Twitter API with the Twitter4J library. If the user filled the second keyword set/subject, the \emph{Actor} starts a second streaming process request. As a reminder, the Twitter API only authorizes two simultaneous streaming processes per user, so it is not possible to add new keywords sets for now. The streaming process is located in the "streaming" method of the \textbf{SearchController}, and initializes a listener subscribed to a server of the Twitter's Streaming API with the both the right query parameters and a \emph{TwitterStream} object initialized from the cached Twitter object. Once a tweet is received, it will trigger the "onStatus" method of the listener object, which will check if the tweet has a geolocation tag and secondly, if it is located in the desired rectangle. If valid, the \emph{Actor} will send a "newtweet" web socket to the client.\\

If a user deletes a tweet previously posted, the information will be received in the "onDeletionNotice" method of the listener object; Though deletions are ignored in GeoTwit, the tweet still deserves to appear in the results despite deletion. If an exception occurs during the streaming process, the "onException" method of the listener object will be triggered. This one tries to convert the \emph{Exception} object into a \emph{TwitterException} object, in order to get the exception's status code and thus to properly inform the client about what happened through the "stopStreaming" web socket.\\

Further details on each step can be found in the extensively commented code, as it would be wasteful to go into them here.

\subsection{Generation and Export of Files in Streaming Mode}
\label{generationOfStreamingFile}
At the end of a streaming process, the user is prompted to export the backup file of the analyzed subject(s) generated during the process. Both the client and the server take action during this process, but everything is explained here, in order to avoid confusion.\\

To begin with, here is the format of the generated text file:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/generated-file-format.jpg}}
\vspace{-5pt}
\caption{format of the generated text file that contains streaming process(es) data}
\end{center}
\end{figure}

\textit{Spaces and tabulations were added in this example in order to make the file more readable; in real files, these are not included, in order to gain space.}\\

And here are the explanations:
\begin{itemize}
	\item \textbf{METATADA}: this section contains the metadata of the current streaming process and is located at the beginning of the file. The metadata contains:
	\begin{itemize}
		\item the \textbf{first subject's search-string};
		\item the \textbf{second subject's search-string}, only if the user inputs a second keyword set; if not, this line does not exist;
		\item the \textbf{language} used to filter the tweets ("ANY" if the user did not select a specific language);
		\item the \textbf{coordinates} used to filter the tweets, as a string-representation of an array of arrays of double (see the illustration above for an example).
	\end{itemize}

	\item \textbf{TWEETS}: this section contains the data of all received geolocated tweets, and is located after the metadata. There is strictly one tweet per line, and each tweet contains the following information, separated by semicolons:
	\begin{itemize}
		\item the \textbf{identifier} of the current tweet, which is formatted as the following: \\"\emph{[SUBJECT\_IDENTIFIER]-subject-\#[TWEET\_ID]}", where \emph{[SUBJECT\_IDENTI-FIER]} corresponds to the string identifier of the subject with which the tweet has been found (either "first" or "second"), and where \emph{[TWEET\_ID]} corresponds to the local counter value of tweets received for the current subject until now (in order to identify tweets of a same subject between them);
		\item the \textbf{date and time} that represent the creation of the current tweet, following a \emph{YYYY-MM-DD'T'hh:mm:ss} format, where '\emph{T}' is a simple character used to separate the date from the time (and which is used by the Scala parser in order to convert this string into a date);
		\item the \textbf{longitude} of the tweet;
		\item the \textbf{longitude} of the tweet;
		\item the \textbf{name of the user} who posted the tweet, bounded by quotation marks (in case the user's name contains a semicolon);
		\item the \textbf{text content} of the tweet, also bounded by quotation marks for the same reason; if a tweet's content contains line breaks, they are automatically removed.
	\end{itemize}

	\item \textbf{RESULTS}: this section contains the results of the streaming process, and is located at the end of the file, after the tweets. The results contain:
	\begin{itemize}
		\item the \textbf{time} the process lasted, formatted in \emph{hh:mm:ss} format;
		\item the \textbf{graphs' data} (one line per graph: $\textbf{GTRT}\to$ Total of received geolocated tweets; $\textbf{GRT}\to$ Reception of geolocated tweets; $\textbf{GPRT}\to$ Parts of the received geolocated tweets; $\textbf{ATRT}\to$ Total of all (with and without geolocation) received tweets; $\textbf{ART}\to$ Reception of all tweets; $\textbf{AGVW}\to$ tweets with geolocation tags vs. tweets without). All graphs have a two-dimensions array of two arrays of numbers, except the GPRT one, which only have a simple array of numbers. If a chart does not exist (if there is only one subject the GPRT one will not exist, for instance) or is empty, an empty array is written (for example: "\underline{[[],[]]}", or "\underline{[]}" for the GPRT).
	\end{itemize}
	Since all the collected tweets are public, there is no confidentiality issues about the storage of this data in an external file.
\end{itemize}

During the process, the file is initially created in the "writeInFile" method of the \textbf{SearchController}, if it does not already on the server side. This method writes the given string value in the file whose name corresponds to the given string. For the streaming process, the files are located in the "/tmp" file and are named as the following: \emph{streaming-[ID].gt}, where \emph{[ID]} corresponds to the unique ID of the current session of the connected user. Every connected user thus has a file having a unique name for his current session. This also means that there can only be one file per session, which allows the server to not be overloaded by files. \emph{gt} is the self-assigned official extension of GeoTwit files, but is just a text file in reality. Note that the file is deleted once it has been downloaded or rejected, as explained later in this chapter.\\

The first time the "writeInFile" method is called (and thus the file is created), it occurs in the "receive" method of the \emph{StreamingSocketActor} class of the \textbf{SearchController}, when the "readyToStream" web socket type is received and when the user's inputs have been validated by the server. This first call allows the server to write the metadata of the current streaming process at the beginning of the file.\\

Then, this method is called each time the server receives a new tweet from the Twitter's server, either in the "onStatus" method of the listener located in the "streaming" method of the \textbf{SearchController}, or in the "receive" method of the \emph{StreamingSocketActor} class, when the actor received a "tweetLocationConfirmation" web socket type (depending on whether user manually draws a rectangle on the map or selects a country from the list - more information in the "\nameref{webSockets}" chapter above). This call writes the received/confirmed tweet in the file.\\

Each second, the client sends the current elapsed time and the chart results to the server (with the "currentResults" web socket), which overwrites them each time. As soon as the streaming process is stopped (either because an exception was thrown or because the user clicked the stop button), the server writes the last received results at the end of the file (still with the same "writeInFile" method). Simultaneously, the client asks the user if he wants to export the generated file, in the "stopStreaming(...)" function of the \emph{search.js} file. If the user says \emph{yes}, he will be able to download the file; if he says \emph{no}, the file will be removed from the server, in order to keep it clean at all times. Note that the user is given a prior warning about the results of his decision.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.7\textwidth]{figures/export-data-pop-up.jpg}}
\vspace{-5pt}
\caption{pop-up asking the user if he wants to export the data as a text file (French buttons are due to the browser's default language)}
\end{center}
\end{figure}
\newpage

In order to easily download the file and to get a feedback about the download process (success/failure), the \emph{johnculviner}'s "jquery.fileDownload" Jquery's download library\footnote{https://github.com/johnculviner/jquery.fileDownload} was used. It allows the developer to use an Ajax-like file-download experience, which is normally not (easily) possible by using the web.\\
The developer just needs to call the library, give it the URL of the action from which the user can download the file. In GeoTwit, this action is the "fileAction" action of the \textbf{SearchController}, which is used either to download or delete the current user's file (you can find more information in the following paragraphs). Here is a sample of code, which is used to download the file:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.7\textwidth]{figures/jquery-filedownload-sample.jpg}}
\vspace{-5pt}
\caption{sample of code used to download the generated file}
\end{center}
\end{figure}

Once the download is finished, the file is immediately removed from the server, thanks to the "deleteFile" function of the \emph{search.js} file. If an error occurs, an alert message containing the URL of the download page is displayed to the user, so he can manually download it. Note that the next time the user will press the "Start Streaming!" button, the file will automatically be removed (in order to avoid storing useless files on the server side).\\

The "deleteFile()" function only sends an Ajax request to the "fileAction" action of the \textbf{SearchController}, in order to delete the file. It does not wait for a response from the server.\\

Regarding the "fileAction" action of the \textbf{SearchController}, this one either downloads or deletes the current user's file, according to its first string parameter (which can either be "download" or "delete"). This action also needs the string(s) that represents the keywords set(s), in order to properly name the file to download. It first gets the file on which the action will be executed, and checks if this file exists. If not, a \emph{BadRequest} response is returned; if yes, the server performs the intended action. If the file needs to be deleted, the server simply executes the "delete" method of the file object; if the file needs to be downloaded, the server has to serve it to the client. In order to accomplish this task, the action must render a file instead of an HTTP result\footnote{More information here: https://www.playframework.com/documentation/2.5.x/ScalaStream}:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.6\textwidth]{figures/rendering-file-in-action.jpg}}
\vspace{-5pt}
\caption{rendering of a file at the end of an action}
\end{center}
\end{figure}
\newpage

The content contains the \emph{java.io.File} object to send; the file name contains the name of the file that the user will see in the download pop-up, and is formatted as the following: \\\emph{[SUBJECT\_1][\_[SUBJECT\_2]]\_STREAMING\_[DATETIME].gt}, where \emph{[SUBJECT\_1]} and \emph{[SUBJECT\_2]} (optional) represent the keywords sets' strings, and where \emph{[DATETIME]} represents the current date and time, following a \emph{YYYY-MM-DD'T'hh\_mm\_ss} format. The "inline" parameter indicates if the file must be served as an inline file (which means that it is directly displayed in the web browser instead of being downloaded through a pop-up).\\

The "Set-Cookie" HTTP header added at the end of the \emph{Result} object is used by the "jquery.fileDownload" library in order to receive a confirmation when the file has been successfully downloaded (in order to remove it from the server).\\

An example of generated file can be found in the \emph{/tmp/EXAMPLE.gt} file.

\subsection{Importing Generated Files}
GeoTwit also allows the user to import a previously generated file, in order to review the results (in a static way; the streaming does not resume). Here again, both the client and server sides are explained, in order to avoid confusion.\\

The user has the possibility to import a generated file by clicking a button located at the beginning of the Dynamic Mode tab's content. In summary, he has to click the button in order send the file to the "uploadAndParseFile" action of the \textbf{SearchController}, which will validate and parse the file, then send back the parsed data to the client as a json format. You can find more details about this process in the paragraphs below.
\bigskip

\subsubsection{Client Side}
The upload process begins as soon as the user chooses a file to upload. Firstly, in order to be able to send the file to the action with an Ajax request (for getting json data back from the server at the end of the process) and to be able to display a progress-bar about the upload, the basic plugin of the \emph{blueimp}'s "jQuery-File-Upload" library\footnote{https://github.com/blueimp/jQuery-File-Upload}-\footnote{https://github.com/blueimp/jQuery-File-Upload/wiki/Basic-plugin for the basic plugin} is used.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.9\textwidth]{figures/jquery-file-upload-progress-bar.jpg}}
\vspace{-5pt}
\caption{progress-bar managed by the jQuery-File-Upload library during an upload process}
\end{center}
\end{figure}
\newpage

In order to initialize the automatic Ajax upload, the "fileupload" method is called on the "file" HTML input. The given parameters indicate that the client expects the server to answer with json data and also provide the refresh function of the progress-bar.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.95\textwidth]{figures/ajax-upload-process-initialization.jpg}}
\vspace{-5pt}
\caption{initialization of the automatic Ajax upload process on the "file" input}
\end{center}
\end{figure}

As mentioned in the code's comment, the URL of the route to which the client sends the file is determined by the HTML input file's "data-url" attribute, which contains the route of the "uploadAndParseFile" action.\\

The "progressall" function episodically calculates the current progression value of the progress-bar by the current loaded data (this number being received from the server), then refreshes the HTML components.\\

The "done" function is called when the upload process is complete: it displays an error if the json data received from the server contains one, and in normal cases, calls the "loadFileResultsComponents(...)" function of the \emph{search.js} file, which loads the components (map, charts, HTML components) as well as the received data (display of the metadata, the tweets and the results).
\bigskip

\subsubsection{Server Side}
On the server side, the "uploadAndParseFile" action first validates the file, which must exist and be a text file, and not be empty. If a validation error occurs, the following json object is returned:
\begin{itemize}
	\item \textbf{error}: \emph{true};
	\item \textbf{reason}: the reason key of the error, which is either "fileEmpty", "wrongFormat" or "missingFile".
\end{itemize}
\newpage

If the file is valid, the action calls the "validateAndParseFile(...)" method of the same controller, which validates the contents of the file. This method returns a json object containing either an error if there was one or the parsed data of the file if everything went smoothly:
\begin{itemize}
	\item The file's content must first have the three "METADATA", "TWEETS" and "RESULTS" sections.
	\item The "\textbf{METADATA}" section must have at least the "FIRST\_SUBJECT" (\emph{String}), "LANGUAGE" (\emph{String}) and "COORDINATES" (array of arrays of \emph{Double}) values; the "SECOND\_SUBJECT" (\emph{String}) value is optional. The order of the values does not matter, as long as all of them (except the optional one) are present in this section.
	\item The "\textbf{TWEETS}" section can be empty (if there was no received tweet during the analysis) and is directly followed by the "RESULTS" section in this case; if it contains tweets, these have to be properly formatted (see the contained data in the "\nameref{generationOfStreamingFile}" chapter above): the date must be as the \emph{YYYY-MM-DD'T'hh:mm:ss} format, the longitude and latitude must be \emph{Double} types, and the user's name and tweet content must be bounded by quotation marks. There must be strictly one tweet per line.
	\item The "\textbf{RESULTS}" section must contain the "ELAPSED\_TIME" (\emph{String}) value, and the data linked to the charts ("GTRT", "GRT", "ATRT", "ART", "AGVW" (arrays of two arrays of \emph{Double}, which can be empty => "\underline{[[],[]]}"), and GPRT (simple array of \emph{Double}, which can also be empty => "\underline{[]}")). The order does not matter here, as long as each of these values is contained in this section.
	\item There should not exist any unnecessary spaces, tabulations or line breaks.
\end{itemize}

While parsing the file for validating it, the algorithm also simultaneously collects all the data in order to gain time; if a validation error occurs, this data is dropped. The algorithm is divided into three parts:
\begin{itemize}
	\item Validation and collection of the "METADATA" section, and retrieval of the other section's start lines.
	\item Validation and collection of the tweets if the metadata were valid and if the file contains the two other sections.
	\item Validation and collection of the results if the two other parts were valid, the return of the json results.
\end{itemize}

In order to validate and collect the values, the algorithm uses regular expressions with parentheses (in order to group data and get them as variables) and match cases to validate them. For example, if the algorithm wants to get the language value of the metadata, it will use the following regular expression, which will match a line beginning with "LANGUAGE:" and followed with a value containing at least one character:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.5\textwidth]{figures/language-regex.jpg}}
\vspace{-5pt}
\caption{Regex used to match the language string in the imported file}
\end{center}
\end{figure}
\newpage

The parentheses contained within the Regex (regular expression) is used to group the value in order to get it as a variable. The algorithm then tries to match the parsed file's lines with this regular expression, by collecting the group as a "l" variable.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.5\textwidth]{figures/language-regex-matching.jpg}}
\vspace{-5pt}
\caption{matching of the regex used for the language}
\end{center}
\end{figure}

The validation of the other metadata and results applies the same principle. Tweet validation is a little more complicated: the Regex validates and collects 6 different variables separated by ';'.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/tweet-regex.jpg}}
\vspace{-20pt}
\caption{Regex used to validate and get data of tweets}
\end{center}
\end{figure}

These variables must be in the following order and must be formatted as follows:
\begin{itemize}
	\item The tweet's identifier, which must start either by "first" or "second" (the "?>" characters indicates to Scala that we do not want to group the character contained in bounded parentheses as a variable), followed by "-subject\#", then by a number. For example: \underline{first-subject\#2}.
	\item The date, which must contain a string formatted as: \emph{XXXX-XX-XXTXX:XX:XX}, where 'X' is a digit and where 'T' is the \emph{'T'} character. For example: \underline{2016-07-22T15:30:14}.
	\item The longitude, which must be a double. For example: \underline{-87.9997769}.
	\item The latitude, which also must be a double. For example: \underline{47.8106521}.
	\item The user's name, which must contain at least one character and must be bounded by quotation marks (note that only the bounded content will be taken into the variable). Quotes are used to avoid bugs with a string that would contain a colon. For example: \underline{"Speedway Jobs"}.
	\item The content, which must follow the same rules as the user's name. For example: \underline{"Hello everybody!"}.
\end{itemize}
\newpage

Once every value of the file is valid and collected, the method sends back a json object, containing the following attributes:
\begin{itemize}
	\item \textbf{error}: \emph{false} (indicates that there was no error);
	\item \textbf{firstSubject}, \textbf{secondSubject} (can be an empty string), \textbf{language}, \textbf{coordinates};
	\item \textbf{tweets} => an array containing each tweet's values: \textbf{subjectIdentifier} (either "first" or "second"), \textbf{dateAndTime}, \textbf{longitude}, \textbf{latitude}, \textbf{user} and \textbf{content};
	\item \textbf{results} => an object containing the results: \textbf{elapsedTime}, \textbf{gtrt}, \textbf{grt}, \textbf{gprt}, \textbf{atrt}, \textbf{art} and \textbf{agvw}.
\end{itemize}

If there was an error, the method returns a json object containing:
\begin{itemize}
	\item \textbf{error}: \emph{true};
	\item \textbf{reason}: the key reason of the error, which is "fileNotValid".
\end{itemize}

This object is forwarded to the client through the "uploadAndParseFile" action. If everything was valid, the client displays data on the screen.
\newpage

\subsection{Retrieving of Static tweets}
\subsubsection{Issues due to Limitations}
Before explaining the concerned algorithms, more details have to be laid out about the function of Twitter's REST API when a large amount of data is available for the current user's request. Indeed, Twitter's server cannot send the whole amount of data at one time from a certain number of tweets, because of the problems that this action can cause (mainly slowness). In order to avoid these kinds of problems, Twitter sends the results by "pages", each page containing a certain amount (up to 100 - configurable with Twitter4J) of tweets. If there are still too many results, the API also limits the number of pages, which complicates the process: indeed, the developer has to get the lowest ID of the received tweets set, in order to make a new request that collects a new tweet set from this ID. Here is an example with a maximum of \underline{three} tweets per page, and a maximum of \underline{two} pages per results' set:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.8\textwidth]{figures/rest-api-limitations.png}}
\vspace{-5pt}
\caption{example of the limitations of the Twitter's REST API}
\end{center}
\end{figure}

In this example, the first request collects the tweets from \#12 to \#7 (from the highest to the lowest IDs, because Twitter logically firstly sends the most recent tweets), by generating two pages. The developer has to manually ask for the next page every time all the tweets of a page are collected, and has to execute a new sub-request with the new page. Once the first request does not have pages anymore, the developer has to get the lowest ID of the retrieved tweets (here: \#7) in order to make a new request to collect the tweets from \#6, and so on.\\

This system raises a huge issue: like it was said before in the analysis part of this document, Twitter indeed limits the number of requests that a user can make in a 15 minutes' period. Since every page's sub-request is considered as a request, and since the user may have to do various requests to get all results, the limit can quickly be reached. For this reason, the number of results are limited in GeoTwit.\\

If this case occurs and the obtained results are limited, the user has to be more specific about his subject(s) and date range. Twitter's REST API unfortunately only supports dates and is incompatible with specific timing, so one cannot fine-tune further than days.

\subsubsection{Algorithms}
When the user clicks on the search button of the static mode's page, the client makes an Ajax request to the server in order to retrieve the list of static tweets (see the "\nameref{retrievalOftweets}" chapter in the client's static part below). The server uses the Twitter's REST API to collect tweets in the "staticResults" action of the \textbf{SearchController}; it first gets and validates the sent parameters, then creates the queries.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.7\textwidth]{figures/rest-api-query-creation.jpg}}
\vspace{-5pt}
\caption{creation of a query for asking the Twitter's REST API with Twitter4J}
\end{center}
\end{figure}

The "setCount" method is used to indicate the maximum number of tweets per page (which can be up to 100 maximum). "setSince" and "setUntil" methods take a \emph{YYYY-MM-DD} date as a string value and are used to filter the tweets respectively by their start and end dates.\\

The action then calls the "getStatictweets(...)" method for each subject, which gets the tweets related to the given parameters and builds a json array object with them. Specifically, this method applies the algorithm seen above on pages and results' sets in order to avoid limitations as much as possible. Firstly, the "maximumNumberOfRequests" parameters indicates to the algorithm the maximum number of requests it can make before risking reaching Twitter's upper limit. After a few tests, it turns out that the maximum number of requests is close to 175 per account over a duration of 15 minutes (this value may be greater, but it provides a lower bound). The following example was generated by putting "println" after each request in the code:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.4\textwidth]{figures/number-static-requests-before-error.jpg}}
\vspace{-5pt}
\caption{example of the number of requests executed before getting an error in a period of 15 minutes}
\end{center}
\end{figure}
\newpage

In order to keep within the margin of error, this number is truncated to \textbf{170} in GeoTwit (approximately 17'000 tweets), which means that if the user enters two different subjects, each search must at the most make \textbf{85} requests (approximately 8'500 tweets per subject). This calculation is managed by the "staticResults" action: the maximum number is contained in the constant variable "MAXIMUM\_NUMBER\_OF\_REQUESTS"; when the action calls the "getStatictweets(...)" method, it indicates the number of requests to be made by the number of subjects that the user selected and by this constant value.\\

Due to these limitations and issues, the static mode is thus less developed than the streaming mode in GeoTwit. Also for the same reasons, \textbf{\underline{the "Accelerated Streaming View" tools that}} \textbf{\underline{were planned in the specifications were aborted}}; as a compromise, the file import functionality was developed in the dynamic mode.
\bigskip

\section{Implementation Details - Client Side}
\subsection{Both Streaming and Static Modes}
\subsubsection{Structure of the client's JavaScript file}
Since all the JavaScript code is contained in the \emph{/public/javascripts/search.js} file, there cannot be a decent UML schema. For this reason, here is the list of functions contained in this file (remember that JavaScript is a non-typed language), ordered by their order of appearance:
\begin{itemize}
	\item \emph{secondsToHhMmSs(seconds)}
	\item \emph{cloneObject(obj)}\cite{elliotbonneville2011}
	\item \emph{erasePolygons(map)}
	\item \emph{drawPolygons(map, coordinates, readFileMode = false)}
	\item \emph{invertCoordinates(coordinates)}
	\item \emph{isPointInSelectedArea(latitude, longitude, polygonsCoordinates)}\cite{RandolphFranklin2016}
	\item \emph{loadCountriesList()}
	\item \emph{loadLanguagesList()}
	\item \emph{loadSearchMap(mapId, hasRectangle, hasCircle)}
	\item \emph{loadStaticMap()}
	\item \emph{loadResultsMap(mode)}
	\item \emph{loadStreamingResultsCharts(hasSecondStream, refresh = true)}
	\item \emph{calculateAndDisplaySpeedValues()}
	\item \emph{loadStreamingResultsComponents()}
	\item \emph{loadFileResultsComponents(data)}
	\item \emph{loadStaticResultsComponents()}
	\item \emph{selectCountryOnMap(countryName, map)}
	\item \emph{initWebSocket()}
	\item \emph{stopStreaming(sendSocket, displayExportPopup = true)}
	\item \emph{deleteFile()}
	\item \emph{addtweetOnPage(mode, subjectNumber, latitude, longitude, date, user, content)}
	\item \emph{getStatictweets()}
	\item \emph{validateDynamicFields()}
	\item \emph{validateStaticFields()}
	\item \emph{getAndFormatKeywords(keywordsSetNumber, mode)}
	\item \emph{\$(document).ready(function()}: occurs when the page successfully loaded all DOM elements.
\end{itemize}

Most of these functions have been explained in detail in the following chapters.
\bigskip

\subsubsection{Used JavaScript Libraries}
\label{usedJavascriptLibraries}
Several JavaScript external libraries have been used in GeoTwit, so as to avoid reinventing the proverbial wheel. About half of them are NPM\footnote{https://www.npmjs.com/} packages located in the \emph{/node\_modules} directory, while the other half are external JavaScript libraries found on the web and located in the \emph{/public/javascripts} directory.
\begin{enumerate}
	\item \textbf{NPM packages}:
	\begin{itemize}
		\item \textbf{Chart.js}\footnote{http://www.chartjs.org/}: allows the developer to draw simple and responsive HTML5 charts.
		\item \textbf{Leaflet}\footnote{http://leafletjs.com/}: allows the developer to build interactive maps coupled with OpenStreetMap.
		\item \textbf{Leaflet.markercluster}\footnote{https://github.com/Leaflet/Leaflet.markercluster}: this plug-in for Leaflet is used to automatically group markers by clusters on the maps.
		\item \textbf{Leaflet.draw}\footnote{https://github.com/Leaflet/Leaflet.draw}: a vector drawing and editing plug-in for Leaflet, used to draw rectangles and circles on the maps (coupled with the next library).
		\item \textbf{Leaflet.draw.drag}\footnote{https://github.com/w8r/Leaflet.draw.drag}: this library is used coupled with the \emph{Leaflet.draw} one, in order to allow the user to draw and drag polygons (rectangles and circles) on the maps with a special toolbar.
		\item \textbf{Moment.js}\footnote{http://momentjs.com/}: allows the developer to parse, validate, manipulate, and display dates in JavaScript.
	\end{itemize}
	\newpage

	Since NPM packages must normally be used with a Node.js server, the NPM package "browserify"\footnote{http://browserify.org/} is used. As a reminder, this package allows the developer to use NPM's packages within a simple JavaScript's client application.  As soon as the \emph{/public/javascripts/search.js} file is updated, the developer has to type the following command into a console to generate the \emph{/public/javascripts/search-bundle.js} file.
	\begin{lstlisting}
		browserify search.js -o search-bundle.js
	\end{lstlisting}
	This \emph{search-bundle.js} file contains a working version of the \emph{search.js} file, with all the code of the NPM's packages included in it. This file is thus the one that is read by the web browser

	\item \textbf{External JavaScript libraries}:
	\begin{itemize}
		\item \textbf{jQuery}\footnote{https://jquery.com/}: provides an easier way to manipulate DOM elements than simple JavaScript code.
		\item \textbf{Bootstrap}\footnote{http://getbootstrap.com/}: provides HTML components, CSS classes and JavaScript code to easily develop responsive interfaces.
		\item \textbf{shapefile-js}\footnote{https://github.com/calvinmetcalf/shapefile-js}: the \emph{calvinmetcalf}'s library provides a tool to read Shapefile-formatted\footnote{https://en.wikipedia.org/wiki/Shapefile} files and parse them in json.
		\item \textbf{pickadate.js}\footnote{http://amsul.ca/pickadate.js/}: provides JavaScript date and time pickers.
		\item \textbf{jquery.fileDownload}\footnote{https://github.com/johnculviner/jquery.fileDownload}:  the \emph{johnculviner}'s library provides an easy way to download file with Ajax requests.
		\item \textbf{jQuery-File-Upload}\footnote{https://github.com/blueimp/jQuery-File-Upload}: the \emph{blueimp}'s library is used to easily upload files with Ajax and to show a progress-bar.
	\end{itemize}
\end{enumerate}

Several of these libraries were already encountered previously in this document; the rest of them will be explained in the following chapters.
\bigskip

\subsubsection{Dates Format}
Since a lot of dates are used in GeoTwit (in the static mode's search page, and in the results pages of both modes) and since they have to be regularly formatted, validated and converted to string objects, the "Moment.js" library is used to simplify this process. This well-known library allows the developer to easily work with dates of both \emph{Date} and \emph{String} types.
\newpage

\subsubsection{Building of the Twitter's Query String}
\label{buildingTwitterQueryString}
In order to filter tweets with the Twitter API, the developer has to give a query string. It is possible to add \underline{AND} and \underline{OR} filter's operator by respectively separating the words with spaces (" ") and commas (",") for the streaming mode or with the " OR " and " AND " strings for the static mode, since parentheses are not supported by the API. Note that the \underline{AND} keyword takes priority over the \underline{OR} one, since comma-separated terms are considered as phrases by the API; here are some examples with in streaming mode (the method is the same for static mode):
\begin{itemize}
	\item $job\to$ interpreted as "job"
	\item $job engineer\to$ job \underline{AND} engineer
	\item $job,engineer\to$ job \underline{OR} engineer
	\item $job engineer,nursing\to$ (job \underline{AND} engineer) \underline{OR} nursing
	\item $job,engineer nursing\to$ job \underline{OR} (engineer \underline{AND} nursing)
	\item $job,engineer nursing,potato\to$ job \underline{OR} (engineer \underline{AND} nursing) \underline{OR} potato
\end{itemize}

Once the user filled the keywords sets (with spaces between words) and pressed the search button, the "getAndFormatKeywords(...)" function is called. Three cases are possible from there:
\begin{enumerate}
	\item The user only filled the AND part ("All these keywords...") $\to$ there is nothing to do here since the words are already separated by spaces.
	\item The user only filled the OR part ("One of these keywords...") $\to$ the algorithm replaces the spaces with commas.
	\item The user filled the AND and the OR parts $\to$ since the \underline{AND} takes priority over the \underline{OR} and since we want to search tweets that contain all the AND words and one of more of the OR words (for example: "(dog \underline{AND} cat) \underline{AND} (eat \underline{OR} drink)", the algorithm has to manually build the string, by taking each of the OR words and by adding them one by one at the end of the AND words, separating all the possibilities with commas. For example, if the user entered "\textbf{dog cat}" in the AND field and "\textbf{eat drink}" in the OR field, the resulting query will be "\textbf{dog cat eat,dog cat drink}", which is interpreted as "(dog \underline{AND} cat \underline{AND} eat) \underline{OR} (dog \underline{AND} cat \underline{AND} drink)".
\end{enumerate}

According to the documentation, the text of the tweet and some entity fields are considered for matches. Specifically, the text attribute of the tweet, the "expanded\_url" and "display\_url" values for links and media, the text for hashtags, and the "screen\_name" value for user mentions are checked for matches. Each phrase of the query (terms between commas, like "\textbf{dog cat eat}" and "\textbf{dog cat drink}") must contain up to 60 characters maximum, otherwise the API will generate an error.
In all cases, the function also locally saves a human-comprehensive query (with "\underline{AND}" and "\underline{OR}" keywords, and parentheses), in order to display it on the results page. These queries are generated by parsing the query sent to the server.
\newpage

\subsubsection{Selection of the Language of the tweets}
In GeoTwit, the user can choose a language with a drop-down menu, in order to filter the received tweets by their language.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/languages-selection.jpg}}
\vspace{-20pt}
\caption{selection of a tweets' language in GeoTwit}
\end{center}
\end{figure}

Since this languages' drop-down menu is located both in the dynamic and static modes, and for scalability issues, the language list is stored in json format, located in the \emph{/public/data/languages.json} file. It is then loaded in the "loadLanguagesList()" function of the JavaScript file. The json file is composed of an array of languages, in which each element has "value" (the BCP 47 language identifier ("en", "fr", etc.) sent to the Twitter's API) and "text" (the displayed text) attributes.
\newpage

The identification of a tweet's language\cite{MitjaTrampus2015} seems to be performed by keywords analysis. However, since tweets' content can be very short, it may be difficult to properly determine the true language, so errors can be possible:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/langid_examples_table.png}}
\vspace{-20pt}
\caption{example of difficulties that the language-identification algorithm can encounter\cite{MitjaTrampus2015}}
\end{center}
\end{figure}

The algorithm also seems to learn from human-identified languages with sets of tweets.
\bigskip

\subsubsection{Selection of the Area(s) in the Search Page}
In both the streaming and the static modes of the application, the user has to choose a location and the area with-in which the tweets will be searched. In the streaming mode, he has the choice to either select a default area in a given list of countries or draw a rectangle area by dragging-and-dropping the cursor on the map. In the static mode, the user can only select a circle area by drag-and-dropping the cursor, since the Twitter's Rest API asks for a circle (it would be tricky and computationally expensive to calculate a bounding circle of the complex polygon representing a country).
\newpage

\emph{Selection of a default area in the list of countries}\\
As a reminder (see the prototype application's "\nameref{leafletCountriesBorders}" chapter above), the country the user selects in the drop-down menu contains a value corresponding to the English name of the country. As soon as a country is selected, the JavaScript code of the "selectCountryOnMap(...)" function reads and converts to Json the "TM\_WORLD\_BORDERS-0.3" Shapefile\footnote{https://en.wikipedia.org/wiki/Shapefile}, which contains all the borders of almost each country of the worlds (note that this file is also read to add the country list in the drop-down menu at the loading of the page, in the "loadCountriesList()" function). Then, the algorithm seeks for the selected country in the Json-converted data, and uses the related coordinates to draw the polygon and the map. The client also sends the server the coordinates of a rectangle bounding the selected country's territories (since a country can have many, like French territories of France and Corsica) as said before (see the "readyToStream" web socket's type). The coordinates of this bounding rectangle are calculated in the "selectCountryOnMap(...)" function: if the country only has one territory (like Switzerland), the algorithm just selects the maximum and minimum latitude and longitude points of the polygon representing the country; if it has many territories, the algorithm selects the maximum and minimum coordinates of all different territories. The server will then search for tweets located inside this rectangle, and send them to the client.\\

\emph{Manual selection of a rectangle/circle}\\
The code of this solution is contained in the \emph{/public/javascripts/search.js} file, specifically in the "loadSearchMap()" function. In order to be able to draw a rectangle/circle on the map with drag-and-drop, the \emph{w8r}'s "Leaflet.draw.drag" library\footnote{https://github.com/w8r/Leaflet.draw.drag} is used. This library further uses the Leaflet's "Leaflet.draw" library\footnote{https://github.com/Leaflet/Leaflet.draw}, which adds support for drawing and editing polygons on Leaflet maps. Note that these libraries are NPM packages, and are thus used with the "browserify" package.\\

In the code, right after the map's initialization, three objects are set: the first one will contain the drawn items, and the two last ones will contain draw control's objects:
\begin{enumerate}
	\item \emph{drawnItems}: this object is added to the map as a feature group and will contain the drawn rectangle/circle.
	\item \emph{drawnControlFull[mapId]}, where "mapId" is the ID (either "streaming" or "static") of the current results' map: this object represents a toolbar that allows the user to draw a rectangle/circle on the map and which is located at the top-left corner of the map element. It is the default drawing that appears when there is no polygon drawn, and disappears as soon as the user draws one.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.82\textwidth]{figures/rectangle-drawing.jpg}}
	\vspace{-5pt}
	\caption{drawing of a rectangle on the dynamic map}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	\newpage

	\item \emph{drawnControlEditOnly[mapId]}: this object represents another toolbar that allows the user to update the drawn rectangle/circle. It only appears when the polygon has been drawn.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.86\textwidth]{figures/rectangle-edition.jpg}}
	\vspace{-5pt}
	\caption{addition of the rectangle on the dynamic map}
	\end{center}
	\end{figure}
	\vspace{-20pt}
\end{enumerate}

As soon as the user draws or edits a rectangle or a circle, the new coordinates are saved in the "boundingRectangleLatLngs" object (the latitude and longitude coordinates of each corner) or in the "circleLatLngRad" (the latitude and longitude of the circle's center' point and the radius in kilometers) object, which will later be sent to the server.
\bigskip

\subsubsection{Grouping of the tweets on the Map}
Since there can be a lot of displayed tweets on the map, they have to be grouped by clusters. Fortunately, the Leaflet library offers a plug-in for its library, called "Leaflet.markercluster"\footnote{https://github.com/Leaflet/Leaflet.markercluster}, that allows the developer to easily group markers on a map by creating cluster groups.\\

In GeoTwit, only one cluster group is created (at the initialization of the results maps, in the "loadResultsMap" function):
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/cluster-group-creation.jpg}}
\vspace{-20pt}
\caption{creation of the cluster group in GeoTwit}
\end{center}
\end{figure}

The developer then just has to add a marker in this cluster group in order to have the library automatically manage the grouping.\\

Once a new tweet is received, the client adds it to the cluster group if it belongs to the area; if it is not in the area, the client just ignores it.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/tweet-adding-in-cluster-group.jpg}}
\vspace{-20pt}
\caption{adding of a tweet in the cluster group, in GeoTwit}
\end{center}
\end{figure}

\subsection{Streaming Mode}
\subsubsection{Interface of the Streaming Process' Results}
During the streaming process, the user can access plenty of information, including:
\begin{itemize}
	\item The elapsed time since the beginning of the streaming process, updated in the "speedInterval" timer object, which is itself initialized in the "loadStreamingResultsComponents()" function called when the client received the "successfulInit" web socket type.
	\item The human-comprehensive queries (for example "dog \underline{AND} (eat \underline{OR} drink)"), generated in the "getAndFormatKeywords(...)" function (see the "\nameref{buildingTwitterQueryString}" chapter) when the client sends the user parameters to the server.
	\item The map displaying all the received tweets located in the selected area (see the "\nameref{receptionOftweetsFromTheServer}" chapter below).
	\item Various charts allowing the user to better analyze the current streaming process and the data related to it (see the "\nameref{charts}" chapter below).
	\item The content of the last 100 received tweets; this number can be changed with the "MAX\_DISPLAYED\_TWEETS" constant variable.
	\item The total number of received tweets with geolocation tags, which correspond to the "nbReceivedtweets" variable.
	\item The average speed of received tweets per minute, which is calculated each second by the "speedInterval" timer object (just like the elapsed time information). The calculation is very simple and just divides the number of received tweets by the elapsed time, then multiplies it by 60 (in order to obtain minutes) and rounds it to two decimals.
The tricky part of the algorithm is that it also calculates the color value of the speed text (which is green when the current speed is good, and red when it is not - the quality of the speed being determined by the "GOOD\_SPEED" constant value). In fact, the algorithm calculates the hue (between "$0\to$ red" and "$120\to$ green") and the lightness values (between 50\% for the bad speed and 25\% for the good speed) of an HSL color\footnote{http://www.w3schools.com/colors/colors\_hsl.asp}, by the calculated speed per minute.
	\begin{figure}[H]
	\centering
	\begin{minipage}{0.45\textwidth}
	\centering
	\fbox{\includegraphics[width=\textwidth]{figures/speed-color-example-1.jpg}}
	\vspace{-20pt}
	\caption{first example of speed's color}
	\end{minipage}\hfill
	\begin{minipage}{0.45\textwidth}
	\centering
	\fbox{\includegraphics[width=\textwidth]{figures/speed-color-example-2.jpg}}
	\vspace{-20pt}
	\caption{second example of color}
	\end{minipage}
	\end{figure}
\end{itemize}

The "speedInterval" timer is stopped when the user clicks on the "Stop Streaming" button.
\newpage

If the user filled in two keywords sets, the data related to the first one (human-comprehensive query, markers on the map, curves of the charts, number of received tweets, average speed's label, content of the tweets, etc.) will appear in \color{blue}blue\color{black}, while the ones related to the second one will appear in \color{Orange}orange\color{black}. Be careful not to confuse the subject's colors with the cluster groups' colors, which appear in green when there are not a lot of tweets in the group, and in red when there are a lot of tweets.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/streaming-interface-example.jpg}}
\vspace{-20pt}
\caption{example of an interface of the streaming process' results}
\end{center}
\end{figure}
\vspace{-10pt}

Charts are properly explained in the "\nameref{charts}" chapter below.
\newpage

\subsubsection{Reception of tweets from the Server}
\label{receptionOftweetsFromTheServer}
Once the streaming process begins, the client first draws the saved selected area (either the rectangle or the complex polygon bounding the area or country). When the server receives a tweet matching the filter, the language and the bounding rectangle, it is sent to the client. Once the client receives the "newtweet" web socket message, it first saves the total number of received tweets (both with or without geolocation) contained in the web socket's content, then acts according to the selected location's type (determined by a Boolean variable set when the user selected the area):
\begin{itemize}
	\item If the user manually selects a rectangular area on the map, it means that the received tweet is certainly in the selected area, so the client just displays it on the map and in the received tweets panel (with the "addtweetOnPage(...)" function). It also increments the local number of received tweets, in order to properly calculate the average reception speed.
	
	\item It is more complicated if the user selects a default area/country from the drop-down menu: since the server searches for tweets within the bounding rectangle, some of the received tweets may not be in the selected area.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.86\textwidth]{figures/tweet-located-in-the-bounding-box-but-not-in-the-polygon.jpg}}
	\vspace{-5pt}
	\caption{example of a tweet located in the bounding box of the selected country's territories, but not in the territories themselves}
	\end{center}
	\end{figure}
	\vspace{-20pt}
	\newpage

	In this case, the client must check that the received tweet belongs to the complex polygon. In order to properly accomplish this task, the Point Inclusion in Polygon algorithm (\emph{PNPOLY})\cite{RandolphFranklin2016} was used with an updated version of the \emph{substack}'s "point-in-polygon" library found on GitHub\footnote{https://github.com/substack/point-in-polygon/blob/master/index.js}, which allows the developer to check if a given point is inside a given polygon. Since the \emph{PNPOLY} algorithm is well explained on the given web site, it will be briefly laid out here. The important points here are: according to the documentation, the algorithm runs a semi-infinite ray horizontally (increasing x, fixed y) out from the test point, and counts how many edges it crosses. At each crossing, the ray switches between inside and outside. This is called the Jordan curve theorem. The case of the ray going through a vertex is handled correctly via a careful selection of inequalities. The ray is tested against each edge to determine if the point is in the half-plane to the left of the extended edge and if the point's Y coordinate is within the edge's Y-range.
\end{itemize}
\newpage

\subsubsection{Charts}
\label{charts}
In order to draw charts in GeoTwit, the Chart.js library is used\footnote{http://www.chartjs.org/}. There are several other well-known libraries (like D3.js\footnote{https://d3js.org/}), but they are either too complicated for the simple use-case of this application or not as user-friendly and well documented as Chart.js.
In GeoTwit, there are six possible charts, ordered in two categories: tweets having geolocation tags, and all tweets (both with or without geolocation tags). The following screenshots were taken during an example test made with the \color{blue}"job" (blue) \color{black}and \color{Orange}"euro \underline{OR} 2016" (orange) \color{black}keywords sets and located in the U.S.A.
\begin{enumerate}
	\item Charts related to \textbf{tweets \underline{having} geolocation tags}:
	\begin{enumerate}
		\item \textbf{Total of received tweets, by time}: this "line" chart displays the total of received tweets that have a geolocation tag, first by seconds, and then by minutes once 60 seconds have elapsed.
		\begin{figure}[H]
		\vspace{-5pt}
		\begin{center}
		\fbox{\includegraphics[width=0.37\textwidth]{figures/gtrt-example.jpg}}
		\vspace{-5pt}
		\caption{example of the "GTRT" chart}
		\end{center}
		\end{figure}
		\vspace{-10pt}

		\item \textbf{Reception of tweets, by time}: this "line" chart displays the number of received tweets during time periods (first by seconds, and then by minutes once 60 seconds have elapsed).
		\begin{figure}[H]
		\vspace{-5pt}
		\begin{center}
		\fbox{\includegraphics[width=0.37\textwidth]{figures/grt-example.jpg}}
		\vspace{-5pt}
		\caption{example of the "GRT" chart}
		\end{center}
		\end{figure}
		\vspace{-10pt}

		\item \textbf{Parts of the received tweets by subject}: this "doughnut" chart compares the part of received tweets for each subject, and is only visible when the user has filled both the keywords sets.
		\begin{figure}[H]
		\vspace{-5pt}
		\begin{center}
		\fbox{\includegraphics[width=0.45\textwidth]{figures/gprt-example.jpg}}
		\vspace{-5pt}
		\caption{example of the "GPRT" chart}
		\end{center}
		\end{figure}
		\vspace{-10pt}
	\end{enumerate}

	\item Charts related to all kinds of tweets (\textbf{with and \underline{without} geolocation tags}):
	 \begin{enumerate}
	 	\item \textbf{Total of received tweets, by time}: this "line" chart displays the total of all received tweets (that do or do not have a geolocation tag), first by seconds, and then by minutes once 60 seconds have elapsed.
	 	\begin{figure}[H]
		\vspace{-5pt}
		\begin{center}
		\fbox{\includegraphics[width=0.45\textwidth]{figures/atrt-example.jpg}}
		\vspace{-5pt}
		\caption{example of the "ATRT" chart}
		\end{center}
		\end{figure}
		\vspace{-10pt}
		\newpage

		\item \textbf{Reception of tweets, by time}: this "line" chart displays the number of all received tweets (that do or do not have a geolocation tag) during time periods (first by seconds, and then by minutes once 60 seconds have elapsed).
		\begin{figure}[H]
		\vspace{-5pt}
		\begin{center}
		\fbox{\includegraphics[width=0.45\textwidth]{figures/art-example.jpg}}
		\vspace{-5pt}
		\caption{example of the "ART" chart}
		\end{center}
		\end{figure}
		\vspace{-10pt}

		\item \textbf{tweets with geolocation vs. tweets without}: this "bar" chart displays the percentage of tweets that have geolocation tags versus the percentage of those which do not have one.
		\begin{figure}[H]
		\vspace{-5pt}
		\begin{center}
		\fbox{\includegraphics[width=0.45\textwidth]{figures/agvw-example.jpg}}
		\vspace{-5pt}
		\caption{example of the "AGVW" chart}
		\end{center}
		\end{figure}
		\vspace{-10pt}
	 \end{enumerate}
\end{enumerate}

All these charts are responsive and react when the user moves the mouse over their elements.
\newpage

Since the library is well documented, everything will not be explained here; the initialization of the charts and the timings of refreshment are located in the "loadStreamingResultsCharts(...)" function. Firstly, the contexts of the charts (the HTML elements) are initialized, then the empty datasets of the charts and the charts themselves. The interesting parts of the algorithm concern the refreshment of the charts; there are two timers refreshing the charts each second (and then each minute for the first one):
\begin{enumerate}
	\item \textbf{lineChartsUpdateInterval}: this timer refreshes all the "line" charts (so 4 on the 6 charts) each second, and then each minute when 60 seconds have elapsed. All the data is evaluated with the "nbReceivedtweets" and "lastNbReceivedtweets" objects, which respectively contain the total number of received tweets (for the tweets with geolocation and for all the tweets) and the last count of received tweets that were calculated during the last tick of the timer.
	\item \textbf{doughnutBarChartsUpdateInterval}: this timer refreshes the "doughnut" and "bar" charts each second. All the data is evaluated using the "nbReceivedtweets" object. The "bar" chart contains two datasets in case of multiple keywords sets (one for each keywords set), in order to display the two subjects' colors. Each data set contains 4 values (2 for the current subject and two null values for the other subject), in order to only display 4 bars (and not 8: \emph{2 subjects x 4 bars}).
\end{enumerate}

Once the charts are initialized, the timers are started. They are stopped when the user clicks on the "Stop Streaming" button.
\bigskip

\subsection{Static Mode}
\subsubsection{Selection of the Dates}
In the static mode, the user has to select the search's start and end dates in the \emph{YYYY-MM-DD} format. Since the Twitter's REST API only allows the developers to get tweets from a maximum of 9 days past, the user cannot select dates incompatible with this condition.\\

In order to facilitate the selection of dates, the \emph{amsul}'s date picker "pickadate.js" library\footnote{https://github.com/amsul/pickadate.js} is used, with the "classic" theme. This library is located in the \emph{public/javascripts} folder in the "picker.min.js", "picker.date.min.js" and the "legacy.js" (this last one is used for older versions of Internet Explorer) files.\\

The date picker is loaded on dates field as soon as the Search page loads successfully.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.7\textwidth]{figures/date-picker-loading.jpg}}
\vspace{-5pt}
\caption{loading of the date picker on the dates fields}
\end{center}
\end{figure}
\vspace{-10pt}
\newpage

\subsubsection{Retrieval of tweets with the Server}
\label{retrievalOftweets}
Once the user has filled all the fields, the client sends an Ajax request to the server, in order to retrieve the results. This Ajax request is located in the "getStatictweets()"method of the \emph{search.js} file. The following parameters are sent in a json format:
\begin{itemize}
	\item \textbf{firstKeywords}: the first keyword set filled by the user, formatted with the "getAndFormatKeywords(...)" function (see the "\nameref{buildingTwitterQueryString}" chapter above).
	\item \textbf{secondKeywords}: the second formatted keyword set filled by the user (it can be empty).
	\item \textbf{fromDate}, \textbf{toDate}: the \emph{YYYY-MM-DD} formatted dates from and to where the tweets will be searched.
	\item \textbf{locationLat}, \textbf{locationLon}, \textbf{locationRad}: the latitude and longitude coordinates of the circle's middle point, and the radius of the circle in kilometers.
	\item \textbf{language}: the BCP 47 language identifier of the search's language.
\end{itemize}

Once the server responds to the Ajax request with json, the response message can contain either an error (in this case, the client just displays an error message) or the list of tweets. Since there can be a lot of tweets, the data transfer can take a while, so the search button changes its display:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/search-button.jpg}}
\vspace{-20pt}
\caption{display of the search button after the user has clicked on it
in order to search for tweets}
\end{center}
\end{figure}
\vspace{-10pt}

Once the tweets' lists have been received, the client first calculates the numbers of tweets, then concatenates the lists (since there is one list per keywords set), sorts the new-created list by the dates and times of the tweets, then adds each tweet one by one on the results map (only for the tweets having a geolocation tag) with the "addtweetOnPage(...)" function, and adds each tweet in the tweets panel.\\

The tweets are also grouped with cluster groups, like in the streaming mode's results (see the "\nameref{receptionOftweetsFromTheServer}" chapter below).
\bigskip

\subsubsection{Interface of the Static Mode's Results}
Once the static search is done, the user can access plenty of information, including:
\begin{itemize}
	\item The human-comprehensive queries (for example "dog \underline{AND} (eat \underline{OR} drink)"), generated in the "getAndFormatKeywords(...)" function (see the "\nameref{buildingTwitterQueryString}" chapter) when the client sends the user parameters to the server.
	\item The map that displays all the tweets located in the selected area. Note that some tweets can be displayed outside the circle, due to the search algorithm of the Twitter's REST API, which is a little tricky.
	\item The content of all the tweets.
	\item The total number of received tweets, both with and without geolocation tags.
\end{itemize}
\newpage

If the user filled the two keywords sets, the data related to the first one will appear in \color{blue}blue\color{black}, while the ones related to the second one will appear in \color{Orange}orange\color{black}.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/static-mode-interface.jpg}}
\vspace{-20pt}
\caption{example of the interface of the static mode's results}
\end{center}
\end{figure}
\vspace{-10pt}

\chapter{Tests}
\label{tests}
This chapter contains all the documentation related to the application's tests.\\
A Twitter account was specially created for this project and has been used for most of these tests:
\begin{itemize}
	\item \textbf{Username}: GeoTwitHeig
	\item \textbf{Password}: @heigVdRocks42
\end{itemize}
Feel free to use these credentials for your own tests.
\bigskip

\section{Functional Tests}
All the development process and thus the functional tests were firstly processed with the version 47.0 of Firefox on Linux Mint. Though, other web browsers were tested in the "\nameref{webBrowsersCompatibilities}" chapter above.

\begin{center}
\def\arraystretch{1.5}
\begin{longtable}{|l|p{10cm}|p{4.4cm}|}\hline
	\textbf{\#} & \textbf{Description} & \textbf{Results}\\\hline \endhead
	\multicolumn{3}{|c|}{\textbf{Miscellaneous}}\\\hline
	1 & If the user tries to access a non-existent page, a "Not Found" page is displayed. & \color{ForestGreen}OK\color{black}\\\hline
	2 & The user can access the about and help pages, regardless of whether he is connected or not. & \color{ForestGreen}OK\color{black}\\\hline
	3 & The web site's GUI is stable. & \color{ForestGreen}OK\color{black}\\\hline
	4 & The web site's GUI is responsive. & \color{Orange}Only for desktop applications\color{black}\\\hline	
	5 & Maps are responsive and allow the user to zoom in and out and to move around the world by drag-and-dropping the content. & \color{ForestGreen}OK\color{black}\\\hline
	
	\multicolumn{3}{|c|}{\textbf{Home page and connection process}}\\\hline
	6 & The user can properly access the application's home page if he is disconnected. & \color{ForestGreen}OK\color{black}\\\hline
	7 & If the user is not connected and try to access pages different than the home page, he is redirected on the home page with an error message. & \color{ForestGreen}OK\color{black}\\\hline
	8 & The user can access the Twitter's authentication page by clicking either on the "Get Started" or on the "Connect" button. & \color{ForestGreen}OK\color{black}\\\hline
	9 & If the user denied the authentication process or if an error occurred, he is properly redirected on the home page with an explicit error message. & \color{ForestGreen}OK\color{black}\\\hline
	10 & Once the user successfully connected, he is able to navigate through the web site and to access the tools. He is however redirected to the search page if he tries to access the home page. & \color{ForestGreen}OK\color{black}\\\hline
	11 & If the connected user clicks on the "Disconnect" button, he is properly disconnected, the session is entirely destroyed and he cannot access search pages anymore. & \color{ForestGreen}OK\color{black}\\\hline
	12 & The session lasts 7 days and the user is not disconnected before this time limit, \underline{except if a modification was done in} \underline{the server's code}. & \color{ForestGreen}OK\color{black}\\\hline
	
	\multicolumn{3}{|c|}{\textbf{Main Search Page}}\\\hline		
	13 & The user is redirected on the main search page if he clicks on the GeoTwit's logo located on the header bar & \color{ForestGreen}OK\color{black}\\\hline
	14 & The user can navigate through both the dynamic and static modes with the tab system. & \color{ForestGreen}OK\color{black}\\\hline
	
	\multicolumn{3}{|c|}{\textbf{Dynamic Mode - Files Importation}}\\\hline		15 & By clicking on the importation button, a pop-up window that allows the user to select a file appears. & \color{ForestGreen}OK\color{black}\\\hline
	16 & If the user selects a wrong type of file (i.e. an image), an error is displayed. & \color{ForestGreen}OK\color{black}\\\hline
	17 & If the selected file is an empty text file, an error is displayed. & \color{ForestGreen}OK\color{black}\\\hline
	18 & If the selected file's content is not a valid GeoTwit content, an error is displayed. & \color{ForestGreen}OK\color{black}\\\hline
	19 & During the importation of the file, a progress-bar is displayed and is automatically refreshed. & \color{ForestGreen}OK\color{black}\\\hline
	20 & Once the importation was successfully done, the user is redirected on the result page and can access the information. & \color{ForestGreen}OK\color{black}\\\hline
	21 & The information displayed are the same as the ones contained in the file and thus the imported ones. & \color{ForestGreen}OK\color{black}\\\hline
	\newpage
	
	\multicolumn{3}{|c|}{\textbf{Dynamic Mode - Fields Validation}}\\\hline	
	22 & The user cannot write more than 60 characters in the keywords sets' fields. & \color{ForestGreen}OK\color{black}\\\hline
	22 & If the user tries to bypass the characters' limitation by editing the text field's properties, the server still displays an error message. & \color{ForestGreen}OK\color{black}\\\hline
	23 & If the user did not set at least one keywords set when clicking on the "Start Streaming" button, an error message appears. & \color{ForestGreen}OK\color{black}\\\hline
	24 & The countries included in the default area list are listed in alphabetical order. & \color{ForestGreen}OK\color{black}\\\hline
	25 & If the user did not either choose a default area in the countries' list or manually draw a rectangle on the map, an error message is displayed. & \color{ForestGreen}OK\color{black}\\\hline
	26 & The user is able to draw only one rectangle on the map and is also able to update it. & \color{ForestGreen}OK\color{black}\\\hline
	27 & If a rectangle is drawn or it the user selected a country, all existing polygons are removed from the map. & \color{ForestGreen}OK\color{black}\\\hline
	28 & The languages list contains all the possible languages in alphabetic order. & \color{ForestGreen}OK\color{black}\\\hline
	29 & Once all fields are valid, the user is properly redirected to the results page. & \color{ForestGreen}OK\color{black}\\\hline
	30 & When the user clicks on the button and everything is valid, the server properly removes the current user's existing file-to-export in its side, if there was one. & \color{ForestGreen}OK\color{black}\\\hline
	
	\multicolumn{3}{|c|}{\textbf{Dynamic Mode - Results Page}}\\\hline	
	31 & The results page's components are properly loaded once the user accessed it. & \color{ForestGreen}OK\color{black}\\\hline
	32 & If the user only filled one subject, only the blue components are displayed and enabled. & \color{ForestGreen}OK\color{black}\\\hline
	33 & If the user filled two subjects, the orange components also appears and are enabled. & \color{ForestGreen}OK\color{black}\\\hline
	34 & The displayed information correspond to the ones the user filled. & \color{ForestGreen}OK\color{black}\\\hline
	35 & The elapsed time's timer is enabled and working. & \color{ForestGreen}OK\color{black}\\\hline
	36 & The user can switch between the map and the charts by using the tab system of the page. & \color{ForestGreen}OK\color{black}\\\hline 
	37 & Tweets are received in the selected location only. & \color{ForestGreen}OK\color{black}\\\hline
	38 & The average speed is correctly refreshed and changes its color according to the speed (green if good, red if bad). & \color{ForestGreen}OK\color{black}\\\hline
	39 & Once a tweet is received, it is correctly displayed on the map and in the tweets panel below with the right color, and the number of received tweet is updated. & \color{ForestGreen}OK\color{black}\\\hline
	40 & Only the last 100 received tweets are displayed in the tweets panel below & \color{ForestGreen}OK\color{black}\\\hline
	41 & Group clusters are properly working and disappear once the user zoomed at a zoom level of 8 or more. & \color{ForestGreen}OK\color{black}\\\hline
	42 & There are 5 available charts if the user entered only one subject, and 6 otherwise. & \color{ForestGreen}OK\color{black}\\\hline
	43 & The charts are correctly refreshed each second, then each minute once 60 seconds passed. In addition, they correctly differentiate the subjects if the user filled two subjects. & \color{ForestGreen}OK\color{black}\\\hline
	44 & The charts are decent, correct, and responsive. & \color{ForestGreen}OK\color{black}\\\hline
	45 & The streaming process is stopped once the user clicked on the "Stop Streaming" button, a new "Make a new search" button appears, and an exportation pop-up is instantaneously displayed. & \color{ForestGreen}OK\color{black}\\\hline
	46 & If the user either clicked the "Cancel" button of the pop-up window or closed it, the file is removed from the server side. & \color{ForestGreen}OK\color{black}\\\hline
	47 & If the user clicked the "Ok" button, he is able to save the file-to-export wherever he wants on his computer, then the file is removed from the server side. He is then able to export this file from the main search page. & \color{ForestGreen}OK\color{black}\\\hline
	48 & Once the user clicked on the "Make a new search" button, he is redirected on the main search page. & \color{ForestGreen}OK\color{black}\\\hline
	 
	\multicolumn{3}{|c|}{\textbf{Static Mode - Fields Validation}}\\\hline
	49 & The keywords sets fields act like the dynamic mode's ones. & \color{ForestGreen}OK\color{black}\\\hline
	50 & The user can only select dates between the current date and the date it was 9 days ago. & \color{ForestGreen}OK\color{black}\\\hline
	51 & If the dates are either not set or not valid, an error message is displayed. & \color{ForestGreen}OK\color{black}\\\hline
	52 & An error message is displayed if the user did not select a circle area on the map. & \color{ForestGreen}OK\color{black}\\\hline
	53 & The user is able to draw only one circle on the map and is able to update it. & \color{ForestGreen}OK\color{black}\\\hline
	54 & The languages list contains all the possible languages in alphabetic order. & \color{ForestGreen}OK\color{black}\\\hline
	55 & Once all the fields are valid, the user can search for results by clicking on the "View Results!" button. & \color{ForestGreen}OK\color{black}\\\hline
	56 & The button changes of behavior during the search process. & \color{ForestGreen}OK\color{black}\\\hline
	57 & If there was no result, a pop-up message is displayed. & \color{ForestGreen}OK\color{black}\\\hline
	58 & If the user exceeded the limit of requests made, a pop-up message is displayed. & \color{ForestGreen}OK\color{black}\\\hline
	59 & If there was no error, the user can access the results page. & \color{ForestGreen}OK\color{black}\\\hline
	 
	\multicolumn{3}{|c|}{\textbf{Static Mode - Results Page}}\\\hline
	60 & The results page act like the dynamic mode's result page, except that the user cannot access charts and everything is fixed. & \color{ForestGreen}OK\color{black}\\\hline
	61 & The tweet panel has no number limit and displays the tweets both with and without geolocation tags. & \color{ForestGreen}OK\color{black}\\\hline
	62 & Tweets are received in the selected location only. & \color{Orange}Certain tweets are displayed outside the circle, but this issue is apparently due to the Twitter's algorithm, which is tricky.\color{black}\\\hline
	
	\multicolumn{3}{|c|}{\textbf{Security}}\\\hline
	63 & If the user tries to bypass the fields' limitations (like the number of characters) by editing the text field's properties in order to bypass the client's error message, the server still displays an error message. &	\color{ForestGreen}OK\color{black}\\\hline
	64 & All Ajax functions are properly protected on the server side (the user must be connected, the parameters are validated, etc.). & \color{ForestGreen}OK\color{black}\\\hline
	65 & The web socket server checks that the user is connected before starting the streaming process. &  \color{ForestGreen}OK\color{black}\\\hline
	66 & The web socket server checks the identity of the client before accepting the discussion. &  \color{red}No\color{black}\\\hline
	 
\end{longtable}
\end{center}
\vspace{-20pt}

All tests thus successfully passed, except the \#4 one (which was thus not specified in the specifications) and the \#62 one (due to the Twitter's algorithms), which partially passed. Since the security of the application was not a main point, basic security issues are covered, but more complex ones like the \#66 test was not developed.
\newpage

\subsection{Web Browsers - Compatibilities}
\label{webBrowsersCompatibilities}
As mentioned above, all the development process and thus the functional tests were processed with the version 47.0 of Firefox on Linux Mint.\\

Since the end users can have different operating systems and/or web browsers, the functional tests' list was also tested with a variety of combinations. Here are the obtained results:
\begin{center}
\def\arraystretch{1.5}
\begin{longtable}{|l|l|l|p{7cm}|}\hline
	\textbf{\#} & \textbf{Operating System} & \textbf{Web Browser} & \textbf{Results}\\\hline \endhead
	1 & Linux Mint & Mozilla Firefox 47 & \color{ForestGreen}OK\color{black}\\\hline
	2 & Linux Mint & Chromium 51 & \color{ForestGreen}Some components' borders are missing.\color{black}\\\hline
	3 & Windows7 & Mozilla Firefox 47 & \color{ForestGreen}OK\color{black}\\\hline
	4 & Windows7 & Google Chrome 51 & \color{ForestGreen}Some components' borders are missing.\color{black}\\\hline
	5 & Windows7 & Internet Explorer 11 & \color{ForestGreen}Certain components (like charts) are either too big or too small; some little bugs of interface.\color{black}\\\hline	
\end{longtable}
\end{center}
\vspace{-20pt}

In summary, all other recent web browsers seem to be able to run the application (sometimes with small bugs of interface, but nothing to worry about).\\

In order to make Internet Explorer work with the application, the default values of the parameters of the JavaScript functions had to be removed, since this web browser does not properly follow the JavaScript's ES6/ES2015 norms. Also, in order to make the file importation work with this browser, the "uploadAndParse" action of the \textbf{SearchController} now also validates the "text/plain" file's content type (used by IE) in addition to the basic "application/octet-stream" used by all other browsers.
\newpage

\section{Load Tests}
Two load tests' types were analyzed:
\begin{enumerate}
	\item \textbf{Tests about the number of received tweets}: these tests are used to analyze the application's reaction when there is a huge amount of data to process, display and/or import.\\
	
	The \underline{static mode} is easy to test here: indeed, since the number of requests is limited to 170 (about 17'000 tweets) by the application's server, we just have to find a subject that contains a number of tweets big enough to reach this limit. The "job" keyword in the U.S.A. always gave good results over the other tests, and was thus used here. Firstly, here are the obtained results:
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/load-tests-static-mode.jpg}}
	\vspace{-5pt}
	\caption{results of the load tests for the static mode with the "job" keyword}
	\end{center}
	\end{figure}
	\vspace{-10pt}
	
	As expected, about 17'000 tweets were received, including about 5'000 ones with geolocation tags. The search process was timed and took 1 minute and 54 seconds. The map was very fluid since the tweets were grouped by cluster groups; however, when the map was zoomed enough to disable the cluster groups (zoom level of 8 or more) in areas with high tweets densities (like the West Coast), the loading took about 10 seconds, but the map was still fluid after the process. When the groups were enabled again by zooming out, the loading took about 2 seconds. This load test was then a success.\\	
	\newpage
	
	Concerning the \underline{dynamic mode}, the streaming process had to run for hours in order to properly accumulate a charge wide enough. In order to get a huge amount of tweets, GeoTwit listened for the "job" and "beach" keywords sets in the U.S.A. (since there is a huge amount of tweets posted with geolocation tags for these subjects in general) for 13 hours and 30 minutes between 21:00 on 26.07.2016 and 10:30 on 27.07.2016. Firstly, here are the obtained results:
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/load-test-streaming-map.png}}
	\vspace{-5pt}
	\caption{results' map of the streaming mode's load tests with the "job" and "beach" keywords}
	\end{center}
	\end{figure}
	\vspace{-10pt}
	
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/load-test-streaming-charts.png}}
	\vspace{-5pt}
	\caption{charts of the streaming mode's load tests with the "job" and "beach" keywords}
	\end{center}
	\end{figure}
	\vspace{-10pt}	
	
	The first comment we can do is about the geolocated reception, whose form is pretty logarithmic. This can be explained by the fact that it was the evening (18:00) in the U.S.A. approximately since the 180th minute, so there were less people/bots posting for jobs.\\
	
	Concerning the load tests' observations, the page was still very fluid: it was possible to navigate the map and to interact with the charts without having any slowness, thanks to the cluster groups. However, zooming the map at a zoom level of 8 or more (which thus removes the cluster groups) on the area with the highest density took some time (about 40 seconds) to load the markers; once they were properly loaded, some little slowness began to appear, but the map was still well usable, though.
	
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/load-tests-streaming-.map-zoomed.jpg}}
	\vspace{-5pt}
	\caption{map of the load tests' results zoomed at a zoom level of 8}
	\end{center}
	\end{figure}
	\vspace{-10pt}
	
	At the end of the process, the backup file was easily exported. The importation process took about 1 minute and 42 seconds (during which the overloaded web browser displayed some "script not answering" pop-up messages), but was successful in the end. Once loaded, the reaction of the file results' map was exactly the same as the previous map. This load test was also successful, then.
	\bigskip
	
	\item \textbf{Tests about the number of applications that simultaneously search for tweets}: since several Twitter accounts are required for properly performing these tests and since it would be annoying to create these account and connect with them, only two accounts were used, but with 4 different browsers in two different machines. The processes perfectly worked and there was not even a single lag.\\
	
	Of course, the number of applications was not wide enough to hope to get interesting reactions, but it though confirms that the server can easily manage several clients at the same time.
\end{enumerate}
\newpage

\section{Subjects Analyzed with the Streaming Mode}
\subsection{The "job" keyword}
When listening to the "job" keyword in the territories of the U.S.A., there was \underline{always} a great tweets reception (about 130 geolocated tweets per minute) at any time of the day (in the U.S.A., so during the European afternoon and evening). Since the majority of these tweets always contained the same texts ("Want to work at...?", "We're hiring!", "Can you recommend anyone for this job?", "This job might be a great fit for you", "If you're looking for a job", etc.) regardless to their locations (which were almost everywhere in the territories), it was concluded that they were posted by bots per time interval. A way to guarantee the veracity of this assumption is to look at the charts generated for this keyword, which are linear in time during the day:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.5\textwidth]{figures/job-chart.jpg}}
\vspace{-5pt}
\caption{total number of received tweets by time, for the "job" subject in the U.S.A.}
\end{center}
\end{figure}
\vspace{-10pt}

This situation was very interesting throughout the application's implementation process, because it assured a way to always have data to test.
\newpage

\subsection{Subjects by Countries}
This section contains analyses separated by countries. The majority of them is related to the subjects already analyzed by the prototype application in the table of the "\nameref{interestingTwitterSubjects}" chapter, but during hours instead of minutes.

\subsubsection{United Kingdom}
Both the "bbc" and the "london" subjects were first analyzed here at the same time, on 18.08.2016 between 15:00 and 17:00.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.9\textwidth]{figures/analysis-uk-bbc-london-map.jpg}}
\vspace{-5pt}
\caption{analysis' map of the "bbc" and "london" keywords in UK}
\end{center}
\end{figure}
\vspace{-10pt}

\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.7\textwidth]{figures/analysis-uk-bbc-london-charts.jpg}}
\vspace{-5pt}
\caption{analysis' charts of the "bbc" and "london" keywords in UK}
\end{center}
\end{figure}
\vspace{-10pt}

The "bbc" subject gave bad results for the geolocated tweets, with an average speed reception of only 0.06 tweet per minute (99.97\% of the tweets did not have geolocation data). The "london" subject was better, although not amazing either (here, 97.84\% did not have geolocation tag). It is interesting to note than the reception of all kinds of tweets of the first subject was in a "stair" shape, while the second subject's one was pretty linear, but in the end their values were very close. Thus, British people seem to be more apt to post tweets related to London than tweets related to the BBC.
\bigskip
\newpage

July 19 was the \underline{hottest day of the Summer} in UK. The idea of analyzing this subject came from the fact that people tend to like complaining about the weather on the social networks, in general. Indeed, by analyzing the "hot" or "sun" keywords, about 370 geolocated tweets and about 160'000 tweets of all kinds were posted between 13:00 and 16:00.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.9\textwidth]{figures/analysis-uk-hot-sun-map.jpg}}
\vspace{-5pt}
\caption{analysis' map of the "hot" or "sun" keywords in UK during the hottest day}
\end{center}
\end{figure}
\vspace{-10pt}

\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.8\textwidth]{figures/analysis-uk-hot-sun-charts.jpg}}
\vspace{-5pt}
\caption{analysis' charts of the "hot" or "sun" keywords in UK during the hottest day}
\end{center}
\end{figure}
\vspace{-10pt}

About 99.77\% of the posted tweets did unfortunately not have geolocation data.


\subsubsection{Italy}
Both the "pizza" and the "roma" subjects were analyzed at the same time on 18.08.2016 between 17:00 and 19:00. Like in the prototype application's analysis, not a single tweet was received during the streaming process, so it was decided that no other test will be performed for this country.
\bigskip

\subsubsection{France}
The word "paris" was tested on 19.07.2016 between 17:00 and 19:00.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/analysis-france-paris-map.png}}
\vspace{-5pt}
\caption{analysis' map of the "paris" keyword in France}
\end{center}
\end{figure}
\vspace{-10pt}
\newpage

Here are the charts' results:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/analysis-france-paris-charts.png}}
\vspace{-5pt}
\caption{analysis' charts of the "paris" keyword in France}
\end{center}
\end{figure}
\vspace{-10pt}

As expected, a lot of tweets were received from Paris, since it was the main concerned subject.\\

France was also analyzed among other countries during the final match of the Euro2016 event (see the "\nameref{euro2016final}" chapter below).
\newpage

\subsubsection{Switzerland}
Switzerland is a small country, but does that mean that people do not post from there on Twitter? A first analysis was made on July 20 and 21 from 21:00 to 04:00, notably during the live show of Iron Maiden at the Paleo Festival of Nyon (with the "paleo" subject), and gave just one result at 22:23:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.9\textwidth]{figures/analysis-switzerland-paleo-map.png}}
\vspace{-5pt}
\caption{analysis' map of the "paleo" keyword in Switzerland}
\end{center}
\end{figure}
\vspace{-10pt}

The second Swiss analysis was made on July 21 between 14:00 and 16:00, and was about the main Swiss industries/companies ("coop \underline{OR} glencore \underline{OR} logitech \underline{OR} migros \underline{OR} nestle \underline{OR} novartis \underline{OR} swatch"); it unfortunately gave no result. Out of curiosity, these keywords were tested the next-day for about two hours in the afternoon for the rest of the world, in order to see the impact of these companies:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.9\textwidth]{figures/analysis-world-swiss-companies-map.png}}
\vspace{-5pt}
\caption{analysis' map of the keywords related to Swiss companies in the world}
\end{center}
\end{figure}
\vspace{-10pt}
Within these 15 received tweets, eight of them were related to "migros" (apparently, a famous Turkish mall has this name and might be related to the Swiss company), five of them were false positives with the "coop" keyword, one was related to "nestle", and the last one was related to "novartis". The charts were simply linear.\\

In conclusion, like we also saw with the prototype application, Switzerland is not a really good country to analyze.
\bigskip

\subsubsection{U.S.A.}
U.S.A. is the country whose people post the most tweets. In addition of the "job" keyword, other subjects like "beach" or the name of the election candidates gives good results. The following test was made on July 27 from 16:30 to 19:00 and compares the "clinton" and "trump" subjects, which are the names of the two current election's favorites:\\
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/analysis-usa-clinton-trump-map.png}}
\vspace{-20pt}
\caption{analysis' map of the "clinton" and "trump" keywords in the U.S.A.}
\end{center}
\end{figure}
\vspace{-10pt}

A lot of tweet were posted from the East Coast, maybe because the time right there was more favorable during the analysis.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/analysis-usa-clinton-trump-charts.png}}
\vspace{-20pt}
\caption{analysis' charts of the "clinton" and "trump" keywords in the U.S.A.}
\end{center}
\end{figure}

For reasons, there was no post during the last analyzed hour. People speak more about the "trump" subject, but a lot of them complain because they do not want him as a president, so this information does not mean that Donald Trump will necessarily be elected. Be aware about the data interpretations!
\newpage

\subsubsection{Argentina}
In order to analyze a country that belongs to an area different than Europe and North America, the Argentina was chosen. The "argentina" keyword was analyzed between 13:30 and 14:45 (between 08:30 and 10:30 in the targeted country) on July 27.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/analysis-argentina-argentina-map.png}}
\vspace{-20pt}
\caption{analysis' map of the "argentina" keywords in Argentina}
\end{center}
\end{figure}
\vspace{-10pt}
Surprisingly, more tweets than expected were collected; the charts were simply linear.
\bigskip

\subsection{Final Match of the Euro2016 Event}
\label{euro2016final}
In addition of the analysis of the Euro2016's final match with the prototype application, the GeoTwit application was developed enough to realize the first and complete analysis of an event with it. Two instances of the application were launched with two different Twitter's accounts (since there cannot be more than two simultaneous streaming processes by account), in order to make two different analyses:
\begin{enumerate}
	\item The first analysis concerned the keyword "euro2016" within all Europe.
	\item The second analysis was composed of two keywords sets (and thus streaming processes), which analyzed the tweets related to the "euro2016" keyword and either to France or Portugal within all Europe; these keywords sets were respectively "euro2016 \underline{AND} (france \underline{OR} fra \underline{OR} francais \underline{OR} french \underline{OR} bleus)" and "euro2016 \underline{AND} (portugal \underline{OR} por \underline{OR} portuguese)".
\end{enumerate}

Unfortunately, since it was the first large-scale test ever, the first analyzed crashed 30 minutes after the beginning and was unable to properly restart. Fortunately, the prototype application also analyzed the "euro2016" keyword, but only in France though. Because of this issue, only the second analysis' results are presented below.\\

The duration of the analysis was \textbf{02:33:46}, and the application received \textbf{392 tweets} during it, including \textbf{228} (about \textbf{1.48 tweets / minute}) related to France and \textbf{164} related to Portugal (about \textbf{1.07 tweets / minute}). As expected, there were more tweets related to France, since this country hosted the event. Here are the maps results (note that since the application was still in development, the interface of the following screenshots may be different as the final interface of the application):
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/analysis-euro2016-final-europe-map.jpg}}
\vspace{-20pt}
\caption{tweets map of the FR-PT final match}
\end{center}
\end{figure}
\vspace{-10pt}
\newpage

The whole Europe posted tweets, and a lot of them were posted from France and Portugal, as expected. By zooming on the map, more details appear:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/analysis-euro2016-final-europe-first-zoomed-map.jpg}}
\vspace{-20pt}
\caption{first zoom on the tweets map of the FR-PT final match}
\end{center}
\end{figure}
\vspace{-10pt}
\newpage

About 215 on 392 received tweets were posted in the Paris area, where the match occurred.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/analysis-euro2016-final-europe-second-zoomed-map.jpg}}
\vspace{-20pt}
\caption{second zoom on the tweets map of the FR-PT final match}
\end{center}
\end{figure}
\vspace{-10pt}
\newpage

Without France, the distribution of tweets through Europe was pretty uniform. There still were less posted tweets from the East part of Europe.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/analysis-euro2016-final-europe-france-map.jpg}}
\vspace{-20pt}
\caption{France map of the FR-PT final match's results}
\end{center}
\end{figure}
\vspace{-10pt}
\newpage

There was no received tweet from Switzerland. In the opposite, people sent a lot of tweets from Paris. In addition of the maps results, the application also gave useful graphs about the reception rate of tweets. The red annotations were manually added with Gimp:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/analysis-euro2016-final-europe-charts.jpg}}
\vspace{-20pt}
\caption{France map of the FR-PT final match's results}
\end{center}
\end{figure}
\vspace{-10pt}

In the first part of graphs (related to tweets that have geolocation tags), both France-related and Portugal-related tweets' receptions took a logarithmic form. At the beginning of the match, the application received a lot of France-related tweets, and then the receptions smoothed.
Concerning the graphs related to all kinds of tweets (with and without geolocation tags), the curves were more linear than before. The reception of the Portugal's tweets had two peaks of data, both when the favorite player of Portugal (Cristiano Ronaldo) goes down injured and when the country scored at the end of the match, giving a final reception of 228'165 tweets related to Portugal, against 164'265 ones related to France. Finally, it should be noted than only a tiny part of posted tweets had geolocation tags (0.14\% for France-related tweets and 0.07\% for Portugal-related tweets), which seems to follow the average percentages received in the prototype application.
\newpage

\subsection{Pokémon GO}
One of the events not to be missed of the semester was without a doubt the Pokémon GO game's release. It indeed created a massive flow of data through the social medias. As a reminder and according to Wikipedia, this application is a free-to-play location-based augmented reality mobile game, which allows players to capture, battle and train virtual Pokémons who appear on device screens as though in the real world. It makes use of GPS and the camera of compatible devices.\\

This test was a worldwide one and was done between 13:00 and 16:00 (Swiss hours) on the 19.07.2016.
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/analysis-pokemon-go-world-map.jpg}}
\vspace{-20pt}
\caption{world map of the Pokémon GO analysis}
\end{center}
\end{figure}
\vspace{-10pt}
\newpage

A large amount of tweets was posted from the South of Asia and in Europe. It is interesting to note that almost all parts of the world posted about this topic.

\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/analysis-pokemon-go-charts.jpg}}
\vspace{-20pt}
\caption{charts of the Pokémon GO analysis}
\end{center}
\end{figure}
\vspace{-10pt}

The charts are pretty linear in time.
\newpage

\subsection{Where are people going on vacation?}
If you ever wondered where are people going on vacation during Summer, these results can go some way toward answering your question:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/analysis-vacation-world-map.png}}
\vspace{-20pt}
\caption{analysis' map of the "vacation" keyword in the whole world}
\end{center}
\end{figure}
\vspace{-10pt}
This analysis was performed on July 17 from 16:00 to 17:30. People tend to spend their vacation mostly in the U.S.A., in central Europe and in the South of Asia.
\bigskip

\subsection{Ranking of the Countries by their Post Rates}
Finally, here is a little ranking made with the tested countries about their post rates of \underline{geolocated} tweets:
\begin{enumerate}
	\item U.S.A
	\item U.K.
	\item France
	\item Argentina
	\item Switzerland
	\item Italy
\end{enumerate}


\chapter{Conclusions}
\section{Achievements}
\subsection{Implementations}
All the initial specifications were properly achieved, and non-planned features have been developed as well. Here is a chronological list of the implementation's steps and achievements that went into the completion of this project:
\begin{enumerate}
	\item Use of the Twitter's APIs, including:
	\begin{itemize}
		\item Retrieval and filtering of tweets by one or more keyword(s).
		\item Retrieval of geographic information attached to a tweet.
		\item Analysis of the percentage of tweets having geographic information (necessary for further processing) and the search of keywords having relevant results, in order to determine the best keywords for use in order to test the different project features.
	\end{itemize}
	\item Use of a cartographic library in order to use geographic maps.
	\item Development of the tweet's acquisition and analysis parts for the dynamic mode first, \textit{then for the static one}.
	\item Development of the web interface of the application, allowing the user to enter subjects to compare, to choose the geographic areas on the map, to visualize results and to interact with the map.
	\item Development of a grouping algorithm that groups the tweets according to the map's zoom level.
	\item \textit{Development of charts that list relevant interesting information about the results.}
	\item \textit{Built-in possibility to export the queried data as a text file at the end of a streaming process and to import this file thereafter.}
	\item Testing and validation.
\end{enumerate}

Italic parts are the developed extra features that were not planned at the beginning of the semester, since some points were still fuzzy at that time.
\newpage

The work was regular and homogeneous over the whole semester, as you can see in the following GitHub's chart:
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=\textwidth]{figures/github-chart.png}}
\vspace{-20pt}
\caption{graph of the GitHub commits that were done during the semester}
\end{center}
\end{figure}
\vspace{-10pt}

There are more commits from June, because the work process jumped from half-time to full-time at this moment.
\bigskip

\subsection{Known Issues}
Even if a particular attention was paid during the development of this project, some issues can still persist. Here are the current known ones:
\begin{itemize}
	\item If a user simultaneously runs two streaming processes on the same computer, but in two different tabs or browsers, both of these processes will write the same file-to-export at the same time, which could give strange results.
	\item Minor bugs of display are present in the browsers different than Firefox.
\end{itemize}
\newpage

\section{Future Extensions}
Here is a non-exhaustive list of various extensions and improvements, which could be developed in a potential future developmental update:
\begin{itemize}
	\item Improvement of the GUI of the web application.
	\item Improvement of the code's structure (creation of multiple files for the JavaScript code, etc.).
	\item Implementation of a user management platform (Sign-up, Log-in) coupled with a database, in order to back up the different obtained results.
	\item Storage and display of the most successful keywords as well as the areas that have the best geographic results within the application.
	\item Retrieval and display of the trending topics on Twitter.
	\item The possibility to separate the results by their phrases (the words contained between each "\underline{OR}"), in order to have improved comparison tools.
	\item The possibility to hide/show the results of a subject, when there is several of them.
	\item The possibility to manually configure and enable/disable the cluster groups on the map.
	\item Addition of a progress-bar that indicates the current progression of the static search's process (since it could take times).
	\item Addition of filters for the search tools, like the possibility to filter tweets by their status (recent or popular) in the static tool.
	\item Find a way to be less limited by Twitter in the static search.
	\item Addition of social widgets (sharing of aggregated results, notes, etc.).	
	\item Addition of a gamification system (allowing users to accumulate points and trophies by the results rate of their search, implementation of user ranking, etc.).
	\item Improvement of the security of the application (especially for the web sockets' management).
\end{itemize}
\newpage

\section{Encountered Problems}
There are two types of encountered problems: the technical and the organizational ones.
\subsection{Technical Problems}
Here is the list of the main technical encountered problems:
\begin{itemize}
	\item Since the Play Framework is pretty new compared to other older technologies, the documentation of this framework is logically poorer. Due to this little problem, I spent many time on significant things in order to properly use them, for example with all the session management and the protection of the application, trying to prevent a non-connected user to access the web site's content.
	In order to solve this problem, I had to search the web a lot and use my knowledge in other programming languages.
	\item I spent many hours trying to integrate the Twitter4J library in the Play project. It was indeed different when I tried in the pure Java prototype application, where everything was simple. I searched for a while to find the right path to include in the project's build file, and when I finally succeeded, I spent many other hours trying to complete the Twitter's connection process, since it was not properly documented in the library and some things changed with Scala.
	\item The learning of the different libraries and technologies took some time, but nothing really significant.
	\item A lot of tiny (but time-consuming) things has to be implemented in order to make the application work, like the algorithm that checks if a point is inside a polygon, the one that selects the borders of a country's territories, the web sockets server, etc.
\end{itemize}
\bigskip

\subsection{Organizational Problems}
And here is the list of the organizational problems:
\begin{itemize}
	\item The main organizational problems were the time management and the personal organization: since there was way more work than I thought in the other courses of the semester, I never was able to work during the 16 planned hours per week. However, I made myself sure to assiduously work during the free hours. The time management was also a little bit chaotic during the two last weeks of the project.
	\item I spend time translating the first documentation's version from French to English (since it was not planned in the beginning), but it was not a huge problem in the end.
\end{itemize}
\newpage	
\section{Final Discussions}
The overall organization and execution of the project has been very smooth. Indeed, every planned task was successfully completed, in addition to a few new ones, which is a personal success. It has been a truly rewarding experience to work my way through this autonomous project over the duration of the semester, owing partly to the steep learning curve that it has provided (like a new language and data mining techniques). Despite not being perfect, I certainly feel satisfied at the end of this enriching process and proud of the end-results.\\

Regarding the future of the application, GeoTwit has several potentially interesting use-cases; for example, a real-time comparison of a population's voting behavior across different parts of a country (e.g. German, French and Italian parts of Switzerland) during national elections akin to exit polls, or an analysis of people's reactions to the passing or proposal of a new law, an analysis of the Twitter's data during a major event/festival, a comparison of certain related subjects (e.g. comparing geographical variation of perception of Java and Scala languages), or lighter hearted topics, for instance, geographic variations in use of the keyword "vacation".\\

A final word on the metaphysical relevance of this project: GeoTwit can philosophically be regarded as a data-mining tool that intersects human behavior and technology via the platform of social networks, allowing an end user to analyze and compare ethnological subjects in conjunction and with the aid of technological advancements.
\newpage

\listoffigures
\newpage

\begin{thebibliography}{9}
\bibitem{elliotbonneville2011}
  elliotbonneville,
  \emph{4 creative ways to clone objects in JavaScript},
  November 2011: http://heyjavascript.com/4-creative-ways-to-clone-objects/
\bibitem{RandolphFranklin2016}
  W. Randolph Franklin,
  \emph{PNPOLY - Point Inclusion in Polygon Test},
  February 2016: https://www.ecse.rpi.edu/Homepages/wrf/Research/Short\_Notes/pnpoly.html
\bibitem{MitjaTrampus2015}
  Mitja Trampus,
  \emph{Twitter - Evaluating language identification performance},
  November 2015: https://blog.twitter.com/2015/evaluating-language-identification-performance
\bibitem{playFramework2016}
  Play Framework Team,
  \emph{Play 2.5.x documentation},
  \\https://www.playframework.com/documentation/2.5.x/Home
\bibitem{twitter2016}
  Twitter Team,
  \emph{Documentation of Twitter API},
  https://dev.twitter.com/overview/documentation
\bibitem{twitter4J2016}
  Twitter4J Team,
  \emph{Twitter4J-4.0.4 JavaDoc},
  http://twitter4j.org/javadoc/index.html
\bibitem{leaflet2016}
  Leaflet Team,
  \emph{Leaflet 0.7 API Reference},
  http://leafletjs.com/reference.html
\end{thebibliography}

\appendix
\titleformat{\chapter}[display]   
{\normalfont\huge\bfseries}{\chaptertitlename\ \thechapter}{20pt}{\Huge}   
\titlespacing*{\chapter}{0pt}{-30pt}{40pt}

\chapter{Content of the Project's Package}
Here is the content of the project's files:
\begin{itemize}
	\item \textbf{\emph{app/GeoTwit}} $\to$ contains the GeoTwit application's code.
	\item \textbf{\emph{doc}} $\to$ contains the project's thesis.
	\item \textbf{\emph{prototypes}} $\to$ contains the four developed prototype applications:
		\begin{itemize}
			\item \emph{leaflet}
			\item \emph{leafletAndTwitter4J}
			\item \emph{leaflet\_countries\_borders}
			\item \emph{twitter4jDesktop}
		\end{itemize}
	\item \textbf{\emph{README.md}} $\to$ contains information about the content of the package and the instructions of installation.
\end{itemize}

\chapter{Instruction Manual}
\label{instruction}
\section{Installation}
In order to use the application, please first install Play Framework with activator on your computer\footnote{https://www.playframework.com/documentation/2.5.x/Installing}. Once done, go in the \emph{/app/GeoTwit} folder, type "\textbf{activator run}" in a console and open the \emph{http://localhost:9000} URL. The process will take a while the first time you access this page, since it has to download the libraries and compile the application.
\bigskip

\section{Software Usage}
Firstly, you can access the "About" and "Help" pages anywhere in the application with or without being connected by clicking on the related links.

\subsection{Home Page}
\begin{enumerate}
	\item Once you successfully reached the web site, you access the GeoTwit's home page. In this page, you can visualize the different features offer by the application, then log in with your Twitter account in order to be able to use them. Click on the \emph{Get Started} button located at the bottom of the page to connect.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/home-page.png}}
	\vspace{-5pt}
	\caption{home page}
	\end{center}
	\end{figure}
	\vspace{-10pt}
	\newpage

	\item By clicking on the connection button, you will have to authorize GeoTwit to use your Twitter account, in order to allow it to read public tweets with your account (none of your account's data are saved or used but the user's name, which is displayed later). Enter you credentials and then click on the "Sign In" button.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/geotwit-authorization-page.png}}
	\vspace{-5pt}
	\caption{Authorization page of GeoTwit}
	\end{center}
	\end{figure}
	\vspace{-10pt}
	You can use the special GeoTwit's Twitter account (\emph{GeoTwitHeig} - \emph{@heigVdRocks42}) for your tests, if you wish.\\
	
	If you click the "Cancel" button, or if something went wrong, you will be redirected on the home page with an explicit error message.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/access_denied.png}}
	\vspace{-5pt}
	\caption{explicit error message that appears when something went wrong during the connection process}
	\end{center}
	\end{figure}
	\vspace{-10pt}
\end{enumerate}

\subsection{Search Pages}
\begin{enumerate}
	\item Once connected, you can access the application's tools through the search pages. These pages contain a header element, a content element and a footer element:
	\begin{itemize}
		\item \textbf{Header element}: this element is located on the top-most part of the interface, and contains the GeoTwit logo as well as a message that indicates with which account you are still connected, and finally a disconnect button that disconnects you and redirects you on the home page once clicked.
		\begin{figure}[H]
		\vspace{-5pt}
		\begin{center}
		\includegraphics[width=0.88\textwidth, right]{figures/header.png}
		\vspace{-20pt}
		\caption{header element of the search pages}
		\end{center}
		\end{figure}
		\vspace{-10pt}
		
		If you click on the logo, you will be redirected on the main search page.
		
		\item \textbf{Content element}: this element is located between the header and the footer, and contains the content of the page you currently are in. You can find more information in the following paragraphs.
		\item \textbf{Footer element}: this element is located on the bottom-most part of the interface, and contains links to respectively access the \emph{Help} and \emph{About} pages.
		\begin{figure}[H]
		\vspace{-5pt}
		\begin{center}
		\includegraphics[width=0.88\textwidth, right]{figures/footer.png}
		\vspace{-20pt}
		\caption{footer element of the search pages}
		\end{center}
		\end{figure}
		\vspace{-10pt}
	\end{itemize}
	
	\item The main search page is the first page you reach once you successfully connected. It contains a tabs system that allows you to navigate through the \underline{dynamic} and \underline{static} modes.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.3\textwidth]{figures/tabs-dynamic-static.png}}
	\vspace{-5pt}
	\caption{tabs system used to navigate through the modes of the application}
	\end{center}
	\end{figure}
	\vspace{-10pt}
\end{enumerate}
\newpage

\subsection{Dynamic Mode}
The dynamic (also called "streaming") mode is the default and most finalized mode of the application. It allows you to listen to a Twitter's stream of data in order to receive and analyze tweets that match the entered filters in real-time.

\begin{enumerate}
	\item In the interface of this mode, you can firstly import a previously exported GeoTwit (".gt") text file by clicking on the importation's button.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/import.png}}
	\vspace{-5pt}
	\caption{button used to import a previously exported GeoTwit file}
	\end{center}
	\end{figure}
	\vspace{-10pt}
	If the file is either not in the right format (i.e. an image), is empty or is not valid, an error will be displayed.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/import-error.png}}
	\vspace{-5pt}
	\caption{error message displayed when an importation error occurs}
	\end{center}
	\end{figure}
	\vspace{-10pt}
	If everything went well, you will be redirected on a frozen version of the dynamic mode's results page that displays the results contained in the file (see the points below).
	
	\item If you want to start a new streaming process, you have to properly fill the form:
	\begin{itemize}
		\item \textbf{Keywords Sets}: you have to fill the first keywords set at least, in order to provide a query to filter tweets. For each of these keywords sets, you can fill both the "One of these keywords..." and/or the "All these keywords..." fields separately, which are respectively used to create \underline{OR} and \underline{AND} filters.
		\begin{figure}[H]
		\vspace{-5pt}
		\begin{center}
		\fbox{\includegraphics[width=0.75\textwidth]{figures/keywords-set.png}}
		\vspace{-5pt}
		\caption{example of value that can be contained in the first keywords set field}
		\end{center}
		\end{figure}
		\vspace{-10pt}
		\underline{AND} filters have priority over the \underline{OR} ones. In the figure above, the generated query filter can be interpreted as "(dog \underline{AND} cat) \underline{AND} (eat \underline{OR} drink)".\\
		
		If you fill the second optional keywords set, a second streaming process will be started simultaneously in order to compare the subjects of each subject between them. As mentioned, each field of the keywords sets must at most be 60 characters long.
		
		\item \textbf{Location}: when selecting a location, you have to choice to either choose a default area among a list of purposed countries or manually draw a rectangle on the map (but never both options at the same time).
		\begin{figure}[H]
		\vspace{-5pt}
		\begin{center}
		\fbox{\includegraphics[width=0.75\textwidth]{figures/default-area.png}}
		\vspace{-5pt}
		\caption{selection of the location with the default area option}
		\end{center}
		\end{figure}
		\vspace{-10pt}
		
		If you want to draw a rectangle, click on the button located on the top-left part of the map.
		\begin{figure}[H]
		\vspace{-5pt}
		\begin{center}
		\fbox{\includegraphics[width=0.3\textwidth]{figures/draw-rectangle-button.png}}
		\vspace{-5pt}
		\caption{button that allows the user to draw a rectangle on the map}
		\end{center}
		\end{figure}
		\vspace{-10pt}	
		\newpage
		
		Then drag-and-drop an area on the map to draw the rectangle.	
		
		\begin{figure}[H]
		\vspace{-5pt}
		\begin{center}
		\fbox{\includegraphics[width=0.75\textwidth]{figures/rectangle.png}}
		\vspace{-5pt}
		\caption{selection of the location by drawing a rectangle}
		\end{center}
		\end{figure}
		\vspace{-10pt}
		
		You can observe than the "Draw" button no longer exists once you drew the polygon, and was replaced by a "Edit" button, which allows you to edit the drawn rectangle.
		
		\item \textbf{Language}: you can either keep the default "Any Language" selected value or select a language among the list of purposed languages, in order to filter tweets by their writing's language.
		\begin{figure}[H]
		\vspace{-5pt}
		\begin{center}
		\fbox{\includegraphics[width=0.75\textwidth]{figures/language.png}}
		\vspace{-5pt}
		\caption{selection of the language used to filter received tweets}
		\end{center}
		\end{figure}
		\vspace{-10pt}
	\end{itemize}
	\newpage
	
	\item When you click on the "Start Streaming" button, an error message will appear if one of the fields is not valid. In the opposite, if everything is valid, you will be redirected on the results page, which displays the real-time results.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.8\textwidth]{figures/streaming-process.png}}
	\vspace{-5pt}
	\caption{annotated example of a streaming process for the tweets that contain the "job" and/or "beach" keywords, are written in English, and was posted from a part of the U.S.A.}
	\end{center}
	\end{figure}
	\vspace{-10pt}
		
	Several information are available on this page, among which:
	\begin{enumerate}
		\item \textbf{the \emph{hh:mm:ss} time elapsed} since the beginning of the process;
		\item \textbf{the language} used to filter the incoming tweets;
		\item \textbf{the human-comprehensive values of the keywords set / subjects}, identified by colors: blue for the first subject (always displayed), and orange for the second one (only displayed if you set the second keywords set);
		\item \textbf{a tabs system} that allows you to navigate through the "map" and "charts" views:
			\begin{itemize}
				\item in the \textbf{map} view, you can see the received tweets grouped by clusters; once you zoom the map enough (a zoom level of 8 or more), these clusters disappear and you can access the tweets' markers one by one. By moving the cursor over a marker, the related tweet's content is displayed:
				\begin{figure}[H]
				\vspace{-5pt}
				\begin{center}
				\fbox{\includegraphics[width=0.35\textwidth]{figures/mouse-hover.png}}
				\vspace{-5pt}
				\caption{display of the tweet content by moving the mouse over the related marker}
				\end{center}
				\end{figure}
				\vspace{-10pt}
				
				\item in the \textbf{charts} view, you can access up to 6 different charts about the reception rate of tweets for each subject. If you filled only one keywords set, the third "parts" chart will not exist.
				\begin{figure}[H]
				\vspace{-5pt}
				\begin{center}
				\includegraphics[width=0.85\textwidth, right]{figures/charts.png}
				\vspace{-20pt}
				\caption{chart view}
				\end{center}
				\end{figure}
				\vspace{-10pt}
				
				The colors of the elements are the same as their related subjects. These charts are firstly refreshed each second until 60 seconds, then each minute. When you move the cursor over a graph's element, you will get more information.
				\bigskip
			\end{itemize}
		\item \textbf{the number of received tweets} for each subject;
		\item \textbf{the speed value of the reception} for each subject;
		\item \textbf{a panel that contains the last 100 received tweet's content}.
	\end{enumerate}
	\bigskip
	
	In addition, a "Stop Streaming" button is located on the top-right part of the page.	
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.25\textwidth]{figures/stop-streaming.png}}
	\vspace{-5pt}
	\caption{the button that allows the user to stop the current streaming process}
	\end{center}
	\end{figure}
	\vspace{-10pt}	
	\newpage
	
	Once you clicked on this button, the current streaming process will stop, a new "Make a new search" button will appear instead of the old "Stop Streaming" one, and a pop-up windows asking you if you want to export the analyzed data as an external file will instantly appear.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/export.png}}
	\vspace{-5pt}
	\caption{pop-up that asks the user if he want to export the analyzed data}
	\end{center}
	\end{figure}
	\vspace{-10pt}
	
	If you press the "Ok" button, you will be able to save the file wherever you want on your computer in order to import it in the application later. If you press the "Cancel" button, the window will close and you will \underline{not} be able to export and thus import the file later.\\
	
	Once the streaming process stopped, you can still consult the results, but everything is frozen. By pressing the "Make a new search" button, you will be redirected on the main search page.
\end{enumerate}
\newpage

\subsection{Static Mode}
The static mode of the application allows you to retrieve and analyze tweets already posted in the last days in a static and frozen way. Be aware of the fact that Twitter \underline{restricts} the maximum possible number of retrieved Tweets, so you may not get every possible results. Indeed, you can retrieve a maximum of "only" 17'000 Tweets if you filled only one subject, or "only" 8'500 per subject if you filled the two keywords sets. If the results seem restricted, reduce the dates range in order to get more specific results and try to be more specific with your search criteria. 

\begin{enumerate}
	\item If you want to statically search for tweets, you have to properly fill the form's fields:
	\begin{itemize}
		\item \textbf{Keywords Sets}: they act the same way as the keywords sets from the dynamic mode above, so please refer to them.
		\item \textbf{Range of Dates}: these values will be used to get the tweets that were posted during this time; they have to be well-formatted \emph{YYYY-MM-DD} dates. Since Twitter allows us to get tweets from at most 9 days ago only, you won't have any result if you search for Tweets before this day. Once you clicked on a field, a date picker element appears:
		\begin{figure}[H]
		\vspace{-5pt}
		\begin{center}
		\fbox{\includegraphics[width=0.5\textwidth]{figures/date-picker.png}}
		\vspace{-5pt}
		\caption{date-picker element that appears when the user clicked on a date field}
		\end{center}
		\end{figure}
		\vspace{-10pt}
		
		\item \textbf{Drawing of a Circle}: in order to select an area, you can only draw a circle in the static mode, because of the Twitter's requirements. The drawing process works the same way as the rectangle's drawing of the dynamic mode.
		\item \textbf{Language}: you can either keep the default "Any Language" selected value or select a language among the list of purposed languages, in order to filter tweets by their writing's language.
	\end{itemize}
	
	\item When you click on the "View Results!" button, an error message will appear if one of the fields is not valid. In the opposite, if everything is valid, the button will change its state and the search process will begin.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/static-search-button.png}}
	\vspace{-5pt}
	\caption{state of the "View Results!" button when the user clicked on it}
	\end{center}
	\end{figure}
	\vspace{-10pt}
	\newpage
	
	Be aware of the fact that the number of requests is strongly limited by Twitter. Thus, you may not be able to make more than a few search per periods of 15 minutes, depending on the topics you searched. If the limit is exceeded, an error message will appear and you will not be able to perform search for the 15 next minutes.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/limit-rate-exceeded.png}}
	\vspace{-5pt}
	\caption{the pop-up window that appears if the user exceeded the maximum limit rate imposed by Twitter}
	\end{center}
	\end{figure}
	\vspace{-10pt}	
	
	\item On the search successfully  completed, you will be redirected on the results page of the static mode; everything is frozen and you can only access the data in a static way. There is currently no possibility to export the retrieved data from the static mode.
	\begin{figure}[H]
	\vspace{-5pt}
	\begin{center}
	\fbox{\includegraphics[width=0.85\textwidth]{figures/static-results.png}}
	\vspace{-5pt}
	\caption{annotated example of a static results' page with the "job" and "beach" keywords}
	\end{center}
	\end{figure}
	\vspace{-10pt}
	
	Due to the complexity of the Twitter's algorithms, some retrieved tweets may be out of the drawn circle. Several information are available on this page, among which:
	\begin{enumerate}
		\item \textbf{the human-comprehensive values of the keywords set / subjects}, identified by colors: blue for the first subject (always displayed), and orange for the second one (only displayed if you set the second keywords set);
		\item \textbf{the language} used to filter the incoming tweets;
		\item \textbf{the number of received geolocated and non-geolocated tweets} for each subject;
		\item \textbf{the map}, in which you can see the retrieved tweets grouped by clusters; once you zoom the map enough (a zoom level of 8 or more), these clusters disappear and you can access the tweets one by one. By moving the cursor over a marker, the related tweet's content is displayed, like in the dynamic mode.
		\item \textbf{a panel that contains all (both with and without geolocation tags) the retrieved tweet's content}.
	\end{enumerate}
	
	\item By clicking on the "Make a new search" button, you will be redirected on the main search page.
\end{enumerate}

\chapter{Initial Specifications}
\section{Functionalities}
The application will provide the following features:
\begin{itemize}
	\item The reading of keywords and the selection of geographic areas on the map by the user.
	\item The retrieval of tweets, using the Twitter's APIs.
		Note that only a certain percentage of these tweets include geographic information, necessary for the future operations; a first filtering will thus be operated here.
	\item The analysis and the filtering of tweets via calculation of the number of tweets by areas and by subjects.
	\item The visualization of the results on maps.
	\item The interaction (zoom-in, zoom-out, etc.) with the maps. The development of this feature will involve the use of appropriate algorithms (like tweet grouping) and libraries.
\end{itemize}

\section{Tasks}
 Here is a chronological list of the implementation's steps and achievements that will be achieved in this project:
\begin{enumerate}
	\item Use of the Twitter's APIs, including:
	\begin{itemize}
		\item Retrieval and filtering of tweets by one or more keyword(s).
		\item Retrieval of geographic information attached to a tweet.
		\item Analysis of the percentage of tweets having geographic information (necessary for further processing) and the search of keywords having relevant results, in order to determine the best keywords for use in order to test the different project features.
	\end{itemize}
	\item Use of the OpenStreetMap API as well as the "Leaflet" library in order to use geographic maps.
	\item Development of the tweet's acquisition and analysis parts.
	\item Development of the web interface of the application, allowing the user to enter subjects to compare, to choose the geographic areas on the map, to visualize results and to interact with the map.
	\item Development of a grouping algorithm that groups the tweets according to the map's zoom level.
	\item Testing and validation.
\end{enumerate}

\section{Used Technologies}
The suggested tools for this project were the \emph{Scala} language coupled with \emph{Play Framework}, while noting other technological choices were also possible.\\

Due to both the personal interests of the student and a certain curiosity for unknown technologies in general, it was decided that these choices will be retained.\\

Lacking proficiency in either of these technologies, the student will initially be required to learn the Scala language and the use of Play Framework before starting the development on the application.\\

Note that different web languages (like \emph{JavaScript} in particular) could be used to complement the client side of the application.

\section{Future Extensions}
Here is a non-exhaustive list of various extensions and improvements, which could be developed if time allows or in a potential future developmental update:
\begin{itemize}
	\item Implementation of a user management platform (Sign-up, Log-in), in order to back up the different obtained results.
	\item Storage of the most successful keywords as well as the areas having the best geographic results within the application.
	\item Implementation of two modes of result display: a real-time updated mode, allowing the application to automatically receive and process new tweets in accordance with the filters (this feature will in any case be developed), and a static way, which could only display the results once without any further updating, and which could even simulate a streaming by analyzing the tweet's publication dates.
	\item Addition of social widgets (sharing of aggregated results, notes, etc.).	
	\item Addition of a point based system (allowing users to accumulate points and trophies by the results rate of their search, implementation of user ranking, etc.).
\end{itemize}

\chapter{Planning}
\vspace{-30pt}
\section{Initial Planning}
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.8\textwidth]{figures/initial-planning.png}}
\vspace{-5pt}
\caption{initial planning of the project}
\end{center}
\end{figure}
\vspace{-10pt}

This planning was largely followed during the first half of the semester (until the 17th week). From there, a rescheduling of the 5 full-time weeks was produced.

\section{Second Half Planning - Rescheduling}
\begin{figure}[H]
\vspace{-5pt}
\begin{center}
\fbox{\includegraphics[width=0.6\textwidth]{figures/second-half-planning.png}}
\vspace{-5pt}
\caption{planning of the second half of the project}
\end{center}
\end{figure}
\vspace{-10pt}

This planning was also largely followed, except that the work was better distributed between the first three weeks and the fourth one, which was planned as a time frame (in case of delay).

\chapter{Logbook}
\begin{center}
\def\arraystretch{1.5}
\begin{longtable}{c|l|c|p{10cm}}
	\textbf{Week} & \textbf{Date} & \textbf{Hours} & \textbf{Task}\\\hline \endhead
	1 & 22.02.2016 & 4 & Conduction of various research on Scala and on the Play framework\\\hline
	2 & 01.03.16 & 0.5 & Meeting with Mrs. Fatemi\\
     & 02.03.16 & 2 & Writing of the specifications and the logbook\\
     & 04.03.16 & 2 & Writing of the planning\\
     & 06.03.16 & 2 & Writing of the planning and creation of the GitHub deposit\\\hline
    3 & 07.03.16 & 6 & Analysis of the Twitter's API and of Twitter4J, and consequently writing of the analysis documentation\\
     & 08.03.16 & 0.5 & Meeting with Mrs. Fatemi\\
     & 08.03.16 & 1 & Analysis of the Twitter's API and of Twitter4J, and consequently writing of the analysis documentation\\
     & 13.03.16 & 1 & Analysis of the Twitter's API and of Twitter4J, and consequently writing of the analysis documentation\\\hline
    4 & 15.03.16 & 0.5 & Meeting with Mrs. Fatemi\\
     & 15.03.16 & 0.5 & Analysis of the libraries usable for the geolocation as well as the interesting keywords for Switzerland, and production of a planning of news\\
     & 16.03.16 & 3 & Analysis of the libraries usable for the geolocation as well as the interesting keywords for Switzerland, and production of a planning of news\\\hline
    5 & 21.03.16 & 4.5 & Installation and production of tests on Twitter4J\\
     & 22.03.16 & 0.5 & Meeting with Mrs. Fatemi\\
     & 27.03.16 & 1.5 & Production of stats in the prototype application\\
     & 28.03.16 & 3.5 & Analysis of various Twitter's research\\\hline
    6 & 04.04.16 & 0.5 & Finalization of the specifications\\
     & 04.04.16 & 3.5 & Analysis of libraries concerning the maps\\
     & 05.04.16 & 0.5 & Meeting with Mrs. Fatemi\\
     & 06.04.16 & 1 & Adaptation of the map vizualisation app for the Tweets' visualization\\
     & 07.04.16 & 4 & Implementation of a web-sockets system between the Java prototype application and the JavaScript one\\
     & 08.04.16 & 2 & Implementation of a web-sockets system between the Java prototype application and the JavaScript one\\\hline
    7 & 11.04.16 & 3 & Display of Tweets in the JavaScript application\\
     & 12.04.16 & 0.5 & Meeting with Mrs. Fatemi\\
     & 17.04.16 & 1.5 & Production of the application's mock-up\\\hline
    8 & 18.04.16 & 3.5 & Production of the application's mock-up and analysis of the already existing applications on the Internet\\
     & 24.04.16 & 3.5 & Analysis of the percentage of Tweets which have geolocation tags with various parameters\\\hline
    9 & 25.04.16 & 4 & Analysis and research of interesting subjects and minor modifications on prototype applications\\
     & 26.04.16 & 0.5 & Meeting with Mrs. Fatemi\\\hline
    10 & 02.05.16 & 3 & Modification of the mock-up, according to the 26.04 meeting\\
     & 02.05.16 & 5 & Development of a new prototype application, allowing a user to draw a polygon all around a country by double-clicking on it\\
     & 02.05.16 & 1 & Translation of documents\\
     & 03.05.16 & 0.5 & Meeting with Mrs. Fatemi\\\hline
    11 & 09.05.16 & 4 & Reading of the Play framework's documentation\\\hline
    12 & 16.05.16 & 3 & Reading of the Play framework's documentation and production of test-applications\\
     & 16.05.16 & 5 & Creation of the application's basics and modification of the documentation\\
     & 17.05.16 & 0.5 & Meeting with Mrs. Fatemi\\\hline
    13 & 23.05.16 & 2 & Importation of the project in the IntelliJ IDE\\
     & 23.05.16 & 6.5 & Integration of the Twitter4J library in the project, development of the connection process and modification of the GUI\\
     & 23.05.16 & 1 & Modification of the documentation\\
     & 24.05.16 & 0.5 & Meeting with Mrs. Fatemi\\\hline
    14 & 30.05.16 & 8 & Development of the search page's GUI\\
     & 31.05.16 & 1 & Development of the search page's GUI\\
     & 31.05.16 & 1 & Translation of the analysis part of the documentation\\
     & 01.06.16 & 2 & Translation of the analysis part of the documentation\\
     & 02.06.16 & 1 & Translation of the analysis part of the documentation\\
     & 03.06.16 & 1.5 & Translation of the analysis part of the documentation\\\hline
    15 & 06.06.16 & 2 & Translation of the analysis part\\
     & 06.06.16 & 0.5 & Writing of the progress report's structure\\
     & 06.06.16 & 0.5 & Translation of the specifications\\
     & 07.06.16 & 2 & Creation and writing of the progress report\\
     & 11.06.16 & 1.5 & Writing of the progress report\\
     & 12.06.16 & 3.5 & Writing of the progress report\\\hline
    16 & 13.06.16 & 4 & Writing of the progress report\\
     & 14.06.16 & 1.5 & Writing of the progress report\\
     & 15.06.16 & 5 & Writing of the progress report\\
     & 16.06.16 & 5 & Writing of the progress report\\
     & 17.06.16 & 1 & Writing of the progress report\\\hline
    18 & 27.06.16 & 1 & Production of the GANTT planning for the second half\\
     & 27.06.16 & 6 & Implementation of sessions in GeoTwit and protection of pages (the user must be authenticated, etc.)\\
     & 28.06.16 & 7 & Implementation of sessions in GeoTwit and protection of pages (the user must be authenticated, etc.)\\
     & 29.06.16 & 3 & Writing of the documentation related to configurations, routes, sessions, flash scopes, cache and actions compositions\\
     & 29.06.16 & 5 & Integration of maps in the application\\
     & 29.06.16 & 1 & Writing of the documentation related to the drawing of rectangle by drag-and-dropping the cursor on the map\\
     & 30.06.16 & 4.5 & Implementation of a web-sockets system between the Play's server and the JavaScript client\\
     & 30.06.16 & 0.5 & Meeting with Mrs. Fatemi\\
     & 01.07.16 & 8 & Implementation of the streaming process in GeoTwit with web sockets\\\hline
    19 & 04.07.16 & 4 & Writing of the documentation related to JavaScript routing and web sockets\\
     & 05.07.16 & 2 & Writing of the documentation related to JavaScript routing and web sockets\\
     & 05.07.16 & 2 & Reading and validation of user's inputs for the streaming process\\
     & 05.07.16 & 1 & Grouping of the Tweets on the map\\
     & 05.07.16 & 3.5 & Selection of countries on the map with the drop-down menu\\
     & 06.07.16 & 0.5 & Meeting with Mrs. Fatemi\\
     & 06.07.16 & 5 & Selection of countries on the map with the drop-down menu\\
     & 06.07.16 & 1.5 & Implementation of two streaming processes at the same time, when the user filled the two keywords sets\\
     & 07.07.16 & 0.5 & Filtering of Tweets by language\\
     & 07.07.16 & 7 & Display of streaming information (number of received Tweets, speed, etc.)\\
     & 08.07.16 & 0.5 & Analysis of the FR-DE match of the Euro2016 event\\
     & 08.07.16 & 8.5 & Implementation of the streamings' charts\\
     & 09.07.16 & 1 & Implementation of the streamings' charts\\
     & 09.07.16 & 1 & Handling of exceptions\\
     & 10.07.16 & 1 & Writing of the documentation related to the analysis of the Euro 2016's final\\\hline
    20 & 11.07.16 & 7 & Writing of the documentation related to the last week's implementation\\
     & 12.07.16 & 5 & Reading and validation of user's inputs for the static mode\\
     & 13.07.16 & 0.5 & Writing of the documentation related to the langages' selection\\
     & 13.07.16 & 4.5 & Reading and validation of user's inputs for the static mode and retrieval of Tweets from the server\\
     & 13.07.16 & 0.5 & Meeting with Mrs. Fatemi\\
     & 14.07.16 & 6.5 & Implementation of the static mode\\
     & 15.07.16 & 4 & Implementation of the static mode\\
     & 15.07.16 & 3 & Generation of the backup files of the streaming's results\\\hline
    21 & 18.07.16 & 4 & Generation of the backup files of the streaming's results\\
     & 18.07.16 & 4 & Writing of the documentation related to the static mode and the generation of backup files\\
     & 18.07.16 & 1 & Review of the documentation\\
     & 19.07.16 & 6 & Review of the documentation\\
     & 19.07.16 & 2 & Writing of the documentation related to the generation of the backup files\\
     & 20.07.16 & 0.5 & Meeting with Mrs. Fatemi\\
     & 20.07.16 & 3.5 & Implementation of the about and help pages and modification of the GUI\\
     & 21.07.16 & 8 & Implementation of the files' importation\\
     & 22.07.16 & 5 & Implementation of the files' importation\\
     & 22.07.16 & 3.5 & Writing of the documentation related to the files' importation\\
     & 23.07.16 & 2 & Writing of the documentation related to the files' importation\\
     & 23.07.16 & 4 & Implementation of the static mode\\
     & 23.07.16 & 2.5 & Writing of the documentation related to the static mode\\
     & 24.07.16 & 3 & Writing of the documentation related to the static mode\\
     & 24.07.16 & 3.5 & Writing of the LaTeX document\\\hline
    22 & 25.07.16 & 14.5 & Writing and review of the LaTeX document and writing of the tests\\
     & 26.07.16 & 10 & Writing and review of the LaTeX document and writing of the tests\\
     & 27.07.16 & 15.5 & Writing and review of the LaTeX document and writing of the tests\\
     & 28.07.16 & 10 & Writing and review of the LaTeX document and writing of the tests\\
     & 29.07.16 & 5 & Finalisation and report printout\\\hline
     \multicolumn{2}{c|}{\textbf{TOTAL}} & \multicolumn{2}{|l}{\textbf{347.5}}\\
\end{longtable}
\end{center}

\end{document}